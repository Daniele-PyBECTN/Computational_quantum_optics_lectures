---
other-links:
  - text: Run notebook in Binder
    href: "https://mybinder.org/v2/gh/Daniele-PyBECTN/Computational_quantum_optics_lectures/notebooks/?urlpath=lab/tree/notebooks/lecture1/numba_and_jax.ipynb"
    icon: file-code
  - text: Run notebook in Google Colab
    href: "https://colab.research.google.com/github/Daniele-PyBECTN/Computational_quantum_optics_lectures/blob/notebooks/notebooks/lecture1/numba_and_jax.ipynb"
    icon: google
---
# Speeding up Python for Linear Algebra Tasks

## Numba: Just-In-Time Compilation

Numba uses LLVM to compile Python functions to machine code at runtime. Key points:

- **Decorators**: Use `@njit` (nopython mode) for best speed.  
- **Type inference**: Numba infers types on first run, then compiles specialized code.  
- **Compilation overhead**: The first call incurs compilation time; subsequent calls are fast.  
- **Object mode vs nopython mode**: Always aim for nopython mode to avoid Python object overhead.

**Example: Matrixâ€“Vector Multiplication**

{{< include ../../code_cells/lecture1/numba_and_jax/numba.qmd >}}

## JAX: XLA Compilation and Automatic Differentiation

JAX is a high-performance library from Google Research that extends NumPy with just-in-time compilation and automatic differentiation. It
- Compiles array operations via XLA, fusing kernels and reducing Python overhead.
- Supports GPU and TPU backends with minimal code changes.
- Provides `grad` for gradients of scalar functions, enabling optimisation and machine-learning tasks.
- Offers advanced transformations like `vmap` (vectorisation) and `pmap` (parallelism on multiple devices).

JAX is widely used in deep learning frameworks (e.g. Flax, Haiku), reinforcement learning, and scientific research (including physics simulations), thanks to its blend of speed and flexibility.

### Comparing Accuracy: AD vs Finite Differences

Below is a Quarto code cell that plots the error of finite differences (varying step size $h$) and automatic differentiation against the true derivative of $f(x) = e^{\sin(x)}$ at $x=1.0$.

{{< include ../../code_cells/lecture1/numba_and_jax/jax.qmd >}}
