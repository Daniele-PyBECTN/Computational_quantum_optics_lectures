---
other-links:
  - text: Run notebook in Binder
    href: "https://mybinder.org/v2/gh/Daniele-PyBECTN/Computational_quantum_optics_lectures/notebooks/?urlpath=lab/tree/notebooks/lecture1/numba_and_jax.ipynb"
    icon: file-code
  - text: Run notebook in Google Colab
    href: "https://colab.research.google.com/github/Daniele-PyBECTN/Computational_quantum_optics_lectures/blob/notebooks/notebooks/lecture1/numba_and_jax.ipynb"
    icon: google
---
# Speeding up Python for Linear Algebra Tasks

Python is easy to read, but pure-Python loops can be slow if you do not leverage optimized libraries (BLAS, LAPACK). Here we explore two tools—Numba and JAX—to accelerate common linear algebra operations.

## Numba: Just-In-Time Compilation

Numba uses LLVM to compile Python functions to machine code at runtime. Key points:

- **Decorators**: Use `@njit` (nopython mode) for best speed.  
- **Type inference**: Numba infers types on first run, then compiles specialized code.  
- **Compilation overhead**: The first call incurs compilation time; subsequent calls are fast.  
- **Object mode vs nopython mode**: Always aim for nopython mode to avoid Python object overhead.

> **JIT Workflow**
> 1. Call function → type inference → LLVM IR generation.  
> 2. LLVM IR → machine code (cached).  
> 3. Subsequent calls use cached machine code.

**Example: Matrix–Vector Multiplication**

{{< embed /notebooks/lecture1/numba_and_jax.qmd#numba echo=true >}}

In practice, Numba can speed up this looped version by 10×–100× compared to pure Python, approaching the speed of NumPy's optimized routines. The reader is encouraged to try the code without the `@njit` decorator to see the difference in performance.

## JAX: XLA Compilation and Automatic Differentiation

JAX is a high-performance library from Google Research that extends NumPy with just-in-time compilation and automatic differentiation. It
- Compiles array operations via XLA, fusing kernels and reducing Python overhead.
- Supports GPU and TPU backends with minimal code changes.
- Provides `grad` for gradients of scalar functions, enabling optimisation and machine-learning tasks.
- Offers advanced transformations like `vmap` (vectorisation) and `pmap` (parallelism on multiple devices).

JAX is widely used in deep learning frameworks (e.g. Flax, Haiku), reinforcement learning, and scientific research (including physics simulations), thanks to its blend of speed and flexibility.

### A Quick Overview of Automatic Differentiation

Automatic differentiation (AD) is a family of techniques to compute exact derivatives of functions defined by computer programs. Unlike symbolic differentiation (which can lead to expression swell) or numerical finite-difference (which suffers from truncation and round-off error), AD exploits the fact that any complex function is ultimately composed of a finite set of elementary operations (addition, multiplication, sin, exp, …) whose derivatives are known exactly.  

#### Limitations of Finite Differences

A common finite-difference formula for a scalar function $f(x)$ is the central difference

$$
\frac{df}{dx}(x)\approx \frac{f(x+h)-f(x-h)}{2h},
$$

with local truncation error $\mathcal{O}(h^2)$. However, this approach has important limitations:

1. **Truncation vs. round-off**: If $h$ is too large, the $\mathcal{O}(h^2)$ term dominates. If $h$ is too small, floating-point cancellation makes the numerator $f(x+h)-f(x-h)$ inaccurate.
2. **Cost with many parameters**: For $f:\mathbb{R}^n\to\mathbb{R}$, the gradient component $i$ is

   $$
   \frac{\partial f}{\partial x_i}(\mathbf{x})\approx \frac{f(\mathbf{x}+h\mathbf{e}_i)-f(\mathbf{x}-h\mathbf{e}_i)}{2h}.
   $$

   Computing all $n$ components requires $2n$ evaluations of $f$, so the cost scales as $\mathcal{O}(n)$ in $f$-calls. For large $n$ (many parameters), this becomes prohibitive.
3. **Non-smooth or branching code**: When $f$ contains control flow or non-differentiable operations, finite differences may give misleading or undefined results.

#### Automatic Differentiation and the Chain Rule

Automatic differentiation (AD) applies the chain rule to each elementary operation in code (addition, multiplication, sin, exp, etc.), yielding exact derivatives up to floating-point precision. For a composition

$$
u = g(x),\quad y = f(u),
$$

AD uses the chain rule:

$$
\frac{dy}{dx} = \frac{df}{du}\frac{dg}{dx}.
$$

In more complex nests, e.g.

$$
v = h(u),\quad u = g(x),\quad y = f(v),
$$

we get

$$
\frac{dy}{dx} = \frac{df}{dv}\frac{dh}{du}\frac{dg}{dx}.
$$

AD comes in two modes:

* **Forward mode** (propagate derivatives from inputs to outputs).
* **Reverse mode** (propagate sensitivities from outputs back to inputs).

JAX implements both and selects the most efficient strategy automatically.

#### Comparing Accuracy: AD vs Finite Differences

Below is a Quarto code cell that plots the error of finite differences (varying step size $h$) and automatic differentiation against the true derivative of $f(x) = e^{\sin(x)}$ at $x=1.0$.

{{< embed /notebooks/lecture1/numba_and_jax.qmd#jax echo=true >}}

This plot illustrates that finite differences achieve minimal error at an optimal $h$, but degrade for too large or too small $h$, while AD remains accurate to machine precision regardless of step size.

## Summary

- **Numba**: Best for speeding up existing NumPy loops with minimal code changes. Ideal when you do not need gradients or accelerators.
- **JAX**: Ideal for optimisation tasks requiring gradients, large-scale batch operations, or GPU/TPU acceleration. The XLA compiler often outperforms loop-based JIT for fused kernels.
