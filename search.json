[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Numerical Methods for Quantum Optics and Open Quantum Systems",
    "section": "",
    "text": "Numerical Methods for Quantum Optics and Open Quantum Systems is a hands-on course that shows you how to model and simulate open quantum systems in quantum optics with Python and QuTiP. The notes mix concise explanations, essential equations, and runnable code cells that work both on your computer and in Google Colab. Everything lives in a Quarto project on GitHub and is published in HTML and PDF for easy reading and collaboration. By the end, you will be able to set up and explore standard problems—such as photon cavities, two-level atoms, and open-system dynamics—using tools you can reuse in research and projects.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Home Page</span>"
    ]
  },
  {
    "objectID": "lecture1/introduction.html",
    "href": "lecture1/introduction.html",
    "title": "2  Introduction",
    "section": "",
    "text": "2.1 Why simulate open quantum systems?\nThe experimental frontier of quantum optics increasingly targets systems that cannot be described by perfectly isolated, unitary dynamics. Photons leak from cavities, solid‑state qubits couple to phonons, and measurement back‑action reshapes quantum states in real time. In these scenarios the open character of the system—the interplay between coherent evolution and irreversible processes—becomes the defining feature, not a perturbation. Analytical solutions exist only for a handful of toy models; to design devices, interpret data, and test conceptual ideas we therefore rely on numerical simulation of open quantum dynamics.\nNumerical methods allow us to:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "lecture1/introduction.html#why-simulate-open-quantum-systems",
    "href": "lecture1/introduction.html#why-simulate-open-quantum-systems",
    "title": "2  Introduction",
    "section": "",
    "text": "Predict observables such as spectra, correlation functions, or entanglement measures before running an experiment.\nPrototype control protocols (e.g., pulse shaping or feedback) that can stabilize fragile quantum states.\nExplore parameter regimes that are inaccessible analytically, revealing new phenomena like dissipative phase transitions or non‑Markovian memory effects.\n\n\n\n\n\n\n\nFigure 2.1: Description of an open quantum system and its practical applications. A quantum system interacts with a macroscopic environment, leading to decoherence and dissipation. The evolution of the system is described the master equation \\(\\dot{\\hat{\\rho}} = \\mathcal{L}_T(t) [\\hat{\\rho}]\\), where \\(\\hat{\\rho}\\) is the density matrix and \\(\\mathcal{L}_T(t)\\) is the Liouville superoperator. The solution can be used to study the steady state and non-equilibrium properties of the system. The theoretical study of open quantum systems offers several tools for modeling spin resonance, optical spectra, and quantum information processing, and their use is certainly not limited to these fields and applications. Reproduced from (Campaioli, Cole, and Hapuarachchi 2024) under a CC BY 4.0 license.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "lecture1/introduction.html#why-python",
    "href": "lecture1/introduction.html#why-python",
    "title": "2  Introduction",
    "section": "2.2 Why Python?",
    "text": "2.2 Why Python?\nPython is not the fastest language for floating‑point arithmetic—compiled languages like C or Fortran still win raw speed benchmarks—but it has become the lingua franca of modern scientific computing. Three qualities make it particularly compelling for our purposes:\n\nExpressiveness – A succinct, readable syntax lowers cognitive overhead and lets us translate mathematical ideas into code quickly.\nRich ecosystem – Numpy, SciPy, Jupyter, Matplotlib, and data‑analysis libraries coexist seamlessly, providing everything from linear algebra kernels to publication‑quality plots.\nCommunity & portability – Tutorials, StackOverflow answers, CI pipelines, and cloud platforms such as Google Colab enable beginners to run the same notebooks locally or on GPUs in the cloud with negligible setup.\n\nMost importantly, Python hosts QuTiP (Quantum Toolbox in Python)(Johansson, Nation, and Nori 2012; Lambert et al. 2024) the de‑facto standard library for simulating open quantum systems. QuTiP wraps efficient C and Fortran back‑ends behind a high‑level interface: you manipulate Qobj instances instead of raw matrices, and you call solvers such as mesolve or mcsolve for Lindblad‑master equations and quantum trajectory simulations, respectively. The package is actively maintained, well documented, and battle‑tested across thousands of research papers.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "lecture1/introduction.html#how-does-python-differ-from-other-mainstream-languages",
    "href": "lecture1/introduction.html#how-does-python-differ-from-other-mainstream-languages",
    "title": "2  Introduction",
    "section": "2.3 How does Python differ from other mainstream languages?",
    "text": "2.3 How does Python differ from other mainstream languages?\n\n\n\n\n\n\n\n\n\nLanguage\nParadigm\nTypical strength\nTypical weakness\n\n\n\n\nC / C++\nCompiled, low‑level\nMaximal performance, fine‑grained memory control\nVerbose, higher barrier to entry, manual parallelization\n\n\nFortran\nCompiled, array‑oriented\nLegacy HPC codes, excellent BLAS/LAPACK bindings\nLimited modern features, smaller community\n\n\nMATLAB\nProprietary, array‑oriented\nIntegrated IDE, built‑in plotting, domain‑specific toolboxes\nLicense cost, closed ecosystem\n\n\nPython\nInterpreted, multi‑paradigm\nReadability, vast open‑source libraries, rapid prototyping\nOverhead of interpreter, GIL limits naive multithreading\n\n\n\nPython balances high‑level productivity with the option to call compiled extensions (via Cython, Numba, or Rust bindings) whenever performance matters.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "lecture1/introduction.html#a-glance-at-julia-and-quantumtoolbox.jl",
    "href": "lecture1/introduction.html#a-glance-at-julia-and-quantumtoolbox.jl",
    "title": "2  Introduction",
    "section": "2.4 A glance at Julia and QuantumToolbox.jl",
    "text": "2.4 A glance at Julia and QuantumToolbox.jl\nWhile Python dominates current scientific computing, it is not the only contender. In recent years, researchers and engineers have been exploring the need for a new programming language—one that combines the performance of compiled languages like C or Fortran with the ease of use and readability of scripting languages like Python or MATLAB. This is the motivation behind Julia.\nJulia promises “C‑like speed with Python‑like syntax” by using just‑in‑time (JIT) compilation and a multiple‑dispatch programming model. Within this language, the package QuantumToolbox.jl(Mercurio et al. 2025) has emerged as a high‑performance analog to QuTiP. It mirrors QuTiP’s API but benefits from Julia’s performance model and native automatic differentiation. Benchmarks already demonstrate significant speed‑ups, especially for large Hilbert spaces and GPU‑accelerated workloads.\nNevertheless, Julia’s ecosystem is still maturing. Its tooling, package stability, and IDE support are evolving rapidly but are not yet as robust as Python’s. Similarly, QuantumToolbox.jl, while powerful, has a smaller user base and fewer educational resources compared to QuTiP. For a course focused on accessibility and broad applicability, we therefore choose to prioritize Python and QuTiP as the more mature and stable learning platform.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "lecture1/introduction.html#course-scope",
    "href": "lecture1/introduction.html#course-scope",
    "title": "2  Introduction",
    "section": "2.5 Course scope",
    "text": "2.5 Course scope\nIn this course we therefore focus on Python + QuTiP. You will learn to:\n\nBuild Hamiltonians and collapse operators in a composable way.\nIntegrate master equations and unravel them into quantum trajectories.\nCompute expectation values, spectra, and correlation functions.\nCouple simulations to optimisation or machine‑learning workflows within the wider Python ecosystem.\n\nWhere Julia can offer useful perspective we will point out parallels, but all hands‑on examples will run in Python notebooks that you can execute locally or on Colab.\n\nTake‑away: Numerical simulation is the microscope of modern quantum optics. Python and QuTiP give us a practical, accessible, and well‑supported platform for that microscope—letting us peer into the dynamics of open quantum systems without getting lost in low‑level details.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "lecture1/introduction.html#first-steps-in-python-lists-loops-and-functions",
    "href": "lecture1/introduction.html#first-steps-in-python-lists-loops-and-functions",
    "title": "2  Introduction",
    "section": "2.6 First steps in Python: lists, loops, and functions",
    "text": "2.6 First steps in Python: lists, loops, and functions\n\n2.6.1 Creating and using lists\nBefore diving into numerical simulations, it’s useful to get acquainted with the basic syntax and features of Python. One of the simplest and most commonly used data structures is the list, which stores a sequence of elements. Lists are flexible—they can contain numbers, strings, or even other lists.\nHere’s how to create and access elements in a list:\n\n\nfruits = ['apple', 'banana', 'cherry']\nprint(f'First fruit: {fruits[0]}')\n\nFirst fruit: apple\n\n\n\n\n\n2.6.2 For loops\nA for loop allows us to iterate through each item in a collection and execute the same block of code for every element. You will use loops constantly—whether you are sweeping parameter values, accumulating results, or analysing datasets—so it is worth seeing the syntax early.\n\n\nfor fruit in fruits:\n    print(f'I like {fruit}')\n\nI like apple\nI like banana\nI like cherry\n\n\n\n\n\n2.6.3 Defining functions\nFunctions bundle reusable logic behind a descriptive name. In quantum‑optics simulations, well‑structured functions help keep notebooks tidy—for instance, collecting the code that builds a Hamiltonian or evaluates an observable in one place. Below is a minimal example that squares a number.\n\n\ndef square(x):\n    return x * x\n\nprint(square(5))\n\n25\n\n\n\n\n\n2.6.4 Lambda (anonymous) functions\nOccasionally we only need a small, throw‑away function—say, as a callback or key in a sort operation. Python’s lambda syntax lets us declare such anonymous functions in a single line, without the ceremony of def.\n\n\nsquare_lambda = lambda x: x * x\nprint(square_lambda(5))\n\n25\n\n\n\n\n\n2.6.5 Complex numbers\nPython has built‑in support for complex numbers, which are represented as a + bj, where a is the real part and b is the imaginary part. This is particularly useful in quantum mechanics, where complex numbers are ubiquitous.\n\n\nz = 1 + 2j\nprint(f'Complex number: {z}')\nprint(f'Real part: {z.real}')\nprint(f'Magnitude: {abs(z)}')\n\nComplex number: (1+2j)\nReal part: 1.0\nMagnitude: 2.23606797749979\n\n\n\n\n\n2.6.6 Why plain Python lists can be slow\nPython lists store references to arbitrary Python objects. Each element carries its own type information and reference count. When you perform arithmetic on list elements, the interpreter must\n\nLook up the byte‑code for each operation.\nResolve types at runtime.\nDispatch to the correct C implementation.\n\nThis per‑element overhead dominates runtime in numerical workloads.\n\n\n2.6.7 Enter numpy\nTo overcome the performance limits of pure‑Python lists, we turn to NumPy, which stores data in contiguous, fixed‑type arrays and dispatches mathematical operations to highly‑optimised C (and often SIMD/GPU) kernels. The example below shows how you can express a million‑element computation in just two vectorised lines.\nnumpy provides fixed‑type, contiguous arrays backed by efficient C (or SIMD/GPU) loops. Operations are dispatched once for the whole array, eliminating Python‑level overhead and unlocking BLAS/LAPACK acceleration.\nAs an example, we can compute the sum of all the elements of a python list, comparing the performance with a numpy array.\n\n\nimport numpy as np\nimport time # Only for benchmarking\n\nmy_list = [i / 1_000_000 for i in range(1_000_000)]\n\nstart = time.time() # start timer\nsum_list = sum(my_list)  # sum using Python list\nend = time.time()  # end timer\nprint(f'Sum using list: {sum_list}, '\n      f'Time taken: {1e3*(end - start):.4f} milliseconds')\n\nmy_list_numpy = np.array(my_list)\nstart = time.time()  # start timer\nsum_numpy = np.sum(my_list_numpy)  # sum using numpy array\nend = time.time()  # end timer\nprint(f'Sum using numpy: {sum_numpy}, '\n      f'Time taken: {1e3*(end - start):.4f} milliseconds')\n\nSum using list: 499999.5, Time taken: 7.7775 milliseconds\nSum using numpy: 499999.5, Time taken: 0.4807 milliseconds\n\n\n\nNumPy is also able to perform vectorized operations, which let us express complex computations in a few lines of code. For example, we can compute a function of all elements in an array without writing explicit loops. This is not only more readable but also significantly faster, as the underlying C code can be optimised for performance.\n\n\n# Vectorized array operations\nx = np.linspace(0, 100, 1_000_000)\ny = np.sin(x) + 0.5 * x**2\nprint(y[:5])  # show first five results\n\n[0.         0.00010001 0.00020002 0.00030005 0.00040008]\n\n\n\nOne line performs a million floating‑point operations in compiled code—often orders of magnitude faster than an explicit Python loop.\n\n\n\n\nCampaioli, Francesco, Jared H. Cole, and Harini Hapuarachchi. 2024. “Quantum Master Equations: Tips and Tricks for Quantum Optics, Quantum Computing, and Beyond.” PRX Quantum 5 (June): 020202. https://doi.org/10.1103/PRXQuantum.5.020202.\n\n\nJohansson, J. R., P. D. Nation, and Franco Nori. 2012. “QuTiP: An open-source Python framework for the dynamics of open quantum systems.” Computer Physics Communications 183 (8): 1760–72. https://doi.org/10.1016/j.cpc.2012.02.021.\n\n\nLambert, Neill, Eric Giguère, Paul Menczel, Boxi Li, Patrick Hopf, Gerardo Suárez, Marc Gali, et al. 2024. “QuTiP 5: The Quantum Toolbox in Python.” arXiv:2412.04705. https://arxiv.org/abs/2412.04705.\n\n\nMercurio, Alberto, Yi-Te Huang, Li-Xun Cai, Yueh-Nan Chen, Vincenzo Savona, and Franco Nori. 2025. “QuantumToolbox.jl: An Efficient Julia Framework for Simulating Open Quantum Systems.” arXiv Preprint arXiv:2504.21440. https://doi.org/10.48550/arXiv.2504.21440.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "lecture1/linear_algebra.html",
    "href": "lecture1/linear_algebra.html",
    "title": "3  Linear Algebra with NumPy and SciPy",
    "section": "",
    "text": "3.1 NumPy: The Foundation of Dense Linear Algebra\nQuantum systems are described by vectors and operators in complex Hilbert spaces. States \\(\\vert \\psi \\rangle\\) correspond to column vectors, and observables—like the Hamiltonian \\(\\hat{H}\\) or spin operators—are represented by matrices. Tasks such as finding energy spectra via eigenvalue decompositions, simulating time evolution through operator exponentials, and building composite systems with tensor (Kronecker) products all reduce to core linear‐algebra operations.\nIn this chapter, we will leverage NumPy’s and SciPy’s routines (backed by optimized BLAS/LAPACK) to perform matrix–matrix products, eigen-decompositions, vector norms, and more. When system size grows, SciPy’s sparse data structures and Krylov‐subspace solvers will let us handle very large, structured operators efficiently.\nBy blending physical intuition (Schrödinger’s equation, expectation values, operator algebra) with hands‐on Python code, you’ll see how powerful and intuitive modern linear‐algebra libraries can be for quantum‐mechanics simulations. Let’s get started!\nNumPy provides the ndarray type, an efficient, N-dimensional array stored in contiguous memory. This layout makes vectorized operations and low-level BLAS calls blazing fast. At its simplest, a 2D ndarray represents a matrix:\n\\[\nA = \\begin{pmatrix}a_{11} & a_{12}\\\\ a_{21} & a_{22}\\end{pmatrix},\n\\]\nand a 1D ndarray represents a column vector:\n\\[\n\\mathbf{v} = \\begin{pmatrix}v_1\\\\ v_2\\end{pmatrix}.\n\\]\nNumPy’s dense arrays form the backbone of many quantum‐simulation tasks—building Hamiltonians, computing overlaps, and propagating states all reduce to these core operations. Having a quick reference for them can speed up both writing and reading simulation code.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Algebra with NumPy and SciPy</span>"
    ]
  },
  {
    "objectID": "lecture1/linear_algebra.html#numpy-the-foundation-of-dense-linear-algebra",
    "href": "lecture1/linear_algebra.html#numpy-the-foundation-of-dense-linear-algebra",
    "title": "3  Linear Algebra with NumPy and SciPy",
    "section": "",
    "text": "3.1.1 Summary of Core Functions\n\n\n\n\n\n\n\n\nOperation\nEquation\nNumPy call\n\n\n\n\nMatrix–matrix product\n\\(C = A B\\)\nC = A.dot(B) or A @ B\n\n\nMatrix–vector product\n\\(\\mathbf{w} = A \\mathbf{v}\\)\nw = A.dot(v)\n\n\nEigenvalues and eigenvectors\n\\(A \\mathbf{x} = \\lambda \\mathbf{x}\\)\nw, v = np.linalg.eig(A)\n\n\nDeterminant\n\\(\\det(A)\\)\nnp.linalg.det(A)\n\n\nInverse\n\\(A^{-1}\\)\nnp.linalg.inv(A)\n\n\nNorm (Frobenius)\n\\(\\|A\\|_F = \\sqrt{\\sum_{ij} \\vert a_{ij} \\vert^2}\\)\nnp.linalg.norm(A)\n\n\nKronecker product\n\\(A \\otimes B\\)\nnp.kron(A, B)\n\n\n\nIn the table above, each abstract operation is paired with its NumPy call. Notice how intuitive the syntax is: the @ operator reads like the usual linear-algebra notation.\n\n\n3.1.2 Matrix–Matrix and Matrix–Vector Multiplication\nLet’s consider a simple example of a 2×2 matrix \\(A\\) and a 2-vector \\(\\mathbf{v}\\). This captures key ideas: operator composition via matrix–matrix products and state evolution via matrix–vector products. Indeed, in quantum mechanics, applying one operator after another corresponds to a matrix–matrix product, while acting on a quantum state uses a matrix–vector product. Consider the following:\n\n\nimport numpy as np\n\n# Define a 2×2 matrix and a 2-vector\nA = np.array([[1, 2], [3, 4]])\nv = np.array([5, 6])\n\n# Matrix–matrix product\nc = A @ A  # same as A.dot(A)\ndisplay(\"A @ A =\", c)\n\n# Matrix–vector product\nw = A @ v  # same as A.dot(v)\ndisplay(\"A @ v =\", w)\n\n'A @ A ='\n\n\narray([[ 7, 10],\n       [15, 22]])\n\n\n'A @ v ='\n\n\narray([17, 39])\n\n\n\nHere, A @ A computes \\(A^2\\), and A @ v computes \\(A\\mathbf{v}\\).\n\n\n3.1.3 Diagonalization\nThe eigenvalue problem is one of the cornerstones of both applied mathematics and quantum mechanics. Given a square matrix \\(A \\in \\mathbb{C}^{n\\times n}\\), we seek scalars \\(\\lambda\\in\\mathbb{C}\\) (eigenvalues) and nonzero vectors \\(\\mathbf{x}\\in\\mathbb{C}^n\\) (eigenvectors) such that\n\\[\nA \\,\\mathbf{x} = \\lambda\\,\\mathbf{x}.\n\\]\nPhysically, in quantum mechanics, \\(A\\) might be the Hamiltonian operator \\(\\hat H\\), its eigenvalues \\(\\lambda\\) correspond to allowed energy levels, and the eigenvectors \\(\\mathbf{x}\\) represent stationary states. Mathematically, diagonalizing \\(A\\) transforms it into a simple form\n\\[\nA = V \\,\\Lambda\\, V^{-1},\n\\]\nwhere \\(\\Lambda\\) is the diagonal matrix of eigenvalues and the columns of \\(V\\) are the corresponding eigenvectors. Once in diagonal form, many operations—such as computing matrix exponentials for time evolution, powers of \\(A\\), or resolving a system of differential equations—become trivial:\n\n\\[\nf(A) = V\\,f(\\Lambda)\\,V^{-1},\\quad\nf(\\Lambda) = \\mathrm{diag}\\bigl(f(\\lambda_1),\\dots,f(\\lambda_n)\\bigr).\n\\]\nIn practice, NumPy’s np.linalg.eig calls optimized LAPACK routines to compute all eigenpairs of a dense matrix:\n\n\nw, v = np.linalg.eig(A)\ndisplay(\"Eigenvalues:\", w)\ndisplay(\"Eigenvectors (as columns):\\n\", v)\n\n'Eigenvalues:'\n\n\narray([-0.37228132,  5.37228132])\n\n\n'Eigenvectors (as columns):\\n'\n\n\narray([[-0.82456484, -0.41597356],\n       [ 0.56576746, -0.90937671]])\n\n\n\nUnder the hood, NumPy calls optimized LAPACK routines to diagonalize dense matrices.\n\n\n3.1.4 Kronecker Product\nIn quantum mechanics, the state space of a composite system is the tensor product of the state spaces of its subsystems. If system 1 has Hilbert space \\(\\mathcal H_A\\) of dimension \\(m\\) and system 2 has \\(\\mathcal H_B\\) of dimension \\(p\\), then the joint space is \\(\\mathcal H_A\\otimes\\mathcal H_B\\) of dimension \\(m p\\). Operators on the composite system factorize as tensor (Kronecker) products of subsystem operators. For example, if \\(A\\) acts on system 1 and \\(B\\) on system 2, then \\[\nA\\otimes B\\;:\\;\\mathcal H_A\\otimes\\mathcal H_B\\to \\mathcal H_A\\otimes\\mathcal H_B\n\\] has matrix elements \\[\n(A\\otimes B)_{(i\\,,\\,\\alpha),(j\\,,\\,\\beta)}\n    = A_{ij}\\,B_{\\alpha\\beta},\n\\] and in block form \\[\nA \\otimes B\n= \\begin{pmatrix}\n    a_{11}\\,B & a_{12}\\,B & \\cdots & a_{1n}\\,B \\\\\n    \\vdots    &        &        & \\vdots     \\\\\n    a_{m1}\\,B & a_{m2}\\,B & \\cdots & a_{mn}\\,B\n\\end{pmatrix},\n\\] yielding an \\(mp\\times nq\\) matrix when \\(A\\in\\mathbb C^{m\\times n}\\) and \\(B\\in\\mathbb C^{p\\times q}\\).\nWhy is this useful? In later chapters we will build multi‐qubit gates (e.g. CNOT, controlled-phase), couple different oscillators, and assemble large Hamiltonians by taking tensor products of single‐mode operators. The Kronecker product lets us lift any local operator into the full, composite Hilbert space.\nIn NumPy, the Kronecker product is computed with np.kron:\n\n\nB = np.array([[0, 1], [1, 0]])  # Pauli-X matrix\nkron = np.kron(A, B)\ndisplay(\"A ⊗ B =\", kron)\n\n'A ⊗ B ='\n\n\narray([[0, 1, 0, 2],\n       [1, 0, 2, 0],\n       [0, 3, 0, 4],\n       [3, 0, 4, 0]])\n\n\n\nKronecker products build composite quantum-system operators from single-subsystem operators.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Algebra with NumPy and SciPy</span>"
    ]
  },
  {
    "objectID": "lecture1/linear_algebra.html#scipy-advanced-algorithms-and-sparse-data",
    "href": "lecture1/linear_algebra.html#scipy-advanced-algorithms-and-sparse-data",
    "title": "3  Linear Algebra with NumPy and SciPy",
    "section": "3.2 SciPy: Advanced Algorithms and Sparse Data",
    "text": "3.2 SciPy: Advanced Algorithms and Sparse Data\nWhile NumPy covers dense linear algebra, SciPy complements it with:\n\n\n\n\n\n\n\nModule\nPurpose\n\n\n\n\nscipy.linalg\nAlternative LAPACK-based routines for dense ops\n\n\nscipy.sparse\nData structures (COO, CSR, CSC) for sparse matrices\n\n\nscipy.sparse.linalg\nIterative solvers (e.g. Arnoldi, Lanczos)\n\n\nscipy.integrate\nODE and quadrature routines\n\n\nscipy.optimize\nRoot-finding and minimization\n\n\nscipy.special\nSpecial mathematical functions\n\n\n\nCompared to NumPy, SciPy’s routines often expose extra options (e.g. choosing solvers) and can handle very large, sparse systems efficiently.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Algebra with NumPy and SciPy</span>"
    ]
  },
  {
    "objectID": "lecture1/linear_algebra.html#some-useful-functions",
    "href": "lecture1/linear_algebra.html#some-useful-functions",
    "title": "3  Linear Algebra with NumPy and SciPy",
    "section": "3.3 Some Useful Functions",
    "text": "3.3 Some Useful Functions\nBelow are a few handy SciPy routines:\n\nDeterminant: scipy.linalg.det\nInverse: scipy.linalg.inv\nFrobenius norm: scipy.linalg.norm\n\n\n\nimport scipy.linalg as la\n\ndet = la.det(A)\ninv = la.inv(A)\nnorm_f = la.norm(A)\ndisplay(det, inv, norm_f)\n\nnp.float64(-2.0)\n\n\narray([[-2. ,  1. ],\n       [ 1.5, -0.5]])\n\n\nnp.float64(5.477225575051661)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Algebra with NumPy and SciPy</span>"
    ]
  },
  {
    "objectID": "lecture1/linear_algebra.html#solving-linear-systems",
    "href": "lecture1/linear_algebra.html#solving-linear-systems",
    "title": "3  Linear Algebra with NumPy and SciPy",
    "section": "3.4 Solving Linear Systems",
    "text": "3.4 Solving Linear Systems\nA linear system has the form\n\\[\nA\\,\\mathbf{x} = \\mathbf{b},\n\\]\nwhere \\(A\\in\\mathbb R^{n\\times n}\\) and \\(\\mathbf b\\in\\mathbb R^n\\) is known. For small \\(n\\) you can even solve by hand. For example, consider the \\(2\\times2\\) system\n\\[\n\\begin{cases}\nx_1 + 2x_2 = 5,\\\\\n3x_1 + 4x_2 = 11.\n\\end{cases}\n\\quad\\Longrightarrow\\quad\nA = \\begin{pmatrix}1 & 2\\\\ 3 & 4\\end{pmatrix},\\;\n\\mathbf b = \\begin{pmatrix}5\\\\11\\end{pmatrix}.\n\\]\nWe can reproduce this with NumPy:\n\n\nA = np.array([[1, 2], [3, 4]])\nb = np.array([5, 11])\nx = np.linalg.solve(A, b)\ndisplay(\"Solution x=\", x)\n\n'Solution x='\n\n\narray([1., 2.])\n\n\n\nSciPy’s sparse module also offers scipy.sparse.linalg.spsolve for large, sparse \\(A\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Algebra with NumPy and SciPy</span>"
    ]
  },
  {
    "objectID": "lecture1/linear_algebra.html#sparse-matrices",
    "href": "lecture1/linear_algebra.html#sparse-matrices",
    "title": "3  Linear Algebra with NumPy and SciPy",
    "section": "3.5 Sparse Matrices",
    "text": "3.5 Sparse Matrices\nAs quantum systems scale to many degrees of freedom, the underlying operators—such as Hamiltonians or Liouvillian superoperators—grow exponentially in dimension but often remain highly structured and sparse. Instead of storing dense arrays with mostly zeros, sparse-matrix formats only record nonzero entries and their indices, dramatically reducing memory requirements. Common physical models, like spin chains with nearest-neighbor couplings or lattice Hamiltonians, have only \\(\\mathcal{O}(N)\\) or \\(\\mathcal{O}(N \\log N)\\) nonzero elements, making sparse representations essential for large-scale simulations.\nIn the following sections, we will:\n\nConstruct sparse matrices in COO formats with SciPy.\nIllustrate basic sparse-matrix operations (matrix–vector products, format conversions).\nUse scipy.sparse.linalg.eigs (Arnoldi) to compute a few eigenvalues of a sparse Hamiltonian.\n\nThe Coordinate (COO) format is a simple way to store sparse matrices. Instead of storing all entries, the COO format only keeps nonzero entries of the form \\((i, j, a_{ij})\\), which saves memory and speeds up computations. Graphically, a 5×5 example with 4 nonzeros might look like:\n\\[\nA = \\begin{pmatrix}\n7 & \\cdot & \\cdot & \\cdot & 1 \\\\\n\\cdot & \\cdot & 2      & \\cdot & \\cdot \\\\\n\\cdot & 3      & \\cdot & \\cdot & \\cdot \\\\\n\\cdot & \\cdot & \\cdot & \\cdot & \\cdot \\\\\n4      & \\cdot & \\cdot & \\cdot & \\cdot\n\\end{pmatrix}\n\\]\nHere each number shows a location and its value. COO is very simple and intuitive, but not the most efficient. For larger matrices, we can use the Compressed Sparse Row (CSR) or Compressed Sparse Column (CSC) formats, which store the nonzero entries in a more compact way. The CSR format is very efficient for matrix–vector products.\nSuch matrix can be created in SciPy using the coo_matrix class:\n\n\nfrom scipy import sparse\n\n# Create a sparse COO matrix\ni = [0, 0, 1, 2, 4] # row indices\nj = [0, 4, 2, 1, 0] # column indices\ndata = [7, 1, 2, 3, 4] # nonzero values\ncoo = sparse.coo_matrix((data, (i, j)), shape=(5, 5))\ncoo\n\n&lt;COOrdinate sparse matrix of dtype 'int64'\n    with 5 stored elements and shape (5, 5)&gt;\n\n\n\nIt is also possible to convert between different sparse formats. For example, to convert a COO matrix to CSR format, you can use the tocsc() method:\n\n\n# Convert COO to CSR format\ncsr = coo.tocsr()\ncsr\n\n&lt;Compressed Sparse Row sparse matrix of dtype 'int64'\n    with 5 stored elements and shape (5, 5)&gt;\n\n\n\nAnd the matrix–vector product is as simple as:\n\n\n# Matrix–vector product\nv = np.array([1, 2, 3, 4, 5])\nw = coo @ v  # same as coo.dot(v)\nw\n\narray([12,  6,  6,  0,  4])\n\n\n\n\n3.5.1 Eigenvalues of Sparse Matrices\nEven with sparse storage, direct methods (dense diagonalization or full factorization) become intractable when the matrix dimension exceeds millions. To extract a few extremal eigenvalues or approximate time evolution, Krylov-subspace approaches (like the Arnoldi algorithm) build a low-dimensional orthonormal basis that captures the action of the operator on a subspace. By repeatedly applying the sparse matrix to basis vectors and orthogonalizing, Arnoldi produces a small Hessenberg matrix whose eigenpairs approximate those of the full operator. This hybrid strategy leverages both memory-efficient storage and iterative linear algebra to access spectral properties of huge quantum systems.\nTo approximate a few eigenvalues of a large, sparse matrix \\(A\\), SciPy’s eigs implements the Arnoldi algorithm. Under the hood it builds an \\(m\\)-dimensional Krylov basis. More precisely, given a starting vector \\(v_1\\) with \\(\\|v_1\\|_2 = 1\\), the \\(m\\)‑dimensional Krylov subspace is\n\\[\n\\mathcal{K}_m(A, v_1) = \\operatorname{span}\\{v_1, Av_1, A^{2}v_1, \\dots, A^{m-1}v_1\\}.\n\\]\nThe Arnoldi iteration produces the decomposition\n\\[\nA V_m = V_m H_m + h_{m+1,m}\\, v_{m+1} e_m^{\\top},\n\\]\nwhere\n\n\\(V_m = [v_1, \\dots, v_m]\\) has orthonormal columns,\n\\(H_m\\) is an \\(m \\times m\\) upper‑Hessenberg matrix,\n\\(e_m\\) is the \\(m\\)‑th canonical basis vector.\n\nThe eigenvalues of \\(H_m\\) are called Ritz values; they approximate eigenvalues of \\(A\\). As \\(m\\) grows, the approximation improves. In practice we combine Arnoldi with a restart strategy (after reaching a given \\(m\\) we keep the most accurate Ritz vectors and build a fresh Krylov basis). SciPy’s scipy.sparse.linalg.eigs wrapper uses the implicitly restarted Arnoldi method from ARPACK.\nAs a pseudo-code, the Arnoldi algorithm can be summarized as follows:\n\nPick a random vector \\(v\\) and normalize it.\nFor \\(j = 1, \\dots, m\\)\n\n\\(w = A v_j\\)\nOrthogonalize: \\[h_{i,j} = v_i^{\\dagger} w, \\quad w \\leftarrow w - h_{i,j} v_i \\quad (i = 1, \\dots, j)\\]\n\\(h_{j+1,j} = \\|w\\|_2\\).\nIf \\(h_{j+1,j} = 0\\), stop (the Krylov subspace is invariant).\n\\(v_{j+1} = w / h_{j+1,j}\\).\n\n\nThe cost is \\(m\\) sparse matrix–vector products and \\(\\mathcal{O}(m^2 n)\\) scalar operations for orthogonalization (which stays moderate when \\(m \\ll n\\)).\nHere’s a concrete example:\n\n\nfrom scipy.sparse.linalg import eigs\n\n# Compute the 2 largest-magnitude eigenvalues of coo\nvals, vecs = eigs(coo, k=2)\ndisplay(\"Sparse eigenvalues:\", vals)\n\n'Sparse eigenvalues:'\n\n\narray([7.53112887+0.j, 2.44948974+0.j])",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Algebra with NumPy and SciPy</span>"
    ]
  },
  {
    "objectID": "lecture1/numba_and_jax.html",
    "href": "lecture1/numba_and_jax.html",
    "title": "4  Speeding up Python for Linear Algebra Tasks",
    "section": "",
    "text": "4.1 Numba: Just-In-Time Compilation\nPython is easy to read, but pure-Python loops can be slow if you do not leverage optimized libraries (BLAS, LAPACK). Here we explore two tools—Numba and JAX—to accelerate common linear algebra operations.\nNumba uses LLVM to compile Python functions to machine code at runtime. Key points:\nExample: Matrix–Vector Multiplication\nfrom numba import njit\nimport numpy as np\nimport time # for timing\n\n@njit\ndef matvec(A, x):\n    m, n = A.shape\n    y = np.zeros(m)\n    for i in range(m):\n        temp = 0.0\n        for j in range(n):\n            temp += A[i, j] * x[j]\n        y[i] = temp\n    return y\n\n# Prepare data\ndim = 500\nA = np.random.rand(dim, dim)\nx = np.random.rand(dim)\n\n# Using NumPy's dot product\nstart = time.time()\ny0 = A @ x\nend = time.time()\nprint(\"NumPy time (ms): \", 1e3*(end - start))\n\n# Using Numba's compiled function\ny0 = matvec(A, x) # First call for compilation\n\nstart = time.time()\ny1 = matvec(A, x)\nend = time.time()\nprint(\"Numba time (ms): \", 1e3*(end - start))\n\nNumPy time (ms):  0.35858154296875\nNumba time (ms):  0.2923011779785156\nIn practice, Numba can speed up this looped version by 10×–100× compared to pure Python, approaching the speed of NumPy’s optimized routines. The reader is encouraged to try the code without the @njit decorator to see the difference in performance.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Speeding up Python for Linear Algebra Tasks</span>"
    ]
  },
  {
    "objectID": "lecture1/numba_and_jax.html#numba-just-in-time-compilation",
    "href": "lecture1/numba_and_jax.html#numba-just-in-time-compilation",
    "title": "4  Speeding up Python for Linear Algebra Tasks",
    "section": "",
    "text": "Decorators: Use @njit (nopython mode) for best speed.\n\nType inference: Numba infers types on first run, then compiles specialized code.\n\nCompilation overhead: The first call incurs compilation time; subsequent calls are fast.\n\nObject mode vs nopython mode: Always aim for nopython mode to avoid Python object overhead.\n\n\nJIT Workflow 1. Call function → type inference → LLVM IR generation.\n2. LLVM IR → machine code (cached).\n3. Subsequent calls use cached machine code.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Speeding up Python for Linear Algebra Tasks</span>"
    ]
  },
  {
    "objectID": "lecture1/numba_and_jax.html#jax-xla-compilation-and-automatic-differentiation",
    "href": "lecture1/numba_and_jax.html#jax-xla-compilation-and-automatic-differentiation",
    "title": "4  Speeding up Python for Linear Algebra Tasks",
    "section": "4.2 JAX: XLA Compilation and Automatic Differentiation",
    "text": "4.2 JAX: XLA Compilation and Automatic Differentiation\nJAX is a high-performance library from Google Research that extends NumPy with just-in-time compilation and automatic differentiation. It - Compiles array operations via XLA, fusing kernels and reducing Python overhead. - Supports GPU and TPU backends with minimal code changes. - Provides grad for gradients of scalar functions, enabling optimisation and machine-learning tasks. - Offers advanced transformations like vmap (vectorisation) and pmap (parallelism on multiple devices).\nJAX is widely used in deep learning frameworks (e.g. Flax, Haiku), reinforcement learning, and scientific research (including physics simulations), thanks to its blend of speed and flexibility.\n\n4.2.1 A Quick Overview of Automatic Differentiation\nAutomatic differentiation (AD) is a family of techniques to compute exact derivatives of functions defined by computer programs. Unlike symbolic differentiation (which can lead to expression swell) or numerical finite-difference (which suffers from truncation and round-off error), AD exploits the fact that any complex function is ultimately composed of a finite set of elementary operations (addition, multiplication, sin, exp, …) whose derivatives are known exactly.\n\nLimitations of Finite Differences\nA common finite-difference formula for a scalar function \\(f(x)\\) is the central difference\n\\[\n\\frac{df}{dx}(x)\\approx \\frac{f(x+h)-f(x-h)}{2h},\n\\]\nwith local truncation error \\(\\mathcal{O}(h^2)\\). However, this approach has important limitations:\n\nTruncation vs. round-off: If \\(h\\) is too large, the \\(\\mathcal{O}(h^2)\\) term dominates. If \\(h\\) is too small, floating-point cancellation makes the numerator \\(f(x+h)-f(x-h)\\) inaccurate.\nCost with many parameters: For \\(f:\\mathbb{R}^n\\to\\mathbb{R}\\), the gradient component \\(i\\) is\n\\[\n\\frac{\\partial f}{\\partial x_i}(\\mathbf{x})\\approx \\frac{f(\\mathbf{x}+h\\mathbf{e}_i)-f(\\mathbf{x}-h\\mathbf{e}_i)}{2h}.\n\\]\nComputing all \\(n\\) components requires \\(2n\\) evaluations of \\(f\\), so the cost scales as \\(\\mathcal{O}(n)\\) in \\(f\\)-calls. For large \\(n\\) (many parameters), this becomes prohibitive.\nNon-smooth or branching code: When \\(f\\) contains control flow or non-differentiable operations, finite differences may give misleading or undefined results.\n\n\n\nAutomatic Differentiation and the Chain Rule\nAutomatic differentiation (AD) applies the chain rule to each elementary operation in code (addition, multiplication, sin, exp, etc.), yielding exact derivatives up to floating-point precision. For a composition\n\\[\nu = g(x),\\quad y = f(u),\n\\]\nAD uses the chain rule:\n\\[\n\\frac{dy}{dx} = \\frac{df}{du}\\frac{dg}{dx}.\n\\]\nIn more complex nests, e.g.\n\\[\nv = h(u),\\quad u = g(x),\\quad y = f(v),\n\\]\nwe get\n\\[\n\\frac{dy}{dx} = \\frac{df}{dv}\\frac{dh}{du}\\frac{dg}{dx}.\n\\]\nAD comes in two modes:\n\nForward mode (propagate derivatives from inputs to outputs).\nReverse mode (propagate sensitivities from outputs back to inputs).\n\nJAX implements both and selects the most efficient strategy automatically.\n\n\nComparing Accuracy: AD vs Finite Differences\nBelow is a Quarto code cell that plots the error of finite differences (varying step size \\(h\\)) and automatic differentiation against the true derivative of \\(f(x) = e^{\\sin(x)}\\) at \\(x=1.0\\).\n\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt # for plotting\n\n# Set JAX to use 64-bit floats\njax.config.update(\"jax_enable_x64\", True)\n\n# Define function and true derivative\ndef f_np(x):\n    return np.exp(np.sin(x))\n\ndef df_true(x):\n    return np.cos(x) * np.exp(np.sin(x))\n\n# Point of evaluation\nx0 = 1.0\n\n# Finite-difference errors for varying h\nhs = np.logspace(-8, -1, 50)\nerrors_fd = []\nfor h in hs:\n    df_fd = (f_np(x0 + h) - f_np(x0 - h)) / (2 * h)\n    errors_fd.append(abs(df_fd - df_true(x0)))\n\n# Automatic differentiation error (constant)\ndf_ad = jax.grad(lambda x: jnp.exp(jnp.sin(x)))(x0)\nerror_ad = abs(np.array(df_ad) - df_true(x0))\n\nprint(f\"AD error: {error_ad}\")\nprint(f\"FD minimum error: {min(errors_fd)}\")\n\n# Plot\nfig, ax = plt.subplots()\nax.loglog(hs, errors_fd, marker=\"o\")\nax.set_xlabel(\"Step size $h$\")\nax.set_ylabel(\"Error of Finite Differences\")\nplt.show()\n\nAD error: 0.0\nFD minimum error: 7.006839553014288e-12\n\n\n\n\n\n\n\n\n\nThis plot illustrates that finite differences achieve minimal error at an optimal \\(h\\), but degrade for too large or too small \\(h\\), while AD remains accurate to machine precision regardless of step size.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Speeding up Python for Linear Algebra Tasks</span>"
    ]
  },
  {
    "objectID": "lecture1/numba_and_jax.html#why-computing-gradients-is-important-in-quantum-physics",
    "href": "lecture1/numba_and_jax.html#why-computing-gradients-is-important-in-quantum-physics",
    "title": "4  Speeding up Python for Linear Algebra Tasks",
    "section": "4.3 Why Computing Gradients Is Important in Quantum Physics",
    "text": "4.3 Why Computing Gradients Is Important in Quantum Physics\nIn quantum physics, many problems reduce to optimizing parameters in a model or a control protocol. Computing gradients of a cost function with respect to these parameters is essential for efficient and reliable optimization.\n\nVariational quantum algorithms: In methods like the variational quantum eigensolver (VQE)(Peruzzo et al. 2014), a parametrised quantum state \\(|\\psi(\\boldsymbol{\\theta})\\rangle\\) depends on parameters \\(\\boldsymbol{\\theta}=(\\theta_1,\\dots,\\theta_n)\\). One minimises the expectation \\[\nE(\\boldsymbol{\\theta}) = \\langle\\psi(\\boldsymbol{\\theta})|\\hat H|\\psi(\\boldsymbol{\\theta})\\rangle.\n\\]\nGradient-based methods require\n\\[\n\\frac{\\partial E}{\\partial \\theta_i} = \\frac{\\partial}{\\partial \\theta_i} \\langle\\psi(\\boldsymbol{\\theta})|\\hat H|\\psi(\\boldsymbol{\\theta})\\rangle.\n\\]\nAD enables exact evaluation of these derivatives through the quantum circuit parameters, improving convergence compared to gradient-free methods.\nQuantum optimal control(D’Alessandro 2021; Khaneja et al. 2005): One shapes control fields \\(u(t)\\) in the Hamiltonian \\[\n\\hat H(t; u) = \\hat H_0 + \\sum_i u_i(t) \\hat H_i\n\\]\nto drive the system from an initial state \\(|\\psi_0\\rangle\\) to a target \\(|\\psi_T\\rangle\\). A typical cost function is\n\\[\nJ[u] = 1 - |\\langle\\psi_T|\\mathcal U_T[u]|\\psi_0\\rangle|^2,\n\\]\nwhere \\(\\mathcal U_T[u]\\) is the time-ordered evolution. Computing gradients \\(\\delta J/\\delta u_i(t)\\) is needed for gradient-ascent pulse engineering (GRAPE) algorithms. AD can differentiate through time-discretised propagators and ODE solvers, automating derivation of \\(\\delta J/\\delta u_i(t)\\) and providing machine-precision gradients for faster convergence.\nParameter estimation and tomography(Lvovsky and Raymer 2009): Maximum-likelihood estimation for quantum states or processes often involves maximising a log-likelihood \\(L(\\boldsymbol{\\theta})\\). Gradients speed up estimation and enable standard optimisers (e.g. L-BFGS).\n\nBy providing exact, efficient gradients even through complex quantum simulations (time evolution, measurement models, noise), automatic differentiation (via JAX or similar frameworks) has become a key tool in modern quantum physics research.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Speeding up Python for Linear Algebra Tasks</span>"
    ]
  },
  {
    "objectID": "lecture1/numba_and_jax.html#summary",
    "href": "lecture1/numba_and_jax.html#summary",
    "title": "4  Speeding up Python for Linear Algebra Tasks",
    "section": "4.4 Summary",
    "text": "4.4 Summary\n\nNumba: Best for speeding up existing NumPy loops with minimal code changes. Ideal when you do not need gradients or accelerators.\nJAX: Ideal for optimisation tasks requiring gradients, large-scale batch operations, or GPU/TPU acceleration. The XLA compiler often outperforms loop-based JIT for fused kernels.\n\n\n\n\n\nD’Alessandro, Domenico. 2021. Introduction to Quantum Control and Dynamics. Chapman; Hall/CRC. https://doi.org/10.1201/9781003051268.\n\n\nKhaneja, Navin, Timo Reiss, Cindie Kehlet, Thomas Schulte-Herbrüggen, and Steffen J. Glaser. 2005. “Optimal Control of Coupled Spin Dynamics: Design of NMR Pulse Sequences by Gradient Ascent Algorithms.” Journal of Magnetic Resonance 172 (2): 296–305. https://doi.org/10.1016/j.jmr.2004.11.004.\n\n\nLvovsky, A. I., and M. G. Raymer. 2009. “Continuous-Variable Optical Quantum-State Tomography.” Rev. Mod. Phys. 81 (March): 299–332. https://doi.org/10.1103/RevModPhys.81.299.\n\n\nPeruzzo, Alberto, Jarrod McClean, Peter Shadbolt, Man-Hong Yung, Xiao-Qi Zhou, Peter J. Love, Alán Aspuru-Guzik, and Jeremy L. O’Brien. 2014. “A Variational Eigenvalue Solver on a Photonic Quantum Processor.” Nature Communications 5 (1). https://doi.org/10.1038/ncomms5213.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Speeding up Python for Linear Algebra Tasks</span>"
    ]
  },
  {
    "objectID": "lecture1/ordinary_differential_equations.html",
    "href": "lecture1/ordinary_differential_equations.html",
    "title": "5  Ordinary Differential Equations",
    "section": "",
    "text": "5.1 General Definition and Examples\nAn ordinary differential equation (ODE) is an equation involving functions of one independent variable (for instance, time) and its derivatives. In the simplest scenario, suppose we have an unknown function \\(y(t)\\). A first-order ODE can be written as:\n\\[\n\\frac{dy(t)}{dt} = f\\bigl(y(t), t\\bigr),\n\\]\nwhere \\(f\\) is a known function, and \\(y(t)\\) is the unknown to be determined. Higher-order ODEs can often be recast as systems of first-order ODEs by introducing additional variables for the higher derivatives.\nTo see how ODEs arise in physical scenarios, consider Newton’s second law, \\(m\\,\\frac{d^2 x}{dt^2} = F(x,t)\\). This second-order ODE can be reduced to a system of two first-order ODEs by introducing an auxiliary variable for velocity \\(v(t) = \\frac{dx}{dt}\\). Then we have:\n\\[\n\\begin{cases}\n    \\frac{dx}{dt} = v,\\\\\n    \\frac{dv}{dt} = \\frac{F(x,t)}{m}.\n  \\end{cases}\n\\]\nIn quantum mechanics, the time-dependent Schrödinger equation\n\\[\ni\\hbar \\,\\frac{d}{dt}\\ket{\\psi(t)} = \\hat{H}\\,\\ket{\\psi(t)}\n\\] can be viewed as a first-order ODE in the Hilbert space: the role of \\(\\ket{\\psi(t)}\\) is analogous to \\(y(t)\\), and \\(-\\frac{i}{\\hbar}\\,\\hat{H}\\) plays the role of \\(f(\\,\\cdot\\,,t)\\) (assuming a time-independent \\(\\hat{H}\\)). This analogy suggests that the Schrödinger equation can be treated using standard ODE solution techniques or, in more complicated cases, numerical integration.\nA linear ODE has the form: \\(\\frac{d\\mathbf{y}(t)}{dt} = A\\,\\mathbf{y}(t) + \\mathbf{b}(t),\\) where \\(\\mathbf{y}(t)\\) is a vector function of time, \\(A\\) is a constant (or possibly time-dependent) matrix, and \\(\\mathbf{b}(t)\\) is a known inhomogeneous term. If \\(\\mathbf{b}(t) = \\mathbf{0}\\), the equation is said to be homogeneous.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Ordinary Differential Equations</span>"
    ]
  },
  {
    "objectID": "lecture1/ordinary_differential_equations.html#solving-linear-odes-by-diagonalizing-the-system-matrix",
    "href": "lecture1/ordinary_differential_equations.html#solving-linear-odes-by-diagonalizing-the-system-matrix",
    "title": "5  Ordinary Differential Equations",
    "section": "5.2 Solving Linear ODEs by Diagonalizing the System Matrix",
    "text": "5.2 Solving Linear ODEs by Diagonalizing the System Matrix\nA common case in quantum mechanics and in classical physics is the linear homogeneous system:\n\\[\n\\frac{d\\mathbf{y}(t)}{dt} = A\\,\\mathbf{y}(t),\n\\quad\n\\mathbf{y}(0) = \\mathbf{y}_0,\n\\tag{5.1}\\]\nwhere \\(A\\) is a constant \\(n\\times n\\) matrix, and \\(\\mathbf{y}_0\\) is the initial condition.\n\n5.2.1 Eigenvalue Decomposition\nIf \\(A\\) is diagonalizable, we can write: \\[A = V \\, D \\, V^{-1},\\] where \\(D\\) is a diagonal matrix whose entries are the eigenvalues \\(\\lambda_i\\) of \\(A\\), and the columns of \\(V\\) are the corresponding eigenvectors. Define: \\[\\mathbf{z}(t) = V^{-1}\\,\\mathbf{y}(t).\\] Then, plugging this into Equation 5.1, we get\n\\[\n\\frac{d\\mathbf{z}(t)}{dt}\n  = V^{-1}\\, \\frac{d\\mathbf{y}(t)}{dt}\n  = V^{-1}\\, A\\, \\mathbf{y}(t)\n  = V^{-1}\\, (V \\, D \\, V^{-1})\\, \\mathbf{y}(t)\n  = D \\, \\mathbf{z}(t).\n\\]\nHence, in the \\(\\mathbf{z}\\)-coordinates, the system becomes a set of \\(n\\) uncoupled first-order ODEs:\n\\[\n\\frac{dz_i}{dt} = \\lambda_i\\, z_i(t),\n  \\quad\n  \\text{for } i = 1,\\dots,n.\n\\]\nThese have the well-known solutions:\n\\[\nz_i(t) = z_i(0)\\,e^{\\lambda_i t}.\n\\] To enforce the initial condition \\(\\mathbf{y}(0) = \\mathbf{y}_0\\), we note that \\(\\mathbf{z}(0) = V^{-1}\\,\\mathbf{y}_0\\). Hence, transforming back, we get:\n\\[\n\\mathbf{y}(t) = V\\, \\mathbf{z}(t)\n  = V\n  \\begin{pmatrix}\n    z_1(0) \\, e^{\\lambda_1 t}\\\\\n    z_2(0) \\, e^{\\lambda_2 t}\\\\\n    \\vdots\\\\\n    z_n(0) \\, e^{\\lambda_n t}\n  \\end{pmatrix}\n  = V \\, e^{D t} \\, V^{-1} \\, \\mathbf{y}_0.\n\\]\nTherefore, we obtain the compact form:\n\\[\n\\mathbf{y}(t) = e^{A t}\\,\\mathbf{y}_0,\n\\]\nor, equivalently,\n\\[\n\\mathbf{y}(t) = V\n  \\begin{pmatrix}\n  e^{\\lambda_1} & & \\\\\n  & \\ddots & \\\\\n  & & e^{\\lambda_n}\n  \\end{pmatrix}\n  V^{-1}\n  \\mathbf{y}_0 \\, .\n\\]\nIn the case of \\(A\\) Hermitian, the time evolution can be expanded as\n\\[\n\\mathbf{y}(t) = \\sum_i (\\mathbf{v}_i^\\dagger \\cdot \\mathbf{y}_0) \\mathbf{v}_i \\, e^{\\lambda_i t} \\, ,\n\\]\nwhere \\(\\mathbf{v}_i\\) are the eigenvectors of the matrix.\n\n\n5.2.2 Relation to the Schrödinger Equation\nWhen dealing with the time-dependent Schrödinger equation for a time-independent Hamiltonian \\(\\hat{H}\\), we can represent \\(\\ket{\\psi(t)}\\) in a certain basis, turning the Schrödinger equation into:\n\\[\ni \\hbar \\,\\frac{d}{dt} \\mathbf{c}(t) = H \\,\\mathbf{c}(t),\n\\]\nor equivalently,\n\\[\n\\frac{d\\mathbf{c}(t)}{dt} = -\\frac{i}{\\hbar} \\, H \\,\\mathbf{c}(t).\n\\]\nWe can identify \\(A = -\\frac{i}{\\hbar}\\, H\\). If \\(H\\) is diagonalizable (e.g., Hermitian matrices always have a complete set of orthonormal eigenvectors), then the above solution technique via diagonalization applies. The resulting exponential solution corresponds to the usual \\(e^{-\\frac{i}{\\hbar} H t}\\) operator that defines unitary time evolution in quantum mechanics.\n\nExample: Harmonic Oscillator\nThe harmonic oscillator is described by the second-order ODE:\n\\[\n\\frac{d^2 x}{dt^2} + \\omega^2 x = 0,\n\\]\nwhich can be rewritten as a first-order system:\n\\[\n\\begin{cases}\n\\frac{dx}{dt} = v, \\\\\n\\frac{dv}{dt} = -\\omega^2 x.\n\\end{cases}\n\\]\nor, in matrix form:\n\\[\n\\frac{d}{dt} \\begin{pmatrix} x \\\\ v \\end{pmatrix} = \\begin{pmatrix} 0 & 1 \\\\ -\\omega^2 & 0 \\end{pmatrix} \\begin{pmatrix} x \\\\ v \\end{pmatrix}.\n\\]\nBy diagonalizing the matrix, we can find the solution to this system.\n\nimport numpy as np\n\nomega = 2.0\n\n# Define the 2x2 matrix A\nA = np.array([[0.0,  1.0],\n              [-omega**2, 0.0]])\n\n# Initial condition: x(0) = 1, v(0) = 0.5\nx0 = np.array([1.0, 0.5])\n\n# Diagonalize A\neigs, V = np.linalg.eig(A)\nV_inv = np.linalg.inv(V)\n\nz0 = V_inv @ x0\n\n# Define a time array\nt_points = np.linspace(0, 2, 200)\n\nX_t = []\nfor t in t_points:\n    # Compute the solution at time t\n    z_t = np.diag(np.exp(eigs * t)) @ z0\n    x_t = V @ z_t # Transform back to original coordinates\n    X_t.append(x_t)\n\nX_t = np.array(X_t).real\n\nprint(\"x(2) = \", X_t[-1])\n\nx(2) =  [-0.84284424  1.18678318]\n\n\nWe have: - A: the system matrix. - y0: initial condition \\(\\mathbf{y}(0)\\). - We diagonalize \\(A\\) to find \\(A = V D V^{-1}\\). - Then \\(\\exp(A t) = V \\exp(D t) V^{-1}\\).\nIf you run the code, you’ll see the final value of \\(\\mathbf{y}(2)\\).\nWe could also visualize the time evolution:\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nax.plot(t_points, X_t[:, 0], label=\"$x(t)$\")\nax.plot(t_points, X_t[:, 1], label=\"$v(t)$\")\nax.plot(t_points, 0.5 * omega**2 * X_t[:, 0]**2 + 0.5 * X_t[:, 1]**2,\n        label=\"$E(t)$\")\nax.set_xlabel(\"Time\")\nax.set_ylabel(\"$y(t)$\")\nax.legend()\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Ordinary Differential Equations</span>"
    ]
  },
  {
    "objectID": "lecture1/ordinary_differential_equations.html#numerical-solution-via-the-euler-method",
    "href": "lecture1/ordinary_differential_equations.html#numerical-solution-via-the-euler-method",
    "title": "5  Ordinary Differential Equations",
    "section": "5.3 Numerical Solution via the Euler Method",
    "text": "5.3 Numerical Solution via the Euler Method\nIn many realistic situations (e.g., time-dependent Hamiltonians, nonlinear effects, large dissipative systems described by master equations), finding an exact analytic solution can be very challenging or impossible. We then rely on numerical methods to solve ODEs.\n\n5.3.1 Forward Euler Method\nOne of the simplest methods is the forward Euler method. Suppose we want to solve: \\[\n\\frac{d\\mathbf{y}(t)}{dt} = \\mathbf{f}(\\mathbf{y}(t), t),\n  \\quad\n  \\mathbf{y}(0) = \\mathbf{y}_0.\n\\]\nWe discretize time into steps \\(t_n = n\\,h\\) with step size \\(h\\). The Euler method approximates the derivative at \\(t_n\\) by a difference quotient:\n\\[\n\\frac{d\\mathbf{y}(t_n)}{dt} \\approx \\frac{\\mathbf{y}_{n+1} - \\mathbf{y}_n}{h}.\n\\]\nHence, the system becomes the algebraic update:\n\\[\n\\mathbf{y}_{n+1} = \\mathbf{y}_n + h\\,\\mathbf{f}(\\mathbf{y}_n, t_n),\n\\]\nwith \\(\\mathbf{y}_0\\) known. After iterating this rule for \\(n=0,1,2,\\dots\\), we obtain an approximate solution at discrete times \\(t_n\\).\n\n\n5.3.2 Stability Criterion for the Euler Method\nWhile the Euler method is straightforward, it can be susceptible to numerical instability when the system has rapidly decaying or oscillatory modes. For example, consider the test equation \\(\\frac{dy}{dt} = \\lambda\\, y,\\) where \\(\\lambda\\) is a (possibly complex) constant. The exact solution is \\(y(t) = y(0)\\,e^{\\lambda t}\\). In the Euler scheme, we get\n\\[\ny_{n+1} = y_n + h\\,\\lambda\\,y_n = (1 + h\\,\\lambda)\\, y_n.\n\\]\nThus,\n\\[\ny_n = (1 + h\\,\\lambda)^n\\, y_0.\n\\]\nFor the method to be stable (i.e., for \\(y_n\\) to remain bounded in the limit \\(n\\to\\infty\\) when the exact solution is stable), we require: \\[|1 + h\\,\\lambda| &lt; 1,\\] when the real part of \\(\\lambda\\) is negative (dissipative system). If this condition is not met, the numerical solution may diverge even though the true solution decays exponentially. In practice, one must choose the time step \\(h\\) small enough to satisfy such stability constraints.\n\nExample: Harmonic Oscillator with Euler Method\nLet’s now implement the forward Euler method for a simpler ODE. Consider the same harmonic oscillator, Euler’s method approximates the evolution as:\n\\[\n\\begin{pmatrix}\n  x_{n+1} \\\\ v_{n+1}\n\\end{pmatrix} \\simeq\n\\begin{pmatrix}\n  x_n \\\\ v_n\n\\end{pmatrix} + h \\,\n\\begin{pmatrix}\n  0 & 1 \\\\ -\\omega^2 & 0\n\\end{pmatrix} \\,\n\\begin{pmatrix}\n  x_n \\\\ v_n\n\\end{pmatrix},\n\\]\nwhere \\(h\\) is the time step.\n\nh = 0.01\n\nX_t_euler = np.zeros((len(t_points), 2))\nX_t_euler[0] = x0\nfor n in range(len(t_points) - 1):\n    X_t_euler[n+1] = X_t_euler[n] + h * A @ X_t_euler[n]\n\nfig, ax = plt.subplots()\nax.plot(t_points, X_t[:, 0], label=\"$x(t)$ (exact)\")\nax.plot(t_points, X_t_euler[:, 0], label=\"$x(t)$ (Euler)\", linestyle='--')\nax.plot(t_points, X_t[:, 1], label=\"$v(t)$ (exact)\")\nax.plot(t_points, X_t_euler[:, 1], label=\"$v(t)$ (Euler)\", linestyle='--')\nax.set_xlabel(\"Time\")\nax.set_ylabel(\"$y(t)$\")\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\nHere we see how the Euler solution compares to the exact solution obtained via diagonalization. Notice that using a large time step \\(h\\) can cause the Euler solution to deviate significantly from the exact decay (and may even diverge if \\(|1 - \\lambda h| \\ge 1\\)).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Ordinary Differential Equations</span>"
    ]
  },
  {
    "objectID": "lecture1/ordinary_differential_equations.html#applying-these-methods-to-the-schrödinger-equation",
    "href": "lecture1/ordinary_differential_equations.html#applying-these-methods-to-the-schrödinger-equation",
    "title": "5  Ordinary Differential Equations",
    "section": "5.4 Applying These Methods to the Schrödinger Equation",
    "text": "5.4 Applying These Methods to the Schrödinger Equation\n\nTime-Independent Hamiltonian\nFor a time-independent Hamiltonian \\(\\hat{H}\\), the Schrödinger equation in vector form reads:\n\\[\ni\\hbar \\,\\frac{d\\mathbf{c}(t)}{dt} = H\\,\\mathbf{c}(t).\n\\]\nBy setting \\(A = -\\tfrac{i}{\\hbar} H\\), we recognize that this is a linear ODE. If \\(H\\) (or \\(A\\)) is diagonalizable, its eigen-decomposition yields an analytic solution. In quantum optics, these solutions describe unitary time evolution of a closed system, often expressed as:\n\\[\n\\mathbf{c}(t) = e^{-\\tfrac{i}{\\hbar}H t} \\,\\mathbf{c}(0).\n\\]\n\n\n5.4.1 Time-Dependent Hamiltonian\nWhen \\(\\hat{H}(t)\\) varies explicitly with time, one no longer has a simple exponential solution. Instead, one can divide the time interval of interest into many small sub-intervals and approximate \\(\\hat{H}(t)\\) as constant in each interval. This procedure is related to the time-ordered exponential, but from a numerical perspective, we can simply implement a step-by-step integration (e.g., Euler, Runge–Kutta, or other higher-order methods) to construct \\(\\ket{\\psi(t_{n+1})}\\) from \\(\\ket{\\psi(t_n)}\\).\n\n\n5.4.2 Open Quantum Systems\nIn open quantum systems, the evolution of the density matrix \\(\\rho(t)\\) is often governed by the master equation:\n\\[\n\\frac{d\\rho(t)}{dt} = \\mathcal{L}[\\rho(t)],\n\\] where \\(\\mathcal{L}\\) is the so-called Liouvillian superoperator, which could contain both Hamiltonian (coherent) parts and dissipative terms. Numerically, one can vectorize \\(\\rho(t)\\) (flattening the matrix into a vector) and represent \\(\\mathcal{L}\\) as a matrix \\(\\mathcal{L}_{\\mathrm{mat}}\\). Then, the equation again has the familiar linear form:\n\\[\n\\frac{d\\mathbf{r}(t)}{dt} = \\mathcal{L}_{\\mathrm{mat}}\\;\\mathbf{r}(t).\n\\]\nHence, the same techniques (matrix diagonalization for analytical solutions, or time stepping methods like Euler, Runge–Kutta, etc. for numerical solutions) remain valid.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Ordinary Differential Equations</span>"
    ]
  },
  {
    "objectID": "lecture1/ordinary_differential_equations.html#conclusion",
    "href": "lecture1/ordinary_differential_equations.html#conclusion",
    "title": "5  Ordinary Differential Equations",
    "section": "5.5 Conclusion",
    "text": "5.5 Conclusion\nIn summary:\n\nAn Ordinary Differential Equation (ODE) involves a function of one variable and its derivatives.\nWhen an ODE is linear and time-independent, one can analytically solve it by diagonalizing the system matrix.\nFor more complicated (time-dependent or nonlinear) problems, numerical integration methods such as the Euler method can be applied.\nThe Euler method is conceptually simple but demands careful choice of time step to ensure stability, particularly when the system matrix has eigenvalues with large negative real parts or when fast decaying/oscillatory modes are present.\nThese ideas are directly applicable to quantum mechanical systems such as the Schrödinger equation or master equations for open systems. In the Schrödinger equation, diagonalization corresponds to finding energy eigenstates and frequencies, while in open quantum systems, vectorization plus diagonalization or numerical iteration handles both coherent and dissipative dynamics.\n\nThroughout the course, we will leverage these fundamental methods—both analytical techniques (e.g., diagonalization) and numerical approaches (e.g., Euler and more sophisticated solvers)—to simulate quantum systems efficiently and accurately.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Ordinary Differential Equations</span>"
    ]
  },
  {
    "objectID": "bibliography.html",
    "href": "bibliography.html",
    "title": "Bibliography",
    "section": "",
    "text": "Campaioli, Francesco, Jared H. Cole, and Harini Hapuarachchi. 2024.\n“Quantum Master Equations: Tips and Tricks for Quantum Optics,\nQuantum Computing, and Beyond.” PRX Quantum 5 (June):\n020202. https://doi.org/10.1103/PRXQuantum.5.020202.\n\n\nD’Alessandro, Domenico. 2021. Introduction to\nQuantum Control and Dynamics. Chapman; Hall/CRC. https://doi.org/10.1201/9781003051268.\n\n\nJohansson, J. R., P. D. Nation, and Franco Nori. 2012. “QuTiP: An open-source Python framework for the dynamics\nof open quantum systems.” Computer Physics\nCommunications 183 (8): 1760–72. https://doi.org/10.1016/j.cpc.2012.02.021.\n\n\nKhaneja, Navin, Timo Reiss, Cindie Kehlet, Thomas Schulte-Herbrüggen,\nand Steffen J. Glaser. 2005. “Optimal Control of Coupled Spin\nDynamics: Design of NMR Pulse Sequences by Gradient Ascent\nAlgorithms.” Journal of Magnetic Resonance 172 (2):\n296–305. https://doi.org/10.1016/j.jmr.2004.11.004.\n\n\nLambert, Neill, Eric Giguère, Paul Menczel, Boxi Li, Patrick Hopf,\nGerardo Suárez, Marc Gali, et al. 2024. “QuTiP 5: The Quantum Toolbox in Python.”\narXiv:2412.04705. https://arxiv.org/abs/2412.04705.\n\n\nLvovsky, A. I., and M. G. Raymer. 2009. “Continuous-Variable\nOptical Quantum-State Tomography.” Rev. Mod. Phys. 81\n(March): 299–332. https://doi.org/10.1103/RevModPhys.81.299.\n\n\nMercurio, Alberto, Yi-Te Huang, Li-Xun Cai, Yueh-Nan Chen, Vincenzo\nSavona, and Franco Nori. 2025. “QuantumToolbox.jl: An Efficient Julia\nFramework for Simulating Open Quantum Systems.” arXiv\nPreprint arXiv:2504.21440. https://doi.org/10.48550/arXiv.2504.21440.\n\n\nPeruzzo, Alberto, Jarrod McClean, Peter Shadbolt, Man-Hong Yung, Xiao-Qi\nZhou, Peter J. Love, Alán Aspuru-Guzik, and Jeremy L. O’Brien. 2014.\n“A Variational Eigenvalue Solver on a Photonic Quantum\nProcessor.” Nature Communications 5 (1). https://doi.org/10.1038/ncomms5213.",
    "crumbs": [
      "Bibliography"
    ]
  },
  {
    "objectID": "appendices/quantum-harmonic-oscillator.html",
    "href": "appendices/quantum-harmonic-oscillator.html",
    "title": "Appendix A — The quantum harmonic oscillator",
    "section": "",
    "text": "As an example, here we consider the quantum harmonic oscillator. Classically, the harmonic oscillator is defined as a system subject to the force \\(\\mathbf{F} = - k \\mathbf{r}\\), where \\(k\\) is the elastic constant. In other words, the force is proportional to the displacement from a stable point (in this case the origin).\nFollowing the relation \\(\\mathbf{F} = - \\nabla V(\\mathbf{r})\\), we can say that the corresponding potential is \\(V(\\mathbf{r}) = k/2 \\ \\mathbf{r}^2\\). The solution of the Schrodinger equation\n\\[\n    i \\hbar \\frac{\\partial}{\\partial t} \\Psi(\\mathbf{r}, t) = -\\frac{\\hbar^2}{2 m} \\nabla^2 \\Psi(\\mathbf{r}, t) + V(\\mathbf{r}) \\Psi(\\mathbf{r}, t) \\, ,\n\\]\nwhere \\(\\hbar\\) is the reduced Planck constant, \\(m\\) is the mass of the particle, and \\(\\nabla^2\\) is the Laplacian operator, gives us the eigenstates of the system. Considering only the one-dimensional case, we obtain the following eigenstates for the quantum harmonic oscillator:\n\\[\n\\psi_n(x) = \\frac{1}{\\sqrt{2^n n!}} \\left(\\frac{m\\omega}{\\pi \\hbar}\\right)^{1/4} e^{-\\frac{m\\omega x^2}{2\\hbar}} H_n\\left(\\sqrt{\\frac{m\\omega}{\\hbar}} x\\right) \\, ,\n\\tag{A.1}\\]\nwhere \\(\\omega = \\sqrt{k / m}\\) is the resonance frequency of the oscillator and \\(H_n\\) is the \\(n\\)-th Hermite polynomial.\nA useful way to describe the quantum harmonic oscillator is by using the ladder operators\n\\[\\begin{eqnarray}\n    \\hat{a} &=& \\sqrt{\\frac{m \\omega}{2 \\hbar}} \\left( \\hat{x} + i \\frac{1}{m \\omega} \\hat{p} \\right) \\\\\n    \\hat{a}^\\dagger &=& \\sqrt{\\frac{m \\omega}{2 \\hbar}} \\left( \\hat{x} - i \\frac{1}{m \\omega} \\hat{p} \\right) \\, ,\n\\end{eqnarray}\\]\nnote that the position \\(\\hat{x}\\) and conjugate momentum \\(\\hat{p}\\) are operators too. If we now write the eigenstates in Equation 5.1 in the bra-ket notation (\\(\\psi_n \\to \\ket{n}\\)), the ladder operators allow us to move from one eigenstate to the next or previous one:\n\\[\\begin{eqnarray}\n    \\label{eq1: destroy operator action}\n    \\hat{a} \\ket{n} &=& \\sqrt{n} \\ket{n-1} \\\\\n    \\label{eq1: creation operator action}\n    \\hat{a}^\\dagger \\ket{n} &=& \\sqrt{n+1} \\ket{n+1} \\, ,\n\\end{eqnarray}\\]\nand it is straightforward to recognize the creation (\\(\\hat{a}^\\dagger\\)) and annihilation (\\(\\hat{a}\\)) operators. In this framework the system Hamiltonian of the quantum harmonic oscillator becomes \\[\n\\hat{H} = \\hbar \\omega \\left( \\hat{a}^\\dagger \\hat{a} + \\frac{1}{2} \\right) \\, .\n\\]\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import eval_hermite, factorial\n\n# Physical parameters\nm = 1.0\nk = 1.0\nw = np.sqrt(k/m)\nalpha = -np.sqrt(2)   # coherent‐state parameter\n\n# Grid\nbounds = 6.0\nx = np.linspace(-bounds, bounds, 1000)\n\n# n-th eigenfunction of the HO (hbar=1)\ndef psi(n, x):\n    Hn = eval_hermite(n, np.sqrt(m*w) * x)\n    norm = (m*w/np.pi)**0.25 / np.sqrt(2**n * factorial(n))\n    return norm * Hn * np.exp(-m*w*x**2/2)\n\n# Build the first six eigenstates and energies\npsi_n = [psi(n, x) for n in range(6)]\nE_n   = [(n + 0.5) * w for n in range(6)]\n\n# Coherent-state wavefunction (real alpha ⇒ no overall phase)\npsi_coh = (m*w/np.pi)**0.25 * np.exp(- (x - np.sqrt(2)*alpha)**2 / 2)\n\n# Plotting\nfig, ax = plt.subplots()\n\n# 1) potential\nax.plot(x, 0.5*k*x**2, 'k--', lw=2, label=r'$V(x)=\\tfrac12 k x^2$')\n\n# 2) coherent state\nax.fill_between(x, psi_coh, color='gray', alpha=0.5)\nax.plot(x, psi_coh, color='gray', lw=2, label='Coherent state')\n\n# 3) eigenstates offset by E_n\nlines = []\nfor n in range(6):\n    y = psi_n[n] + E_n[n]\n    line, = ax.plot(x, y, lw=2, label=fr'$|{n}\\rangle$')\n    lines.append(line)\n\n# Cosmetics\nax.set_ylim(0, 7)\nax.set_xlabel(r'$x$')\n\n# State labels on the right\nfor n, line in enumerate(lines):\n    ax.text(4.5, E_n[n] + 0.2, rf'$|{n}\\rangle$', color=line.get_color())\n\nax.text(-3.5, 6.5, r\"$V(x)$\")\nax.text(-3, 0.9, r\"$\\ket{\\alpha}$\", color=\"grey\")\nax.annotate(\"\", xy=(-5.5,3.5), xytext=(-5.5,2.5), arrowprops=dict(arrowstyle=\"&lt;-&gt;\"))\nax.text(-5.4, 3, r\"$\\hbar \\omega$\", ha=\"left\", va=\"center\")\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure A.1: First eigenstates of one-dimensional the quantum harmonic oscillator, each of them vertically shifted by the corresponding eigenvalue. The grey-filled curve corresponds to a coherent state with \\(\\alpha=-\\sqrt{2}\\). The used parameter are \\(m=1\\), \\(\\omega=1\\), and \\(\\hbar=1\\).\n\n\n\n\n\nIt is worth introducing the coherent state \\(\\ket{\\alpha}\\) of the harmonic oscillator, defined as the eigenstate of the destroy operator, with eigenvalue \\(\\alpha\\), in other words, \\(\\hat{a} \\ket{\\alpha} = \\alpha \\ket{\\alpha}\\). It can be expressed analytically in terms of the eigenstates of the quantum harmonic oscillator \\[\n\\begin{aligned}\n    \\ket{\\alpha} = e^{-\\frac{1}{2} \\abs{\\alpha}^2} \\sum_{n=0}^{\\infty} \\frac{\\alpha^n}{\\sqrt{n!}} \\ket{n} \\, ,\n\\end{aligned}\n\\]\nand it can be seen as the most classic-like state since it has the minimum uncertainty \\(\\Delta x \\Delta p = \\hbar / 2\\).\nFigure A.1 shows the first eigenstates of the quantum harmonic oscillator, each of them vertically shifted by the respective energy, while the grey-filled curve is a coherent state with \\(\\alpha = - \\sqrt{2}\\). The black dashed curve is the potential, choosing \\(k = 1\\), \\(m = 1\\), and \\(\\hbar = 1\\). It is worth noting that also the groundstate \\(\\ket{0}\\) has a nonzero energy (\\(E_0 = \\hbar \\omega / 2\\)).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The quantum harmonic oscillator</span>"
    ]
  }
]