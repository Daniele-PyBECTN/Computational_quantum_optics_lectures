[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Numerical Methods for Quantum Optics and Open Quantum Systems",
    "section": "",
    "text": "Numerical Methods for Quantum Optics and Open Quantum Systems is a hands-on course that shows you how to model and simulate open quantum systems in quantum optics with Python and QuTiP. The notes mix concise explanations, essential equations, and runnable code cells that work both on your computer and in Google Colab. Everything lives in a Quarto project on GitHub and is published in HTML and PDF for easy reading and collaboration. By the end, you will be able to set up and explore standard problems—such as photon cavities, two-level atoms, and open-system dynamics—using tools you can reuse in research and projects.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Home Page</span>"
    ]
  },
  {
    "objectID": "lecture1/introduction.html",
    "href": "lecture1/introduction.html",
    "title": "2  Introduction",
    "section": "",
    "text": "2.1 Why simulate open quantum systems?\nThe experimental frontier of quantum optics increasingly targets systems that cannot be described by perfectly isolated, unitary dynamics. Photons leak from cavities, solid‑state qubits couple to phonons, and measurement back‑action reshapes quantum states in real time. In these scenarios the open character of the system—the interplay between coherent evolution and irreversible processes—becomes the defining feature, not a perturbation. Analytical solutions exist only for a handful of toy models; to design devices, interpret data, and test conceptual ideas we therefore rely on numerical simulation of open quantum dynamics.\nNumerical methods allow us to:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "lecture1/introduction.html#why-simulate-open-quantum-systems",
    "href": "lecture1/introduction.html#why-simulate-open-quantum-systems",
    "title": "2  Introduction",
    "section": "",
    "text": "Predict observables such as spectra, correlation functions, or entanglement measures before running an experiment.\nPrototype control protocols (e.g., pulse shaping or feedback) that can stabilize fragile quantum states.\nExplore parameter regimes that are inaccessible analytically, revealing new phenomena like dissipative phase transitions or non‑Markovian memory effects.\n\n\n\n\n\n\n\nFigure 2.1: Description of an open quantum system and its practical applications. A quantum system interacts with a macroscopic environment, leading to decoherence and dissipation. The evolution of the system is described the master equation \\(\\dot{\\hat{\\rho}} = \\mathcal{L}_T(t) [\\hat{\\rho}]\\), where \\(\\hat{\\rho}\\) is the density matrix and \\(\\mathcal{L}_T(t)\\) is the Liouville superoperator. The solution can be used to study the steady state and non-equilibrium properties of the system. The theoretical study of open quantum systems offers several tools for modeling spin resonance, optical spectra, and quantum information processing, and their use is certainly not limited to these fields and applications. Reproduced from (Campaioli, Cole, and Hapuarachchi 2024) under a CC BY 4.0 license.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "lecture1/introduction.html#why-python",
    "href": "lecture1/introduction.html#why-python",
    "title": "2  Introduction",
    "section": "2.2 Why Python?",
    "text": "2.2 Why Python?\nPython is not the fastest language for floating‑point arithmetic—compiled languages like C or Fortran still win raw speed benchmarks—but it has become the lingua franca of modern scientific computing. Three qualities make it particularly compelling for our purposes:\n\nExpressiveness – A succinct, readable syntax lowers cognitive overhead and lets us translate mathematical ideas into code quickly.\nRich ecosystem – Numpy, SciPy, Jupyter, Matplotlib, and data‑analysis libraries coexist seamlessly, providing everything from linear algebra kernels to publication‑quality plots.\nCommunity & portability – Tutorials, StackOverflow answers, CI pipelines, and cloud platforms such as Google Colab enable beginners to run the same notebooks locally or on GPUs in the cloud with negligible setup.\n\nMost importantly, Python hosts QuTiP (Quantum Toolbox in Python)(Johansson, Nation, and Nori 2012; Lambert et al. 2024) the de‑facto standard library for simulating open quantum systems. QuTiP wraps efficient C and Fortran back‑ends behind a high‑level interface: you manipulate Qobj instances instead of raw matrices, and you call solvers such as mesolve or mcsolve for Lindblad‑master equations and quantum trajectory simulations, respectively. The package is actively maintained, well documented, and battle‑tested across thousands of research papers.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "lecture1/introduction.html#how-does-python-differ-from-other-mainstream-languages",
    "href": "lecture1/introduction.html#how-does-python-differ-from-other-mainstream-languages",
    "title": "2  Introduction",
    "section": "2.3 How does Python differ from other mainstream languages?",
    "text": "2.3 How does Python differ from other mainstream languages?\n\n\n\n\n\n\n\n\n\nLanguage\nParadigm\nTypical strength\nTypical weakness\n\n\n\n\nC / C++\nCompiled, low‑level\nMaximal performance, fine‑grained memory control\nVerbose, higher barrier to entry, manual parallelization\n\n\nFortran\nCompiled, array‑oriented\nLegacy HPC codes, excellent BLAS/LAPACK bindings\nLimited modern features, smaller community\n\n\nMATLAB\nProprietary, array‑oriented\nIntegrated IDE, built‑in plotting, domain‑specific toolboxes\nLicense cost, closed ecosystem\n\n\nPython\nInterpreted, multi‑paradigm\nReadability, vast open‑source libraries, rapid prototyping\nOverhead of interpreter, GIL limits naive multithreading\n\n\n\nPython balances high‑level productivity with the option to call compiled extensions (via Cython, Numba, or Rust bindings) whenever performance matters.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "lecture1/introduction.html#a-glance-at-julia-and-quantumtoolbox.jl",
    "href": "lecture1/introduction.html#a-glance-at-julia-and-quantumtoolbox.jl",
    "title": "2  Introduction",
    "section": "2.4 A glance at Julia and QuantumToolbox.jl",
    "text": "2.4 A glance at Julia and QuantumToolbox.jl\nWhile Python dominates current scientific computing, it is not the only contender. In recent years, researchers and engineers have been exploring the need for a new programming language—one that combines the performance of compiled languages like C or Fortran with the ease of use and readability of scripting languages like Python or MATLAB. This is the motivation behind Julia.\nJulia promises “C‑like speed with Python‑like syntax” by using just‑in‑time (JIT) compilation and a multiple‑dispatch programming model. Within this language, the package QuantumToolbox.jl(Mercurio et al. 2025) has emerged as a high‑performance analog to QuTiP. It mirrors QuTiP’s API but benefits from Julia’s performance model and native automatic differentiation. Benchmarks already demonstrate significant speed‑ups, especially for large Hilbert spaces and GPU‑accelerated workloads.\nNevertheless, Julia’s ecosystem is still maturing. Its tooling, package stability, and IDE support are evolving rapidly but are not yet as robust as Python’s. Similarly, QuantumToolbox.jl, while powerful, has a smaller user base and fewer educational resources compared to QuTiP. For a course focused on accessibility and broad applicability, we therefore choose to prioritize Python and QuTiP as the more mature and stable learning platform.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "lecture1/introduction.html#course-scope",
    "href": "lecture1/introduction.html#course-scope",
    "title": "2  Introduction",
    "section": "2.5 Course scope",
    "text": "2.5 Course scope\nIn this course we therefore focus on Python + QuTiP. You will learn to:\n\nBuild Hamiltonians and collapse operators in a composable way.\nIntegrate master equations and unravel them into quantum trajectories.\nCompute expectation values, spectra, and correlation functions.\nCouple simulations to optimisation or machine‑learning workflows within the wider Python ecosystem.\n\nWhere Julia can offer useful perspective we will point out parallels, but all hands‑on examples will run in Python notebooks that you can execute locally or on Colab.\n\nTake‑away: Numerical simulation is the microscope of modern quantum optics. Python and QuTiP give us a practical, accessible, and well‑supported platform for that microscope—letting us peer into the dynamics of open quantum systems without getting lost in low‑level details.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "lecture1/introduction.html#first-steps-in-python-lists-loops-and-functions",
    "href": "lecture1/introduction.html#first-steps-in-python-lists-loops-and-functions",
    "title": "2  Introduction",
    "section": "2.6 First steps in Python: lists, loops, and functions",
    "text": "2.6 First steps in Python: lists, loops, and functions\n\n2.6.1 Creating and using lists\nBefore diving into numerical simulations, it’s useful to get acquainted with the basic syntax and features of Python. One of the simplest and most commonly used data structures is the list, which stores a sequence of elements. Lists are flexible—they can contain numbers, strings, or even other lists.\nHere’s how to create and access elements in a list:\n\n\nfruits = ['apple', 'banana', 'cherry']\nprint(f'First fruit: {fruits[0]}')\n\nFirst fruit: apple\n\n\n\n\n\n2.6.2 For loops\nA for loop allows us to iterate through each item in a collection and execute the same block of code for every element. You will use loops constantly—whether you are sweeping parameter values, accumulating results, or analysing datasets—so it is worth seeing the syntax early.\n\n\nfor fruit in fruits:\n    print(f'I like {fruit}')\n\nI like apple\nI like banana\nI like cherry\n\n\n\n\n\n2.6.3 Defining functions\nFunctions bundle reusable logic behind a descriptive name. In quantum‑optics simulations, well‑structured functions help keep notebooks tidy—for instance, collecting the code that builds a Hamiltonian or evaluates an observable in one place. Below is a minimal example that squares a number.\n\n\ndef square(x):\n    return x * x\n\nprint(square(5))\n\n25\n\n\n\n\n\n2.6.4 Lambda (anonymous) functions\nOccasionally we only need a small, throw‑away function—say, as a callback or key in a sort operation. Python’s lambda syntax lets us declare such anonymous functions in a single line, without the ceremony of def.\n\n\nsquare_lambda = lambda x: x * x\nprint(square_lambda(5))\n\n25\n\n\n\n\n\n2.6.5 Complex numbers\nPython has built‑in support for complex numbers, which are represented as a + bj, where a is the real part and b is the imaginary part. This is particularly useful in quantum mechanics, where complex numbers are ubiquitous.\n\n\nz = 1 + 2j\nprint(f'Complex number: {z}')\nprint(f'Real part: {z.real}')\nprint(f'Magnitude: {abs(z)}')\n\nComplex number: (1+2j)\nReal part: 1.0\nMagnitude: 2.23606797749979\n\n\n\n\n\n2.6.6 Why plain Python lists can be slow\nPython lists store references to arbitrary Python objects. Each element carries its own type information and reference count. When you perform arithmetic on list elements, the interpreter must\n\nLook up the byte‑code for each operation.\nResolve types at runtime.\nDispatch to the correct C implementation.\n\nThis per‑element overhead dominates runtime in numerical workloads.\n\n\n2.6.7 Enter numpy\nTo overcome the performance limits of pure‑Python lists, we turn to NumPy, which stores data in contiguous, fixed‑type arrays and dispatches mathematical operations to highly‑optimised C (and often SIMD/GPU) kernels. The example below shows how you can express a million‑element computation in just two vectorised lines.\nnumpy provides fixed‑type, contiguous arrays backed by efficient C (or SIMD/GPU) loops. Operations are dispatched once for the whole array, eliminating Python‑level overhead and unlocking BLAS/LAPACK acceleration.\nAs an example, we can compute the sum of all the elements of a python list, comparing the performance with a numpy array.\n\n\nimport numpy as np\nimport time # Only for benchmarking\n\nmy_list = [i / 1_000_000 for i in range(1_000_000)]\n\nstart = time.time() # start timer\nsum_list = sum(my_list)  # sum using Python list\nend = time.time()  # end timer\nprint(f'Sum using list: {sum_list}, '\n      f'Time taken: {1e3*(end - start):.4f} milliseconds')\n\nmy_list_numpy = np.array(my_list)\nstart = time.time()  # start timer\nsum_numpy = np.sum(my_list_numpy)  # sum using numpy array\nend = time.time()  # end timer\nprint(f'Sum using numpy: {sum_numpy}, '\n      f'Time taken: {1e3*(end - start):.4f} milliseconds')\n\nSum using list: 499999.5, Time taken: 7.7531 milliseconds\nSum using numpy: 499999.5, Time taken: 0.3664 milliseconds\n\n\n\nNumPy is also able to perform vectorized operations, which let us express complex computations in a few lines of code. For example, we can compute a function of all elements in an array without writing explicit loops. This is not only more readable but also significantly faster, as the underlying C code can be optimised for performance.\n\n\n# Vectorized array operations\nx = np.linspace(0, 100, 1_000_000)\ny = np.sin(x) + 0.5 * x**2\nprint(y[:5])  # show first five results\n\n[0.         0.00010001 0.00020002 0.00030005 0.00040008]\n\n\n\nOne line performs a million floating‑point operations in compiled code—often orders of magnitude faster than an explicit Python loop.\n\n\n\n\nCampaioli, Francesco, Jared H. Cole, and Harini Hapuarachchi. 2024. “Quantum Master Equations: Tips and Tricks for Quantum Optics, Quantum Computing, and Beyond.” PRX Quantum 5 (June): 020202. https://doi.org/10.1103/PRXQuantum.5.020202.\n\n\nJohansson, J. R., P. D. Nation, and Franco Nori. 2012. “QuTiP: An open-source Python framework for the dynamics of open quantum systems.” Computer Physics Communications 183 (8): 1760–72. https://doi.org/10.1016/j.cpc.2012.02.021.\n\n\nLambert, Neill, Eric Giguère, Paul Menczel, Boxi Li, Patrick Hopf, Gerardo Suárez, Marc Gali, et al. 2024. “QuTiP 5: The Quantum Toolbox in Python.” arXiv:2412.04705. https://arxiv.org/abs/2412.04705.\n\n\nMercurio, Alberto, Yi-Te Huang, Li-Xun Cai, Yueh-Nan Chen, Vincenzo Savona, and Franco Nori. 2025. “QuantumToolbox.jl: An Efficient Julia Framework for Simulating Open Quantum Systems.” arXiv Preprint arXiv:2504.21440. https://doi.org/10.48550/arXiv.2504.21440.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "lecture1/linear_algebra.html",
    "href": "lecture1/linear_algebra.html",
    "title": "3  Linear Algebra with NumPy and SciPy",
    "section": "",
    "text": "3.1 NumPy: The Foundation of Dense Linear Algebra\nQuantum systems are described by vectors and operators in complex Hilbert spaces. States \\(\\vert \\psi \\rangle\\) correspond to column vectors, and observables—like the Hamiltonian \\(\\hat{H}\\) or spin operators—are represented by matrices. Tasks such as finding energy spectra via eigenvalue decompositions, simulating time evolution through operator exponentials, and building composite systems with tensor (Kronecker) products all reduce to core linear‐algebra operations.\nIn this chapter, we will leverage NumPy’s and SciPy’s routines (backed by optimized BLAS/LAPACK) to perform matrix–matrix products, eigen-decompositions, vector norms, and more. When system size grows, SciPy’s sparse data structures and Krylov‐subspace solvers will let us handle very large, structured operators efficiently.\nBy blending physical intuition (Schrödinger’s equation, expectation values, operator algebra) with hands‐on Python code, you’ll see how powerful and intuitive modern linear‐algebra libraries can be for quantum‐mechanics simulations. Let’s get started!\nNumPy provides the ndarray type, an efficient, N-dimensional array stored in contiguous memory. This layout makes vectorized operations and low-level BLAS calls blazing fast. At its simplest, a 2D ndarray represents a matrix:\n\\[\nA = \\begin{pmatrix}a_{11} & a_{12}\\\\ a_{21} & a_{22}\\end{pmatrix},\n\\]\nand a 1D ndarray represents a column vector:\n\\[\n\\mathbf{v} = \\begin{pmatrix}v_1\\\\ v_2\\end{pmatrix}.\n\\]\nNumPy’s dense arrays form the backbone of many quantum‐simulation tasks—building Hamiltonians, computing overlaps, and propagating states all reduce to these core operations. Having a quick reference for them can speed up both writing and reading simulation code.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Algebra with NumPy and SciPy</span>"
    ]
  },
  {
    "objectID": "lecture1/linear_algebra.html#numpy-the-foundation-of-dense-linear-algebra",
    "href": "lecture1/linear_algebra.html#numpy-the-foundation-of-dense-linear-algebra",
    "title": "3  Linear Algebra with NumPy and SciPy",
    "section": "",
    "text": "3.1.1 Summary of Core Functions\n\n\n\n\n\n\n\n\nOperation\nEquation\nNumPy call\n\n\n\n\nMatrix–matrix product\n\\(C = A B\\)\nC = A.dot(B) or A @ B\n\n\nMatrix–vector product\n\\(\\mathbf{w} = A \\mathbf{v}\\)\nw = A.dot(v)\n\n\nEigenvalues and eigenvectors\n\\(A \\mathbf{x} = \\lambda \\mathbf{x}\\)\nw, v = np.linalg.eig(A)\n\n\nDeterminant\n\\(\\det(A)\\)\nnp.linalg.det(A)\n\n\nInverse\n\\(A^{-1}\\)\nnp.linalg.inv(A)\n\n\nNorm (Frobenius)\n\\(\\|A\\|_F = \\sqrt{\\sum_{ij} \\vert a_{ij} \\vert^2}\\)\nnp.linalg.norm(A)\n\n\nKronecker product\n\\(A \\otimes B\\)\nnp.kron(A, B)\n\n\n\nIn the table above, each abstract operation is paired with its NumPy call. Notice how intuitive the syntax is: the @ operator reads like the usual linear-algebra notation.\n\n\n3.1.2 Matrix–Matrix and Matrix–Vector Multiplication\nLet’s consider a simple example of a 2×2 matrix \\(A\\) and a 2-vector \\(\\mathbf{v}\\). This captures key ideas: operator composition via matrix–matrix products and state evolution via matrix–vector products. Indeed, in quantum mechanics, applying one operator after another corresponds to a matrix–matrix product, while acting on a quantum state uses a matrix–vector product. Consider the following:\n\n\nimport numpy as np\n\n# Define a 2×2 matrix and a 2-vector\nA = np.array([[1, 2], [3, 4]])\nv = np.array([5, 6])\n\n# Matrix–matrix product\nc = A @ A  # same as A.dot(A)\ndisplay(\"A @ A =\", c)\n\n# Matrix–vector product\nw = A @ v  # same as A.dot(v)\ndisplay(\"A @ v =\", w)\n\n'A @ A ='\n\n\narray([[ 7, 10],\n       [15, 22]])\n\n\n'A @ v ='\n\n\narray([17, 39])\n\n\n\nHere, A @ A computes \\(A^2\\), and A @ v computes \\(A\\mathbf{v}\\).\n\n\n3.1.3 Diagonalization\nThe eigenvalue problem is one of the cornerstones of both applied mathematics and quantum mechanics. Given a square matrix \\(A \\in \\mathbb{C}^{n\\times n}\\), we seek scalars \\(\\lambda\\in\\mathbb{C}\\) (eigenvalues) and nonzero vectors \\(\\mathbf{x}\\in\\mathbb{C}^n\\) (eigenvectors) such that\n\\[\nA \\,\\mathbf{x} = \\lambda\\,\\mathbf{x}.\n\\]\nPhysically, in quantum mechanics, \\(A\\) might be the Hamiltonian operator \\(\\hat H\\), its eigenvalues \\(\\lambda\\) correspond to allowed energy levels, and the eigenvectors \\(\\mathbf{x}\\) represent stationary states. Mathematically, diagonalizing \\(A\\) transforms it into a simple form\n\\[\nA = V \\,\\Lambda\\, V^{-1},\n\\]\nwhere \\(\\Lambda\\) is the diagonal matrix of eigenvalues and the columns of \\(V\\) are the corresponding eigenvectors. Once in diagonal form, many operations—such as computing matrix exponentials for time evolution, powers of \\(A\\), or resolving a system of differential equations—become trivial:\n\n\\[\nf(A) = V\\,f(\\Lambda)\\,V^{-1},\\quad\nf(\\Lambda) = \\mathrm{diag}\\bigl(f(\\lambda_1),\\dots,f(\\lambda_n)\\bigr).\n\\]\nIn practice, NumPy’s np.linalg.eig calls optimized LAPACK routines to compute all eigenpairs of a dense matrix:\n\n\nw, v = np.linalg.eig(A)\ndisplay(\"Eigenvalues:\", w)\ndisplay(\"Eigenvectors (as columns):\\n\", v)\n\n'Eigenvalues:'\n\n\narray([-0.37228132,  5.37228132])\n\n\n'Eigenvectors (as columns):\\n'\n\n\narray([[-0.82456484, -0.41597356],\n       [ 0.56576746, -0.90937671]])\n\n\n\nUnder the hood, NumPy calls optimized LAPACK routines to diagonalize dense matrices.\n\n\n3.1.4 Kronecker Product\nIn quantum mechanics, the state space of a composite system is the tensor product of the state spaces of its subsystems. If system 1 has Hilbert space \\(\\mathcal H_A\\) of dimension \\(m\\) and system 2 has \\(\\mathcal H_B\\) of dimension \\(p\\), then the joint space is \\(\\mathcal H_A\\otimes\\mathcal H_B\\) of dimension \\(m p\\). Operators on the composite system factorize as tensor (Kronecker) products of subsystem operators. For example, if \\(A\\) acts on system 1 and \\(B\\) on system 2, then \\[\nA\\otimes B\\;:\\;\\mathcal H_A\\otimes\\mathcal H_B\\to \\mathcal H_A\\otimes\\mathcal H_B\n\\] has matrix elements \\[\n(A\\otimes B)_{(i\\,,\\,\\alpha),(j\\,,\\,\\beta)}\n    = A_{ij}\\,B_{\\alpha\\beta},\n\\] and in block form \\[\nA \\otimes B\n= \\begin{pmatrix}\n    a_{11}\\,B & a_{12}\\,B & \\cdots & a_{1n}\\,B \\\\\n    \\vdots    &        &        & \\vdots     \\\\\n    a_{m1}\\,B & a_{m2}\\,B & \\cdots & a_{mn}\\,B\n\\end{pmatrix},\n\\] yielding an \\(mp\\times nq\\) matrix when \\(A\\in\\mathbb C^{m\\times n}\\) and \\(B\\in\\mathbb C^{p\\times q}\\).\nWhy is this useful? In later chapters we will build multi‐qubit gates (e.g. CNOT, controlled-phase), couple different oscillators, and assemble large Hamiltonians by taking tensor products of single‐mode operators. The Kronecker product lets us lift any local operator into the full, composite Hilbert space.\nIn NumPy, the Kronecker product is computed with np.kron:\n\n\nB = np.array([[0, 1], [1, 0]])  # Pauli-X matrix\nkron = np.kron(A, B)\ndisplay(\"A ⊗ B =\", kron)\n\n'A ⊗ B ='\n\n\narray([[0, 1, 0, 2],\n       [1, 0, 2, 0],\n       [0, 3, 0, 4],\n       [3, 0, 4, 0]])\n\n\n\nKronecker products build composite quantum-system operators from single-subsystem operators.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Algebra with NumPy and SciPy</span>"
    ]
  },
  {
    "objectID": "lecture1/linear_algebra.html#scipy-advanced-algorithms-and-sparse-data",
    "href": "lecture1/linear_algebra.html#scipy-advanced-algorithms-and-sparse-data",
    "title": "3  Linear Algebra with NumPy and SciPy",
    "section": "3.2 SciPy: Advanced Algorithms and Sparse Data",
    "text": "3.2 SciPy: Advanced Algorithms and Sparse Data\nWhile NumPy covers dense linear algebra, SciPy complements it with:\n\n\n\n\n\n\n\nModule\nPurpose\n\n\n\n\nscipy.linalg\nAlternative LAPACK-based routines for dense ops\n\n\nscipy.sparse\nData structures (COO, CSR, CSC) for sparse matrices\n\n\nscipy.sparse.linalg\nIterative solvers (e.g. Arnoldi, Lanczos)\n\n\nscipy.integrate\nODE and quadrature routines\n\n\nscipy.optimize\nRoot-finding and minimization\n\n\nscipy.special\nSpecial mathematical functions\n\n\n\nCompared to NumPy, SciPy’s routines often expose extra options (e.g. choosing solvers) and can handle very large, sparse systems efficiently.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Algebra with NumPy and SciPy</span>"
    ]
  },
  {
    "objectID": "lecture1/linear_algebra.html#some-useful-functions",
    "href": "lecture1/linear_algebra.html#some-useful-functions",
    "title": "3  Linear Algebra with NumPy and SciPy",
    "section": "3.3 Some Useful Functions",
    "text": "3.3 Some Useful Functions\nBelow are a few handy SciPy routines:\n\nDeterminant: scipy.linalg.det\nInverse: scipy.linalg.inv\nFrobenius norm: scipy.linalg.norm\n\n\n\nimport scipy.linalg as la\n\ndet = la.det(A)\ninv = la.inv(A)\nnorm_f = la.norm(A)\ndisplay(det, inv, norm_f)\n\nnp.float64(-2.0)\n\n\narray([[-2. ,  1. ],\n       [ 1.5, -0.5]])\n\n\nnp.float64(5.477225575051661)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Algebra with NumPy and SciPy</span>"
    ]
  },
  {
    "objectID": "lecture1/linear_algebra.html#solving-linear-systems",
    "href": "lecture1/linear_algebra.html#solving-linear-systems",
    "title": "3  Linear Algebra with NumPy and SciPy",
    "section": "3.4 Solving Linear Systems",
    "text": "3.4 Solving Linear Systems\nA linear system has the form\n\\[\nA\\,\\mathbf{x} = \\mathbf{b},\n\\]\nwhere \\(A\\in\\mathbb R^{n\\times n}\\) and \\(\\mathbf b\\in\\mathbb R^n\\) is known. For small \\(n\\) you can even solve by hand. For example, consider the \\(2\\times2\\) system\n\\[\n\\begin{cases}\nx_1 + 2x_2 = 5,\\\\\n3x_1 + 4x_2 = 11.\n\\end{cases}\n\\quad\\Longrightarrow\\quad\nA = \\begin{pmatrix}1 & 2\\\\ 3 & 4\\end{pmatrix},\\;\n\\mathbf b = \\begin{pmatrix}5\\\\11\\end{pmatrix}.\n\\]\nWe can reproduce this with NumPy:\n\n\nA = np.array([[1, 2], [3, 4]])\nb = np.array([5, 11])\nx = np.linalg.solve(A, b)\ndisplay(\"Solution x=\", x)\n\n'Solution x='\n\n\narray([1., 2.])\n\n\n\nSciPy’s sparse module also offers scipy.sparse.linalg.spsolve for large, sparse \\(A\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Algebra with NumPy and SciPy</span>"
    ]
  },
  {
    "objectID": "lecture1/linear_algebra.html#sparse-matrices",
    "href": "lecture1/linear_algebra.html#sparse-matrices",
    "title": "3  Linear Algebra with NumPy and SciPy",
    "section": "3.5 Sparse Matrices",
    "text": "3.5 Sparse Matrices\nAs quantum systems scale to many degrees of freedom, the underlying operators—such as Hamiltonians or Liouvillian superoperators—grow exponentially in dimension but often remain highly structured and sparse. Instead of storing dense arrays with mostly zeros, sparse-matrix formats only record nonzero entries and their indices, dramatically reducing memory requirements. Common physical models, like spin chains with nearest-neighbor couplings or lattice Hamiltonians, have only \\(\\mathcal{O}(N)\\) or \\(\\mathcal{O}(N \\log N)\\) nonzero elements, making sparse representations essential for large-scale simulations.\nIn the following sections, we will:\n\nConstruct sparse matrices in COO formats with SciPy.\nIllustrate basic sparse-matrix operations (matrix–vector products, format conversions).\nUse scipy.sparse.linalg.eigs (Arnoldi) to compute a few eigenvalues of a sparse Hamiltonian.\n\nThe Coordinate (COO) format is a simple way to store sparse matrices. Instead of storing all entries, the COO format only keeps nonzero entries of the form \\((i, j, a_{ij})\\), which saves memory and speeds up computations. Graphically, a 5×5 example with 4 nonzeros might look like:\n\\[\nA = \\begin{pmatrix}\n7 & \\cdot & \\cdot & \\cdot & 1 \\\\\n\\cdot & \\cdot & 2      & \\cdot & \\cdot \\\\\n\\cdot & 3      & \\cdot & \\cdot & \\cdot \\\\\n\\cdot & \\cdot & \\cdot & \\cdot & \\cdot \\\\\n4      & \\cdot & \\cdot & \\cdot & \\cdot\n\\end{pmatrix}\n\\]\nHere each number shows a location and its value. COO is very simple and intuitive, but not the most efficient. For larger matrices, we can use the Compressed Sparse Row (CSR) or Compressed Sparse Column (CSC) formats, which store the nonzero entries in a more compact way. The CSR format is very efficient for matrix–vector products.\nSuch matrix can be created in SciPy using the coo_matrix class:\n\n\nfrom scipy import sparse\n\n# Create a sparse COO matrix\ni = [0, 0, 1, 2, 4] # row indices\nj = [0, 4, 2, 1, 0] # column indices\ndata = [7, 1, 2, 3, 4] # nonzero values\ncoo = sparse.coo_matrix((data, (i, j)), shape=(5, 5))\ncoo\n\n&lt;COOrdinate sparse matrix of dtype 'int64'\n    with 5 stored elements and shape (5, 5)&gt;\n\n\n\nIt is also possible to convert between different sparse formats. For example, to convert a COO matrix to CSR format, you can use the tocsc() method:\n\n\n# Convert COO to CSR format\ncsr = coo.tocsr()\ncsr\n\n&lt;Compressed Sparse Row sparse matrix of dtype 'int64'\n    with 5 stored elements and shape (5, 5)&gt;\n\n\n\nAnd the matrix–vector product is as simple as:\n\n\n# Matrix–vector product\nv = np.array([1, 2, 3, 4, 5])\nw = coo @ v  # same as coo.dot(v)\nw\n\narray([12,  6,  6,  0,  4])\n\n\n\n\n3.5.1 Eigenvalues of Sparse Matrices\nEven with sparse storage, direct methods (dense diagonalization or full factorization) become intractable when the matrix dimension exceeds millions. To extract a few extremal eigenvalues or approximate time evolution, Krylov-subspace approaches (like the Arnoldi algorithm) build a low-dimensional orthonormal basis that captures the action of the operator on a subspace. By repeatedly applying the sparse matrix to basis vectors and orthogonalizing, Arnoldi produces a small Hessenberg matrix whose eigenpairs approximate those of the full operator. This hybrid strategy leverages both memory-efficient storage and iterative linear algebra to access spectral properties of huge quantum systems.\nTo approximate a few eigenvalues of a large, sparse matrix \\(A\\), SciPy’s eigs implements the Arnoldi algorithm. Under the hood it builds an \\(m\\)-dimensional Krylov basis. More precisely, given a starting vector \\(v_1\\) with \\(\\|v_1\\|_2 = 1\\), the \\(m\\)‑dimensional Krylov subspace is\n\\[\n\\mathcal{K}_m(A, v_1) = \\operatorname{span}\\{v_1, Av_1, A^{2}v_1, \\dots, A^{m-1}v_1\\}.\n\\]\nThe Arnoldi iteration produces the decomposition\n\\[\nA V_m = V_m H_m + h_{m+1,m}\\, v_{m+1} e_m^{\\top},\n\\]\nwhere\n\n\\(V_m = [v_1, \\dots, v_m]\\) has orthonormal columns,\n\\(H_m\\) is an \\(m \\times m\\) upper‑Hessenberg matrix,\n\\(e_m\\) is the \\(m\\)‑th canonical basis vector.\n\nThe eigenvalues of \\(H_m\\) are called Ritz values; they approximate eigenvalues of \\(A\\). As \\(m\\) grows, the approximation improves. In practice we combine Arnoldi with a restart strategy (after reaching a given \\(m\\) we keep the most accurate Ritz vectors and build a fresh Krylov basis). SciPy’s scipy.sparse.linalg.eigs wrapper uses the implicitly restarted Arnoldi method from ARPACK.\nAs a pseudo-code, the Arnoldi algorithm can be summarized as follows:\n\nPick a random vector \\(v\\) and normalize it.\nFor \\(j = 1, \\dots, m\\)\n\n\\(w = A v_j\\)\nOrthogonalize: \\[h_{i,j} = v_i^{\\dagger} w, \\quad w \\leftarrow w - h_{i,j} v_i \\quad (i = 1, \\dots, j)\\]\n\\(h_{j+1,j} = \\|w\\|_2\\).\nIf \\(h_{j+1,j} = 0\\), stop (the Krylov subspace is invariant).\n\\(v_{j+1} = w / h_{j+1,j}\\).\n\n\nThe cost is \\(m\\) sparse matrix–vector products and \\(\\mathcal{O}(m^2 n)\\) scalar operations for orthogonalization (which stays moderate when \\(m \\ll n\\)).\nHere’s a concrete example:\n\n\nfrom scipy.sparse.linalg import eigs\n\n# Compute the 2 largest-magnitude eigenvalues of coo\nvals, vecs = eigs(coo, k=2)\ndisplay(\"Sparse eigenvalues:\", vals)\n\n'Sparse eigenvalues:'\n\n\narray([7.53112887+0.j, 2.44948974+0.j])",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Algebra with NumPy and SciPy</span>"
    ]
  },
  {
    "objectID": "lecture1/numba_and_jax.html",
    "href": "lecture1/numba_and_jax.html",
    "title": "4  Speeding up Python for Linear Algebra Tasks",
    "section": "",
    "text": "4.1 Numba: Just-In-Time Compilation\nPython is easy to read, but pure-Python loops can be slow if you do not leverage optimized libraries (BLAS, LAPACK). Here we explore two tools—Numba and JAX—to accelerate common linear algebra operations.\nNumba uses LLVM to compile Python functions to machine code at runtime. Key points:\nExample: Matrix–Vector Multiplication\nIn practice, Numba can speed up this looped version by 10×–100× compared to pure Python, approaching the speed of NumPy’s optimized routines. The reader is encouraged to try the code without the @njit decorator to see the difference in performance.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Speeding up Python for Linear Algebra Tasks</span>"
    ]
  },
  {
    "objectID": "lecture1/numba_and_jax.html#numba-just-in-time-compilation",
    "href": "lecture1/numba_and_jax.html#numba-just-in-time-compilation",
    "title": "4  Speeding up Python for Linear Algebra Tasks",
    "section": "",
    "text": "Decorators: Use @njit (nopython mode) for best speed.\n\nType inference: Numba infers types on first run, then compiles specialized code.\n\nCompilation overhead: The first call incurs compilation time; subsequent calls are fast.\n\nObject mode vs nopython mode: Always aim for nopython mode to avoid Python object overhead.\n\n\nJIT Workflow 1. Call function → type inference → LLVM IR generation.\n2. LLVM IR → machine code (cached).\n3. Subsequent calls use cached machine code.\n\n\n\n\nfrom numba import njit\nimport numpy as np\nimport time # for timing\n\n@njit\ndef matvec(A, x):\n    m, n = A.shape\n    y = np.zeros(m)\n    for i in range(m):\n        temp = 0.0\n        for j in range(n):\n            temp += A[i, j] * x[j]\n        y[i] = temp\n    return y\n\n# Prepare data\ndim = 500\nA = np.random.rand(dim, dim)\nx = np.random.rand(dim)\n\n# Using NumPy's dot product\nstart = time.time()\ny0 = A @ x\nend = time.time()\nprint(\"NumPy time (ms): \", 1e3*(end - start))\n\n# Using Numba's compiled function\ny0 = matvec(A, x) # First call for compilation\n\nstart = time.time()\ny1 = matvec(A, x)\nend = time.time()\nprint(\"Numba time (ms): \", 1e3*(end - start))\n\nNumPy time (ms):  0.3037452697753906\nNumba time (ms):  0.31113624572753906",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Speeding up Python for Linear Algebra Tasks</span>"
    ]
  },
  {
    "objectID": "lecture1/numba_and_jax.html#jax-xla-compilation-and-automatic-differentiation",
    "href": "lecture1/numba_and_jax.html#jax-xla-compilation-and-automatic-differentiation",
    "title": "4  Speeding up Python for Linear Algebra Tasks",
    "section": "4.2 JAX: XLA Compilation and Automatic Differentiation",
    "text": "4.2 JAX: XLA Compilation and Automatic Differentiation\nJAX is a high-performance library from Google Research that extends NumPy with just-in-time compilation and automatic differentiation. It - Compiles array operations via XLA, fusing kernels and reducing Python overhead. - Supports GPU and TPU backends with minimal code changes. - Provides grad for gradients of scalar functions, enabling optimisation and machine-learning tasks. - Offers advanced transformations like vmap (vectorisation) and pmap (parallelism on multiple devices).\nJAX is widely used in deep learning frameworks (e.g. Flax, Haiku), reinforcement learning, and scientific research (including physics simulations), thanks to its blend of speed and flexibility.\n\n4.2.1 A Quick Overview of Automatic Differentiation\nAutomatic differentiation (AD) is a family of techniques to compute exact derivatives of functions defined by computer programs. Unlike symbolic differentiation (which can lead to expression swell) or numerical finite-difference (which suffers from truncation and round-off error), AD exploits the fact that any complex function is ultimately composed of a finite set of elementary operations (addition, multiplication, sin, exp, …) whose derivatives are known exactly.\n\nLimitations of Finite Differences\nA common finite-difference formula for a scalar function \\(f(x)\\) is the central difference\n\\[\n\\frac{df}{dx}(x)\\approx \\frac{f(x+h)-f(x-h)}{2h},\n\\]\nwith local truncation error \\(\\mathcal{O}(h^2)\\). However, this approach has important limitations:\n\nTruncation vs. round-off: If \\(h\\) is too large, the \\(\\mathcal{O}(h^2)\\) term dominates. If \\(h\\) is too small, floating-point cancellation makes the numerator \\(f(x+h)-f(x-h)\\) inaccurate.\nCost with many parameters: For \\(f:\\mathbb{R}^n\\to\\mathbb{R}\\), the gradient component \\(i\\) is\n\\[\n\\frac{\\partial f}{\\partial x_i}(\\mathbf{x})\\approx \\frac{f(\\mathbf{x}+h\\mathbf{e}_i)-f(\\mathbf{x}-h\\mathbf{e}_i)}{2h}.\n\\]\nComputing all \\(n\\) components requires \\(2n\\) evaluations of \\(f\\), so the cost scales as \\(\\mathcal{O}(n)\\) in \\(f\\)-calls. For large \\(n\\) (many parameters), this becomes prohibitive.\nNon-smooth or branching code: When \\(f\\) contains control flow or non-differentiable operations, finite differences may give misleading or undefined results.\n\n\n\nAutomatic Differentiation and the Chain Rule\nAutomatic differentiation (AD) applies the chain rule to each elementary operation in code (addition, multiplication, sin, exp, etc.), yielding exact derivatives up to floating-point precision. For a composition\n\\[\nu = g(x),\\quad y = f(u),\n\\]\nAD uses the chain rule:\n\\[\n\\frac{dy}{dx} = \\frac{df}{du}\\frac{dg}{dx}.\n\\]\nIn more complex nests, e.g.\n\\[\nv = h(u),\\quad u = g(x),\\quad y = f(v),\n\\]\nwe get\n\\[\n\\frac{dy}{dx} = \\frac{df}{dv}\\frac{dh}{du}\\frac{dg}{dx}.\n\\]\nAD comes in two modes:\n\nForward mode (propagate derivatives from inputs to outputs).\nReverse mode (propagate sensitivities from outputs back to inputs).\n\nJAX implements both and selects the most efficient strategy automatically.\n\n\nComparing Accuracy: AD vs Finite Differences\nBelow is a Quarto code cell that plots the error of finite differences (varying step size \\(h\\)) and automatic differentiation against the true derivative of \\(f(x) = e^{\\sin(x)}\\) at \\(x=1.0\\).\n\n\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt # for plotting\n\n# Set JAX to use 64-bit floats\njax.config.update(\"jax_enable_x64\", True)\n\n# Define function and true derivative\ndef f_np(x):\n    return np.exp(np.sin(x))\n\ndef df_true(x):\n    return np.cos(x) * np.exp(np.sin(x))\n\n# Point of evaluation\nx0 = 1.0\n\n# Finite-difference errors for varying h\nhs = np.logspace(-8, -1, 50)\nerrors_fd = []\nfor h in hs:\n    df_fd = (f_np(x0 + h) - f_np(x0 - h)) / (2 * h)\n    errors_fd.append(abs(df_fd - df_true(x0)))\n\n# Automatic differentiation error (constant)\ndf_ad = jax.grad(lambda x: jnp.exp(jnp.sin(x)))(x0)\nerror_ad = abs(np.array(df_ad) - df_true(x0))\n\nprint(f\"AD error: {error_ad}\")\nprint(f\"FD minimum error: {min(errors_fd)}\")\n\n# Plot\nfig, ax = plt.subplots()\nax.loglog(hs, errors_fd, marker=\"o\")\nax.set_xlabel(\"Step size $h$\")\nax.set_ylabel(\"Error of Finite Differences\")\n\n# Show in Quarto\nplt.savefig(\"_tmp_fig.svg\")\nplt.close(fig)\nSVG(\"_tmp_fig.svg\")\n\nAD error: 0.0\nFD minimum error: 7.006839553014288e-12\n\n\n\n\n\n\n\n\n\n\nThis plot illustrates that finite differences achieve minimal error at an optimal \\(h\\), but degrade for too large or too small \\(h\\), while AD remains accurate to machine precision regardless of step size.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Speeding up Python for Linear Algebra Tasks</span>"
    ]
  },
  {
    "objectID": "lecture1/numba_and_jax.html#why-computing-gradients-is-important-in-quantum-physics",
    "href": "lecture1/numba_and_jax.html#why-computing-gradients-is-important-in-quantum-physics",
    "title": "4  Speeding up Python for Linear Algebra Tasks",
    "section": "4.3 Why Computing Gradients Is Important in Quantum Physics",
    "text": "4.3 Why Computing Gradients Is Important in Quantum Physics\nIn quantum physics, many problems reduce to optimizing parameters in a model or a control protocol. Computing gradients of a cost function with respect to these parameters is essential for efficient and reliable optimization.\n\nVariational quantum algorithms: In methods like the variational quantum eigensolver (VQE)(Peruzzo et al. 2014), a parametrised quantum state \\(|\\psi(\\boldsymbol{\\theta})\\rangle\\) depends on parameters \\(\\boldsymbol{\\theta}=(\\theta_1,\\dots,\\theta_n)\\). One minimises the expectation \\[\nE(\\boldsymbol{\\theta}) = \\langle\\psi(\\boldsymbol{\\theta})|\\hat H|\\psi(\\boldsymbol{\\theta})\\rangle.\n\\]\nGradient-based methods require\n\\[\n\\frac{\\partial E}{\\partial \\theta_i} = \\frac{\\partial}{\\partial \\theta_i} \\langle\\psi(\\boldsymbol{\\theta})|\\hat H|\\psi(\\boldsymbol{\\theta})\\rangle.\n\\]\nAD enables exact evaluation of these derivatives through the quantum circuit parameters, improving convergence compared to gradient-free methods.\nQuantum optimal control(D’Alessandro 2021; Khaneja et al. 2005): One shapes control fields \\(u(t)\\) in the Hamiltonian \\[\n\\hat H(t; u) = \\hat H_0 + \\sum_i u_i(t) \\hat H_i\n\\]\nto drive the system from an initial state \\(|\\psi_0\\rangle\\) to a target \\(|\\psi_T\\rangle\\). A typical cost function is\n\\[\nJ[u] = 1 - |\\langle\\psi_T|\\mathcal U_T[u]|\\psi_0\\rangle|^2,\n\\]\nwhere \\(\\mathcal U_T[u]\\) is the time-ordered evolution. Computing gradients \\(\\delta J/\\delta u_i(t)\\) is needed for gradient-ascent pulse engineering (GRAPE) algorithms. AD can differentiate through time-discretised propagators and ODE solvers, automating derivation of \\(\\delta J/\\delta u_i(t)\\) and providing machine-precision gradients for faster convergence.\nParameter estimation and tomography(Lvovsky and Raymer 2009): Maximum-likelihood estimation for quantum states or processes often involves maximising a log-likelihood \\(L(\\boldsymbol{\\theta})\\). Gradients speed up estimation and enable standard optimisers (e.g. L-BFGS).\n\nBy providing exact, efficient gradients even through complex quantum simulations (time evolution, measurement models, noise), automatic differentiation (via JAX or similar frameworks) has become a key tool in modern quantum physics research.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Speeding up Python for Linear Algebra Tasks</span>"
    ]
  },
  {
    "objectID": "lecture1/numba_and_jax.html#summary",
    "href": "lecture1/numba_and_jax.html#summary",
    "title": "4  Speeding up Python for Linear Algebra Tasks",
    "section": "4.4 Summary",
    "text": "4.4 Summary\n\nNumba: Best for speeding up existing NumPy loops with minimal code changes. Ideal when you do not need gradients or accelerators.\nJAX: Ideal for optimisation tasks requiring gradients, large-scale batch operations, or GPU/TPU acceleration. The XLA compiler often outperforms loop-based JIT for fused kernels.\n\n\n\n\n\nD’Alessandro, Domenico. 2021. Introduction to Quantum Control and Dynamics. Chapman; Hall/CRC. https://doi.org/10.1201/9781003051268.\n\n\nKhaneja, Navin, Timo Reiss, Cindie Kehlet, Thomas Schulte-Herbrüggen, and Steffen J. Glaser. 2005. “Optimal Control of Coupled Spin Dynamics: Design of NMR Pulse Sequences by Gradient Ascent Algorithms.” Journal of Magnetic Resonance 172 (2): 296–305. https://doi.org/10.1016/j.jmr.2004.11.004.\n\n\nLvovsky, A. I., and M. G. Raymer. 2009. “Continuous-Variable Optical Quantum-State Tomography.” Rev. Mod. Phys. 81 (March): 299–332. https://doi.org/10.1103/RevModPhys.81.299.\n\n\nPeruzzo, Alberto, Jarrod McClean, Peter Shadbolt, Man-Hong Yung, Xiao-Qi Zhou, Peter J. Love, Alán Aspuru-Guzik, and Jeremy L. O’Brien. 2014. “A Variational Eigenvalue Solver on a Photonic Quantum Processor.” Nature Communications 5 (1). https://doi.org/10.1038/ncomms5213.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Speeding up Python for Linear Algebra Tasks</span>"
    ]
  },
  {
    "objectID": "lecture1/ordinary_differential_equations.html",
    "href": "lecture1/ordinary_differential_equations.html",
    "title": "5  Ordinary Differential Equations",
    "section": "",
    "text": "5.1 General Definition and Examples\nAn ordinary differential equation (ODE) is an equation involving functions of one independent variable (for instance, time) and its derivatives. In the simplest scenario, suppose we have an unknown function \\(y(t)\\). A first-order ODE can be written as:\n\\[\n\\frac{dy(t)}{dt} = f\\bigl(y(t), t\\bigr),\n\\]\nwhere \\(f\\) is a known function, and \\(y(t)\\) is the unknown to be determined. Higher-order ODEs can often be recast as systems of first-order ODEs by introducing additional variables for the higher derivatives.\nTo see how ODEs arise in physical scenarios, consider Newton’s second law, \\(m\\,\\frac{d^2 x}{dt^2} = F(x,t)\\). This second-order ODE can be reduced to a system of two first-order ODEs by introducing an auxiliary variable for velocity \\(v(t) = \\frac{dx}{dt}\\). Then we have:\n\\[\n\\begin{cases}\n    \\frac{dx}{dt} = v,\\\\\n    \\frac{dv}{dt} = \\frac{F(x,t)}{m}.\n  \\end{cases}\n\\]\nIn quantum mechanics, the time-dependent Schrödinger equation\n\\[\ni\\hbar \\,\\frac{d}{dt}\\ket{\\psi(t)} = \\hat{H}\\,\\ket{\\psi(t)}\n\\] can be viewed as a first-order ODE in the Hilbert space: the role of \\(\\ket{\\psi(t)}\\) is analogous to \\(y(t)\\), and \\(-\\frac{i}{\\hbar}\\,\\hat{H}\\) plays the role of \\(f(\\,\\cdot\\,,t)\\) (assuming a time-independent \\(\\hat{H}\\)). This analogy suggests that the Schrödinger equation can be treated using standard ODE solution techniques or, in more complicated cases, numerical integration.\nA linear ODE has the form: \\(\\frac{d\\mathbf{y}(t)}{dt} = A\\,\\mathbf{y}(t) + \\mathbf{b}(t),\\) where \\(\\mathbf{y}(t)\\) is a vector function of time, \\(A\\) is a constant (or possibly time-dependent) matrix, and \\(\\mathbf{b}(t)\\) is a known inhomogeneous term. If \\(\\mathbf{b}(t) = \\mathbf{0}\\), the equation is said to be homogeneous.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Ordinary Differential Equations</span>"
    ]
  },
  {
    "objectID": "lecture1/ordinary_differential_equations.html#solving-linear-odes-by-diagonalizing-the-system-matrix",
    "href": "lecture1/ordinary_differential_equations.html#solving-linear-odes-by-diagonalizing-the-system-matrix",
    "title": "5  Ordinary Differential Equations",
    "section": "5.2 Solving Linear ODEs by Diagonalizing the System Matrix",
    "text": "5.2 Solving Linear ODEs by Diagonalizing the System Matrix\nA common case in quantum mechanics and in classical physics is the linear homogeneous system:\n\\[\n\\frac{d\\mathbf{y}(t)}{dt} = A\\,\\mathbf{y}(t),\n\\quad\n\\mathbf{y}(0) = \\mathbf{y}_0,\n\\tag{5.1}\\]\nwhere \\(A\\) is a constant \\(n\\times n\\) matrix, and \\(\\mathbf{y}_0\\) is the initial condition.\n\n5.2.1 Eigenvalue Decomposition\nIf \\(A\\) is diagonalizable, we can write: \\[A = V \\, D \\, V^{-1},\\] where \\(D\\) is a diagonal matrix whose entries are the eigenvalues \\(\\lambda_i\\) of \\(A\\), and the columns of \\(V\\) are the corresponding eigenvectors. Define: \\[\\mathbf{z}(t) = V^{-1}\\,\\mathbf{y}(t).\\] Then, plugging this into Equation 5.1, we get\n\\[\n\\frac{d\\mathbf{z}(t)}{dt}\n  = V^{-1}\\, \\frac{d\\mathbf{y}(t)}{dt}\n  = V^{-1}\\, A\\, \\mathbf{y}(t)\n  = V^{-1}\\, (V \\, D \\, V^{-1})\\, \\mathbf{y}(t)\n  = D \\, \\mathbf{z}(t).\n\\]\nHence, in the \\(\\mathbf{z}\\)-coordinates, the system becomes a set of \\(n\\) uncoupled first-order ODEs:\n\\[\n\\frac{dz_i}{dt} = \\lambda_i\\, z_i(t),\n  \\quad\n  \\text{for } i = 1,\\dots,n.\n\\]\nThese have the well-known solutions:\n\\[\nz_i(t) = z_i(0)\\,e^{\\lambda_i t}.\n\\] To enforce the initial condition \\(\\mathbf{y}(0) = \\mathbf{y}_0\\), we note that \\(\\mathbf{z}(0) = V^{-1}\\,\\mathbf{y}_0\\). Hence, transforming back, we get:\n\\[\n\\mathbf{y}(t) = V\\, \\mathbf{z}(t)\n  = V\n  \\begin{pmatrix}\n    z_1(0) \\, e^{\\lambda_1 t}\\\\\n    z_2(0) \\, e^{\\lambda_2 t}\\\\\n    \\vdots\\\\\n    z_n(0) \\, e^{\\lambda_n t}\n  \\end{pmatrix}\n  = V \\, e^{D t} \\, V^{-1} \\, \\mathbf{y}_0.\n\\]\nTherefore, we obtain the compact form:\n\\[\n\\mathbf{y}(t) = e^{A t}\\,\\mathbf{y}_0,\n\\]\nor, equivalently,\n\\[\n\\mathbf{y}(t) = V\n  \\begin{pmatrix}\n  e^{\\lambda_1} & & \\\\\n  & \\ddots & \\\\\n  & & e^{\\lambda_n}\n  \\end{pmatrix}\n  V^{-1}\n  \\mathbf{y}_0 \\, .\n\\]\nIn the case of \\(A\\) Hermitian, the time evolution can be expanded as\n\\[\n\\mathbf{y}(t) = \\sum_i (\\mathbf{v}_i^\\dagger \\cdot \\mathbf{y}_0) \\mathbf{v}_i \\, e^{\\lambda_i t} \\, ,\n\\]\nwhere \\(\\mathbf{v}_i\\) are the eigenvectors of the matrix.\n\n\n5.2.2 Relation to the Schrödinger Equation\nWhen dealing with the time-dependent Schrödinger equation for a time-independent Hamiltonian \\(\\hat{H}\\), we can represent \\(\\ket{\\psi(t)}\\) in a certain basis, turning the Schrödinger equation into:\n\\[\ni \\hbar \\,\\frac{d}{dt} \\mathbf{c}(t) = H \\,\\mathbf{c}(t),\n\\]\nor equivalently,\n\\[\n\\frac{d\\mathbf{c}(t)}{dt} = -\\frac{i}{\\hbar} \\, H \\,\\mathbf{c}(t).\n\\]\nWe can identify \\(A = -\\frac{i}{\\hbar}\\, H\\). If \\(H\\) is diagonalizable (e.g., Hermitian matrices always have a complete set of orthonormal eigenvectors), then the above solution technique via diagonalization applies. The resulting exponential solution corresponds to the usual \\(e^{-\\frac{i}{\\hbar} H t}\\) operator that defines unitary time evolution in quantum mechanics.\n\nExample: Harmonic Oscillator\nThe harmonic oscillator is described by the second-order ODE:\n\\[\n\\frac{d^2 x}{dt^2} + \\omega^2 x = 0,\n\\]\nwhich can be rewritten as a first-order system:\n\\[\n\\begin{cases}\n\\frac{dx}{dt} = v, \\\\\n\\frac{dv}{dt} = -\\omega^2 x.\n\\end{cases}\n\\]\nor, in matrix form:\n\\[\n\\frac{d}{dt} \\begin{pmatrix} x \\\\ v \\end{pmatrix} = \\begin{pmatrix} 0 & 1 \\\\ -\\omega^2 & 0 \\end{pmatrix} \\begin{pmatrix} x \\\\ v \\end{pmatrix}.\n\\]\nBy diagonalizing the matrix, we can find the solution to this system.\n\n\nimport numpy as np\n\nomega = 2.0\n\n# Define the 2x2 matrix A\nA = np.array([[0.0,  1.0],\n              [-omega**2, 0.0]])\n\n# Initial condition: x(0) = 1, v(0) = 0.5\nx0 = np.array([1.0, 0.5])\n\n# Diagonalize A\neigs, V = np.linalg.eig(A)\nV_inv = np.linalg.inv(V)\n\nz0 = V_inv @ x0\n\n# Define a time array\nt_points = np.linspace(0, 2, 200)\n\nX_t = []\nfor t in t_points:\n    # Compute the solution at time t\n    z_t = np.diag(np.exp(eigs * t)) @ z0\n    x_t = V @ z_t # Transform back to original coordinates\n    X_t.append(x_t)\n\nX_t = np.array(X_t).real\n\nprint(\"x(2) = \", X_t[-1])\n\nx(2) =  [-0.84284424  1.18678318]\n\n\n\nWe have: - A: the system matrix. - y0: initial condition \\(\\mathbf{y}(0)\\). - We diagonalize \\(A\\) to find \\(A = V D V^{-1}\\). - Then \\(\\exp(A t) = V \\exp(D t) V^{-1}\\).\nIf you run the code, you’ll see the final value of \\(\\mathbf{y}(2)\\).\nWe could also visualize the time evolution:\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nax.plot(t_points, X_t[:, 0], label=\"$x(t)$\")\nax.plot(t_points, X_t[:, 1], label=\"$v(t)$\")\nax.plot(t_points, 0.5 * omega**2 * X_t[:, 0]**2 + 0.5 * X_t[:, 1]**2,\n        label=\"$E(t)$\")\nax.set_xlabel(\"Time\")\nax.set_ylabel(\"$y(t)$\")\nax.legend()\n\n# Show in Quarto\nplt.savefig(\"_tmp_fig.svg\")\nplt.close(fig)\nSVG(\"_tmp_fig.svg\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Ordinary Differential Equations</span>"
    ]
  },
  {
    "objectID": "lecture1/ordinary_differential_equations.html#sec-ode-euler-method",
    "href": "lecture1/ordinary_differential_equations.html#sec-ode-euler-method",
    "title": "5  Ordinary Differential Equations",
    "section": "5.3 Numerical Solution via the Euler Method",
    "text": "5.3 Numerical Solution via the Euler Method\nIn many realistic situations (e.g., time-dependent Hamiltonians, nonlinear effects, large dissipative systems described by master equations), finding an exact analytic solution can be very challenging or impossible. We then rely on numerical methods to solve ODEs.\n\n5.3.1 Forward Euler Method\nOne of the simplest methods is the forward Euler method. Suppose we want to solve: \\[\n\\frac{d\\mathbf{y}(t)}{dt} = \\mathbf{f}(\\mathbf{y}(t), t),\n  \\quad\n  \\mathbf{y}(0) = \\mathbf{y}_0.\n\\]\nWe discretize time into steps \\(t_n = n\\,h\\) with step size \\(h\\). The Euler method approximates the derivative at \\(t_n\\) by a difference quotient:\n\\[\n\\frac{d\\mathbf{y}(t_n)}{dt} \\approx \\frac{\\mathbf{y}_{n+1} - \\mathbf{y}_n}{h}.\n\\]\nHence, the system becomes the algebraic update:\n\\[\n\\mathbf{y}_{n+1} = \\mathbf{y}_n + h\\,\\mathbf{f}(\\mathbf{y}_n, t_n),\n\\]\nwith \\(\\mathbf{y}_0\\) known. After iterating this rule for \\(n=0,1,2,\\dots\\), we obtain an approximate solution at discrete times \\(t_n\\).\n\n\n5.3.2 Stability Criterion for the Euler Method\nWhile the Euler method is straightforward, it can be susceptible to numerical instability when the system has rapidly decaying or oscillatory modes. For example, consider the test equation \\(\\frac{dy}{dt} = \\lambda\\, y,\\) where \\(\\lambda\\) is a (possibly complex) constant. The exact solution is \\(y(t) = y(0)\\,e^{\\lambda t}\\). In the Euler scheme, we get\n\\[\ny_{n+1} = y_n + h\\,\\lambda\\,y_n = (1 + h\\,\\lambda)\\, y_n.\n\\]\nThus,\n\\[\ny_n = (1 + h\\,\\lambda)^n\\, y_0.\n\\]\nFor the method to be stable (i.e., for \\(y_n\\) to remain bounded in the limit \\(n\\to\\infty\\) when the exact solution is stable), we require: \\[|1 + h\\,\\lambda| &lt; 1,\\] when the real part of \\(\\lambda\\) is negative (dissipative system). If this condition is not met, the numerical solution may diverge even though the true solution decays exponentially. In practice, one must choose the time step \\(h\\) small enough to satisfy such stability constraints.\n\nExample: Harmonic Oscillator with Euler Method\nLet’s now implement the forward Euler method for a simpler ODE. Consider the same harmonic oscillator, Euler’s method approximates the evolution as:\n\\[\n\\begin{pmatrix}\n  x_{n+1} \\\\ v_{n+1}\n\\end{pmatrix} \\simeq\n\\begin{pmatrix}\n  x_n \\\\ v_n\n\\end{pmatrix} + h \\,\n\\begin{pmatrix}\n  0 & 1 \\\\ -\\omega^2 & 0\n\\end{pmatrix} \\,\n\\begin{pmatrix}\n  x_n \\\\ v_n\n\\end{pmatrix},\n\\]\nwhere \\(h\\) is the time step.\n\n\nh = 0.01\n\nX_t_euler = np.zeros((len(t_points), 2))\nX_t_euler[0] = x0\nfor n in range(len(t_points) - 1):\n    X_t_euler[n+1] = X_t_euler[n] + h * A @ X_t_euler[n]\n\nfig, ax = plt.subplots()\nax.plot(t_points, X_t[:, 0], label=\"$x(t)$ (exact)\")\nax.plot(t_points, X_t_euler[:, 0], label=\"$x(t)$ (Euler)\", linestyle='--')\nax.plot(t_points, X_t[:, 1], label=\"$v(t)$ (exact)\")\nax.plot(t_points, X_t_euler[:, 1], label=\"$v(t)$ (Euler)\", linestyle='--')\nax.set_xlabel(\"Time\")\nax.set_ylabel(\"$y(t)$\")\nax.legend()\n\n# Show in Quarto\nplt.savefig(\"_tmp_fig.svg\")\nplt.close(fig)\nSVG(\"_tmp_fig.svg\")\n\n\n\n\n\n\n\n\n\nHere we see how the Euler solution compares to the exact solution obtained via diagonalization. Notice that using a large time step \\(h\\) can cause the Euler solution to deviate significantly from the exact decay (and may even diverge if \\(|1 - \\lambda h| \\ge 1\\)).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Ordinary Differential Equations</span>"
    ]
  },
  {
    "objectID": "lecture1/ordinary_differential_equations.html#applying-these-methods-to-the-schrödinger-equation",
    "href": "lecture1/ordinary_differential_equations.html#applying-these-methods-to-the-schrödinger-equation",
    "title": "5  Ordinary Differential Equations",
    "section": "5.4 Applying These Methods to the Schrödinger Equation",
    "text": "5.4 Applying These Methods to the Schrödinger Equation\n\nTime-Independent Hamiltonian\nFor a time-independent Hamiltonian \\(\\hat{H}\\), the Schrödinger equation in vector form reads:\n\\[\ni\\hbar \\,\\frac{d\\mathbf{c}(t)}{dt} = H\\,\\mathbf{c}(t).\n\\]\nBy setting \\(A = -\\tfrac{i}{\\hbar} H\\), we recognize that this is a linear ODE. If \\(H\\) (or \\(A\\)) is diagonalizable, its eigen-decomposition yields an analytic solution. In quantum optics, these solutions describe unitary time evolution of a closed system, often expressed as:\n\\[\n\\mathbf{c}(t) = e^{-\\tfrac{i}{\\hbar}H t} \\,\\mathbf{c}(0).\n\\]\n\n\n5.4.1 Time-Dependent Hamiltonian\nWhen \\(\\hat{H}(t)\\) varies explicitly with time, one no longer has a simple exponential solution. Instead, one can divide the time interval of interest into many small sub-intervals and approximate \\(\\hat{H}(t)\\) as constant in each interval. This procedure is related to the time-ordered exponential, but from a numerical perspective, we can simply implement a step-by-step integration (e.g., Euler, Runge–Kutta, or other higher-order methods) to construct \\(\\ket{\\psi(t_{n+1})}\\) from \\(\\ket{\\psi(t_n)}\\).\n\n\n5.4.2 Open Quantum Systems\nIn open quantum systems, the evolution of the density matrix \\(\\rho(t)\\) is often governed by the master equation:\n\\[\n\\frac{d\\rho(t)}{dt} = \\mathcal{L}[\\rho(t)],\n\\] where \\(\\mathcal{L}\\) is the so-called Liouvillian superoperator, which could contain both Hamiltonian (coherent) parts and dissipative terms. Numerically, one can vectorize \\(\\rho(t)\\) (flattening the matrix into a vector) and represent \\(\\mathcal{L}\\) as a matrix \\(\\mathcal{L}_{\\mathrm{mat}}\\). Then, the equation again has the familiar linear form:\n\\[\n\\frac{d\\mathbf{r}(t)}{dt} = \\mathcal{L}_{\\mathrm{mat}}\\;\\mathbf{r}(t).\n\\]\nHence, the same techniques (matrix diagonalization for analytical solutions, or time stepping methods like Euler, Runge–Kutta, etc. for numerical solutions) remain valid.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Ordinary Differential Equations</span>"
    ]
  },
  {
    "objectID": "lecture1/ordinary_differential_equations.html#conclusion",
    "href": "lecture1/ordinary_differential_equations.html#conclusion",
    "title": "5  Ordinary Differential Equations",
    "section": "5.5 Conclusion",
    "text": "5.5 Conclusion\nIn summary:\n\nAn Ordinary Differential Equation (ODE) involves a function of one variable and its derivatives.\nWhen an ODE is linear and time-independent, one can analytically solve it by diagonalizing the system matrix.\nFor more complicated (time-dependent or nonlinear) problems, numerical integration methods such as the Euler method can be applied.\nThe Euler method is conceptually simple but demands careful choice of time step to ensure stability, particularly when the system matrix has eigenvalues with large negative real parts or when fast decaying/oscillatory modes are present.\nThese ideas are directly applicable to quantum mechanical systems such as the Schrödinger equation or master equations for open systems. In the Schrödinger equation, diagonalization corresponds to finding energy eigenstates and frequencies, while in open quantum systems, vectorization plus diagonalization or numerical iteration handles both coherent and dissipative dynamics.\n\nThroughout the course, we will leverage these fundamental methods—both analytical techniques (e.g., diagonalization) and numerical approaches (e.g., Euler and more sophisticated solvers)—to simulate quantum systems efficiently and accurately.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Ordinary Differential Equations</span>"
    ]
  },
  {
    "objectID": "lecture1/phase_space_ode.html",
    "href": "lecture1/phase_space_ode.html",
    "title": "6  From Hamilton’s equations to the Liouville equation in phase space",
    "section": "",
    "text": "6.1 From Hamilton’s equations to Liouville’s continuity law\nIn the previous chapter we showed that many linear ordinary differential equations (ODEs) that appear in quantum mechanics can be solved elegantly by writing them in matrix form and diagonalising the matrix. In classical mechanics, however, the equations of motion\n\\[\n\\begin{cases}\n\\dot x \\,=\\, \\dfrac{p}{m},\\\\\n\\dot p \\,=\\, -\\dfrac{\\partial V (x)}{\\partial x}\n\\end{cases}\n\\tag{6.1}\\]\nbecome non‑linear as soon as the potential \\(V(x)\\) is non‑quadratic. Consequently the state vector \\(\\mathbf{y} = (x,p)^{\\mathsf T}\\) no longer satisfies a linear system \\(\\dot{\\mathbf{y}} = A\\,\\mathbf{y}\\). As an example, we will consider the Duffing-like oscillator with a quartic potential \\(V(x) = \\frac{1}{2} k x^2 + g x^4\\).\nIn Hamiltonian mechanics we usually track a single phase‑space point \\((x(t),p(t))\\) by solving the Hamilton equations in Equation 6.1. Yet many physical questions are statistical:\nAnswering these requires an equation for a phase‑space density \\(\\rho(x(t),p(t), t)\\), not individual trajectories. The Liouville equation supplies precisely that.\nThe Liouville equation describes how a classical probability density function in phase space evolves over time. It is a fundamental result in classical statistical mechanics and emerges directly from Hamilton’s equations.\nWe aim to derive: \\[\n\\frac{\\partial \\rho}{\\partial t} + \\{ \\rho, H \\} = 0\n\\]\nwhere:\nWe begin with the canonical equations of motion for a 1D system: \\[\n\\begin{cases}\n\\dot{x} = \\frac{\\partial H}{\\partial p}\\\\\n\\dot{p} = -\\frac{\\partial H}{\\partial x}\n\\end{cases}\n\\]\nThese equations describe the deterministic evolution of a point \\((x(t), p(t))\\) in phase space.\nLet \\(\\rho(x, p, t)\\) be the density of an ensemble of classical systems in phase space. To study how this density evolves along the flow of the system, we compute the total derivative: \\[\n\\frac{d}{dt} \\rho(x(t), p(t), t) =\n\\frac{\\partial \\rho}{\\partial t}\n+ \\frac{\\partial \\rho}{\\partial x} \\frac{dx}{dt}\n+ \\frac{\\partial \\rho}{\\partial p} \\frac{dp}{dt}\n\\]\nSubstituting Hamilton’s equations: \\[\n\\frac{d\\rho}{dt} =\n\\frac{\\partial \\rho}{\\partial t}\n+ \\frac{\\partial \\rho}{\\partial x} \\frac{\\partial H}{\\partial p}\n- \\frac{\\partial \\rho}{\\partial p} \\frac{\\partial H}{\\partial x}\n= \\frac{\\partial \\rho}{\\partial t} + \\{ \\rho, H \\}\n\\]\nIn Hamiltonian mechanics, the phase space flow is incompressible: it preserves the volume element \\(dx \\wedge dp\\). This implies that the density \\(\\rho\\) remains constant along each trajectory:\n\\[\n\\frac{d}{dt} \\rho(x(t), p(t), t) = 0\n\\]\nHence, we obtain: \\[\n\\frac{\\partial \\rho}{\\partial t} + \\{ \\rho, H \\} = 0 \\qquad \\text{or} \\qquad \\frac{\\partial \\rho}{\\partial t} = \\{H, \\rho\\}\n\\]\nThis is the Liouville equation.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>From Hamilton's equations to the Liouville equation in phase space</span>"
    ]
  },
  {
    "objectID": "lecture1/phase_space_ode.html#from-hamiltons-equations-to-liouvilles-continuity-law",
    "href": "lecture1/phase_space_ode.html#from-hamiltons-equations-to-liouvilles-continuity-law",
    "title": "6  From Hamilton’s equations to the Liouville equation in phase space",
    "section": "",
    "text": "Given ignorance about the exact initial state, how does a whole ensemble of points evolve?\nWhich quantities remain constant under the flow, and why?\n\n\n\n\n\n\n\\(\\rho(x, p, t)\\) is the probability density in phase space,\n\\(H(x, p)\\) is the Hamiltonian of the system,\n\\(\\{f, g\\} = \\frac{\\partial f}{\\partial x} \\frac{\\partial g}{\\partial p} - \\frac{\\partial f}{\\partial p} \\frac{\\partial g}{\\partial x}\\) denotes the Poisson bracket.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>From Hamilton's equations to the Liouville equation in phase space</span>"
    ]
  },
  {
    "objectID": "lecture1/phase_space_ode.html#physical-interpretation",
    "href": "lecture1/phase_space_ode.html#physical-interpretation",
    "title": "6  From Hamilton’s equations to the Liouville equation in phase space",
    "section": "6.2 Physical Interpretation",
    "text": "6.2 Physical Interpretation\n\nThe equation describes how a probability distribution in phase space flows under Hamiltonian evolution.\nThe term \\(\\{ \\rho, H \\}\\) encodes the flow of the distribution due to the system’s dynamics.\nThe total number of systems is conserved, and the phase-space density is transported without compression.\n\nIn short: Liouville’s theorem states that the probability density is constant along the trajectories of the system in phase space.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>From Hamilton's equations to the Liouville equation in phase space</span>"
    ]
  },
  {
    "objectID": "lecture1/phase_space_ode.html#discretizing-the-liouville-operator-with-finite-differences",
    "href": "lecture1/phase_space_ode.html#discretizing-the-liouville-operator-with-finite-differences",
    "title": "6  From Hamilton’s equations to the Liouville equation in phase space",
    "section": "6.3 Discretizing the Liouville Operator with Finite Differences",
    "text": "6.3 Discretizing the Liouville Operator with Finite Differences\nIn classical statistical mechanics, the Liouville equation governs the time evolution of a probability density in phase space. To simulate this numerically, we can discretize phase space and rewrite the Liouville operator as a sparse matrix, using finite difference approximations for derivatives.\nFor a 1D system with Hamiltonian \\(H(x, p) = \\frac{p^2}{2m} + V(x)\\), we compute:\n\\[\n\\{ H, \\rho \\} = \\frac{\\partial H}{\\partial x} \\frac{\\partial \\rho}{\\partial p} - \\frac{\\partial H}{\\partial p} \\frac{\\partial \\rho}{\\partial x} = \\frac{\\partial V}{\\partial x} \\frac{\\partial \\rho}{\\partial p} - \\frac{p}{m} \\frac{\\partial \\rho}{\\partial x}\n\\]\n\n6.3.1 Discretizing Phase Space\nWe define a uniform grid of \\(N_x\\) points over \\(x\\) and \\(N_p\\) points over \\(p\\):\n\n\\(x_i = x_0 + i \\cdot \\Delta x\\), for \\(i = 0, \\dots, N_x - 1\\)\n\\(p_j = p_0 + j \\cdot \\Delta p\\), for \\(j = 0, \\dots, N_p - 1\\)\n\nThe phase space density \\(\\rho(x_i, p_j)\\) is stored as a 2D array or flattened into a vector \\(\\vec{\\rho} \\in \\mathbb{R}^{N_x N_p}\\).\nWe now define central difference matrices for the derivatives. Using second-order central differences: \\[\n\\left. \\frac{\\partial \\rho}{\\partial x} \\right|_{x_i} \\approx\n\\frac{\\rho(x_{i+1}) - \\rho(x_{i-1})}{2\\Delta x}\n\\]\nThis corresponds to a matrix \\(D_x \\in \\mathbb{R}^{N_x \\times N_x}\\) with the stencil: \\[\nD_x = \\frac{1}{2 \\Delta x}\n\\begin{pmatrix}\n0 & 1 & 0 & \\cdots & 0 \\\\\n-1 & 0 & 1 & \\cdots & 0 \\\\\n0 & -1 & 0 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\ddots & 1 \\\\\n0 & 0 & \\cdots & -1 & 0 \\\\\n\\end{pmatrix}\n\\tag{6.2}\\]\nAnalogously: \\[\nD_p = \\frac{1}{2 \\Delta p}\n\\begin{pmatrix}\n0 & 1 & 0 & \\cdots & 0 \\\\\n-1 & 0 & 1 & \\cdots & 0 \\\\\n0 & -1 & 0 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\ddots & 1 \\\\\n0 & 0 & \\cdots & -1 & 0 \\\\\n\\end{pmatrix}\n\\tag{6.3}\\]\nBoth matrices are sparse, antisymmetric, and can be constructed with sparse matrix tools of scipy.sparse.\n\n\n6.3.2 Building the Liouville Matrix Operator\nOnce we flatten the 2D array \\(\\rho(x_i, p_j)\\) into a vector \\(\\vec{\\rho} \\in \\mathbb{R}^{N_x N_p}\\), we define:\n\n\\(P = \\text{diag}(p_j / m)\\) of shape \\(N_p \\times N_p\\)\n\\(\\partial_x V = \\text{diag}(\\partial_x V(x_i))\\) of shape \\(N_x \\times N_x\\)\n\nThen the full Liouville matrix \\(L\\) becomes:\n\\[\nL = (I_x \\otimes D_p) \\cdot (\\partial_x V \\otimes I_p) - (D_x \\otimes I_p) \\cdot (I_x \\otimes P)\n\\tag{6.4}\\]\nHere:\n\n\\(I_x\\), \\(I_p\\): identity matrices on position and momentum spaces\n\\(\\otimes\\): Kronecker product\n\nThis is a sparse matrix acting on \\(\\vec{\\rho}\\), and encodes the total effect of the classical flow in phase space.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>From Hamilton's equations to the Liouville equation in phase space</span>"
    ]
  },
  {
    "objectID": "lecture1/phase_space_ode.html#time-evolution",
    "href": "lecture1/phase_space_ode.html#time-evolution",
    "title": "6  From Hamilton’s equations to the Liouville equation in phase space",
    "section": "6.4 Time Evolution",
    "text": "6.4 Time Evolution\nWe can evolve the discretized density using an ODE solver: \\[\n\\frac{d \\vec{\\rho}}{dt} = L \\vec{\\rho}\n\\tag{6.5}\\]\nThus, we have reduced the problem to a linear ordinary differential equation (ODE) system, which can be solved using the standard tools discussed in Chapter 5.\nBefore concluding this section, let us summarize some important points:\n\nThe Liouville operator can be expressed as a sparse matrix using finite differences.\nPosition and momentum derivatives are replaced by central difference matrices.\nThe discretized Liouville equation is a linear ODE system for the phase-space density vector.\nThe phase space grid must be fine enough to resolve the flow.\nBoundary conditions (periodic, reflecting, absorbing) must be chosen according to the physics.\nThis approach is analogous to how quantum Hamiltonians are discretized into matrices using finite differences.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>From Hamilton's equations to the Liouville equation in phase space</span>"
    ]
  },
  {
    "objectID": "lecture1/phase_space_ode.html#sec-classical-nonlinear-oscillator",
    "href": "lecture1/phase_space_ode.html#sec-classical-nonlinear-oscillator",
    "title": "6  From Hamilton’s equations to the Liouville equation in phase space",
    "section": "6.5 Running example: a quartic non-linear oscillator",
    "text": "6.5 Running example: a quartic non-linear oscillator\nLet us keep the algebra to a minimum and pick the potential\n\\[\nV(x)=\\tfrac 12 k x^{2}+g x^{4},\n\\]\nwith \\(k&gt;0\\) (harmonic part) and \\(g&gt;0\\) (hardening quartic term). Its Hamiltonian reads\n\\[\nH(x,p)=\\frac{p^{2}}{2m}+V(x).\n\\]\nAlthough the equations of motion are non‑linear, we could still integrate them numerically using scipy.integrate.solve_ivp. However, this goes out of the scope of this course, as we are interested in linear ordinary differential equations and their matrix equivalents. To restore linearity we have to take a step back and study phase‑space functions rather than individual trajectories.\nWe can now construct the matrix operators for the Liouville equation following Equation 6.2, Equation 6.3, and Equation 6.4. We can take advantage of the tools provided by scipy.sparse to create the sparse matrices efficiently. We start by importing the necessary libraries\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy as sp\nimport scipy.sparse as sparse\nimport functools as ft\n\n# Define a Gaussian function, useful for initial conditions\ndef gaussian(x, mu, sigma):\n    \"\"\"Generate a Gaussian function.\"\"\"\n    norm_factor = 1 / (sigma * np.sqrt(2 * np.pi))\n    return np.exp(-0.5 * ((x - mu) / sigma) ** 2) * norm_factor\n\n\nAnd then we define the grid and the operators for the phase space:\n\n\nN_x = 150 # Number of grid points in position space\nN_px = 150 # Number of grid points in momentum space\nx_bound = 5 # Position space boundary\npx_bound = 5 # Momentum space boundary\n\n# Identity matrices for the different dimensions\nIx = sparse.eye(N_x)\nIpx = sparse.eye(N_px)\n\nx_list = np.linspace(-x_bound, x_bound, N_x)\npx_list = np.linspace(-px_bound, px_bound, N_px)\n\ndx = x_list[1] - x_list[0]\ndpx = px_list[1] - px_list[0]\n\n# Define the operators\nx_op = sparse.diags(x_list)\npx_op = sparse.diags(px_list)\n\n# Use central differences for derivatives\nd_x_op = sparse.diags([np.ones(N_x-1)/(2*dx), \n                -np.ones(N_x-1)/(2*dx)], offsets=[1, -1])\nd_px_op = sparse.diags([np.ones(N_px-1)/(2*dpx),\n                -np.ones(N_px-1)/(2*dpx)], offsets=[1, -1])\n\n# Create the full operator for the 4D phase space\nx = ft.reduce(sparse.kron, [Ipx, x_op]).todia()\npx = ft.reduce(sparse.kron, [px_op, Ix]).todia()\n\nd_x = ft.reduce(sparse.kron, [Ipx, d_x_op]).todia()\nd_px = ft.reduce(sparse.kron, [d_px_op, Ix]).todia()\n\n\nWe can now compute the time evolution defined by Equation 6.5 by using the Euler method described in Section 5.3:\n\n\nm = 0.5 # Mass of the particle\nk = 2.0 # Spring constant\nG = 0.15 # Nonlinear constant\n\ndV_dx = k * x + 4 * G * x @ x @ x\n\n# Liouville operator\nL = dV_dx @ d_px - (px / m) @ d_x\n\nt_list = np.linspace(0, 2, 100000)\n\n# Initial state: we will use a Gaussian wave packet to avoid singularities\nx_0 = gaussian(x_list, 1.0, np.sqrt(0.5))\np_0 = gaussian(px_list, 0.1, np.sqrt(0.5))\nrho_0 = np.kron(p_0, x_0)\n\nx_t = np.zeros(t_list.shape[0])\nv_t = np.zeros(t_list.shape[0])\nE_t = np.zeros(t_list.shape[0])\n\nx_t[0] = rho_0.dot(x.dot(rho_0))\nv_t[0] = rho_0.dot(px.dot(rho_0)) / m\nE_t[0] = (0.5 * m * v_t[0]**2 + 0.5 * k * x_t[0]**2 + G * x_t[0]**4)\n\nrho_t = [rho_0.copy()]\nfor i, t in enumerate(t_list[1:], 1):\n    drho_dt = L @ rho_t[-1]\n\n    # Simple Euler integration\n    rho_t.append(rho_t[-1] + drho_dt * (t_list[1] - t_list[0]))\n\n    x_t[i] = rho_t[-1].dot(x.dot(rho_t[-1]))\n    v_t[i] = rho_t[-1].dot(px.dot(rho_t[-1])) / m\n    E_t[i] = (0.5 * m * v_t[i]**2 + 0.5 * k * x_t[i]**2 + G * x_t[i]**4)\n\n\nAnd we can finally visualize the evolution of the phase space density \\(\\rho(x, p, t)\\) as a 2D plot animation:\n\n\nfrom IPython.display import HTML\nfrom matplotlib.animation import FuncAnimation\n\nplt.rcParams.update({\"font.size\": 8})\nfig, ax = plt.subplots(figsize=(4.6, 2.8))\n\nfig.suptitle(r\"Phase space density $\\rho(x, p, t)$ evolution\")\n\nimg = ax.pcolormesh(x_list, px_list, rho_t[0].reshape(N_x, N_px),\n                    shading=\"gouraud\", rasterized=True,\n                    vmin=-0.3, vmax=0.3, cmap=\"PuOr\")\nax.set_xlabel(\"Position $x$\")\nax.set_ylabel(\"Momentum $p_x$\")\nax.set_xlim(-5, 5)\nax.set_ylim(-5, 5)\n\ndef animate(i):\n    img.set_array(rho_t[i])\n    return img,\n\nani = FuncAnimation(fig, animate, frames=range(0, len(t_list), 2000), interval=100, blit=True)\n\nplt.close(fig)\nHTML(ani.to_jshtml())\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nIt is instructive to compare the results of this simulation with the quantum case, as shown in Appendix B. Notice how in the quantum case the Wigner (which is the quantum analogue of the phase space density) can take negative values, while in the classical case the phase space density is always non-negative.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>From Hamilton's equations to the Liouville equation in phase space</span>"
    ]
  },
  {
    "objectID": "lecture1/quantum_objects_numpy.html",
    "href": "lecture1/quantum_objects_numpy.html",
    "title": "7  Representing Quantum States and Operators with NumPy",
    "section": "",
    "text": "7.1 Pauli Operators\nIn quantum mechanics, states and observables are represented using the algebra of Hilbert spaces. However, their infinite dimensions is incompatible with numerical simulations, that always requires finite elements. Hence, we truncate Hilbert spaces to a finite size, allowing us to run the quantum calculation on a computer. We can thus say that the whole problem of numerical quantum mechanics is then reduced to a problem of linear algebra. However, the intricated tensor structures of many-body Hilbert spaces requires also a powerful organization of the code and an easy way to access relevant information.\nIn the following we consider a system with a Hilbert space of dimension \\(d\\). The set of basis states \\(\\{|k\\rangle: k=1, \\ldots, d\\}\\) form an orthonormal basis, i.e., \\(\\left\\langle k \\mid k^{\\prime}\\right\\rangle=\\delta_{k, k^{\\prime}}\\). In general there are systems with an infinite dimensional Hilbert space, or systems, where the dimension is too large to be tractable on a computer. In this case \\(d\\) denotes the number of truncated basis states, which is used in the numerical simulation. For a given choice of basis states we can express any state vector and any operator as\n\\[\n|\\psi\\rangle = \\sum_{k=1}^{d} c_{k} |k\\rangle, \\quad \\hat{A}=\\sum_{k, l} A_{k l}|k\\rangle\\langle l|,\n\\]\nwhere \\(c_{k} = \\langle k \\mid \\psi\\rangle\\) and \\(A_{k l}=\\langle k| \\hat{A}|l\\rangle\\). Therefore, in numerical simulations we represent states by vectors and operators by matrices according to the mapping\n\\[\n|\\psi\\rangle \\mapsto \\vec{\\psi}=\\left(\\begin{array}{c}\nc_{1} \\\\\nc_{2} \\\\\n\\vdots \\\\\nc_{d}\n\\end{array}\\right), \\quad \\hat{A} \\mapsto A=\\left(\\begin{array}{cccc}\nA_{11} & A_{12} & \\ldots & A_{1 d} \\\\\nA_{21} & A_{22} & \\ldots & A_{2 d} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nA_{d 1} & A_{d 2} & \\ldots & A_{d d}\n\\end{array}\\right) .\n\\]\nThe left and right operations of an operator on a vector then simply translate into matrix vector multiplications,\n\\[\n\\hat{A}|\\psi\\rangle \\mapsto \\texttt{np.dot(A, psi)}, \\quad \\langle \\psi| \\hat{A} \\mapsto \\texttt{np.dot(np.conj(psi.T), A)},\n\\]\nwhere in Numpy np.conj(psi.T) is the hermitian transpose of a matrix or vector.\nThe Pauli operators are fundamental in quantum mechanics, especially in the context of qubits. They are represented as matrices in a two-dimensional Hilbert space, which is the simplest non-trivial quantum system.\n\\[\n\\hat{\\sigma}_{x} \\mapsto\\left(\\begin{array}{cc}\n0 & 1  \\\\\n1 & 0\n\\end{array}\\right), \\quad \\hat{\\sigma}_{y} \\mapsto\\left(\\begin{array}{cc}\n0 & i \\\\\n-i & 0\n\\end{array}\\right), \\quad \\hat{\\sigma}_{z} \\mapsto\\left(\\begin{array}{cc}\n1 & 0 \\\\\n0 & -1\n\\end{array}\\right) .\n\\]\nIn Numpy we simply define the corresponding matrices",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Representing Quantum States and Operators with NumPy</span>"
    ]
  },
  {
    "objectID": "lecture1/quantum_objects_numpy.html#pauli-operators",
    "href": "lecture1/quantum_objects_numpy.html#pauli-operators",
    "title": "7  Representing Quantum States and Operators with NumPy",
    "section": "",
    "text": "import numpy as np\n\nsx = np.array([[0, 1], [1, 0]])\nsy = np.array([[0, 1j], [-1j, 0]])\nsz = np.array([[1, 0], [0, -1]])",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Representing Quantum States and Operators with NumPy</span>"
    ]
  },
  {
    "objectID": "lecture1/quantum_objects_numpy.html#harmonic-oscillator",
    "href": "lecture1/quantum_objects_numpy.html#harmonic-oscillator",
    "title": "7  Representing Quantum States and Operators with NumPy",
    "section": "7.2 Harmonic Oscillator",
    "text": "7.2 Harmonic Oscillator\nIn a Hilbert space of dimension \\(N\\), quantum states can be represented as vectors, and operators as matrices. Here we demonstrate the destroy operator, \\(a\\), which lowers the state by one quantum number. For a detailed discussion on the quantum harmonic oscillator and the bosonic annihilation operator, refer to Appendix A.\nFor a harmonic oscillator with number states \\(|n\\rangle\\) the only nonzero matrix elements of the annihilation operator \\(\\hat{a}\\) are given by \\(\\langle n-1| \\hat{a}|n\\rangle=\\sqrt{n}\\)\n\\[\n\\hat{a} \\mapsto A=\\left(\\begin{array}{cccccc}\n0 & 1 & \\ldots & \\cdots & \\ldots & 0  \\\\\n0 & 0 & \\sqrt{2} & \\ldots & \\cdots & 0 \\\\\n0 & 0 & 0 & \\sqrt{3} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\ddots & \\vdots \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & 0 & \\sqrt{d-1} \\\\\n0 & 0 & 0 & \\cdots & \\cdots & 0\n\\end{array}\\right)\n\\]\nThis operator acts on Fock states to lower their quantum number by one, with a factor of \\(\\sqrt{n}\\), where \\(n\\) is the quantum number of the initial state. In other words, \\(\\hat{a}|n\\rangle = \\sqrt{n}|n-1\\rangle\\). In the following code, we define the destroy operator by using NumPy, and we also define some Fock states for demonstration.\nIn Numpy we use the command np.diag(v, k=r), which creates a diagonal matrix with the elements of the vector v placed in the \\(r\\)-th diagonal \\((r=0, \\pm 1, \\pm 2, \\ldots)\\).\n\n\ndef destroy(d):\n    # creates a vector of the d-1 off-diagonal elements\n    v=np.sqrt( np.arange(d-1) )\n    # matrix with the elements of vec placed in the upper diagonal\n    a=np.diag(v,k=1)\n    return a\n\n# Define the fock states\ndef fock(d, i):\n    res = np.zeros(d)\n    res[i] = 1\n    return res\n\nd = 7\nzero_state = fock(d, 0)\none_state = fock(d, 1)\ntwo_state = fock(d, 2)\nthree_state = fock(d, 3)\n\ndestroy_operator = destroy(d)\ndestroy_operator\n\narray([[0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        ],\n       [0.        , 0.        , 1.        , 0.        , 0.        ,\n        0.        , 0.        ],\n       [0.        , 0.        , 0.        , 1.41421356, 0.        ,\n        0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 1.73205081,\n        0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        ,\n        2.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 2.23606798],\n       [0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        ]])\n\n\n\nOther operators (e.g., \\(\\hat{a}^{\\dagger}, \\hat{a}^{\\dagger} \\hat{a}\\)) can be obtained by a hermitian transpose\n\\[\n\\hat{a}^{\\dagger} \\mapsto\\texttt{np.conj(a.T)}\n\\]\nand matrix multiplications\n\\[\n\\hat{a}^{\\dagger} \\hat{a} \\mapsto \\texttt{np.matmul(np.conj(a.T) , a)} \\, .\n\\]\nNote that in some cases this introduces truncation artifacts. For example, the matrix for the operator \\(M=\\texttt{np.matmul(a, np.conj(a.T))}\\) has a zero diagonal element \\(M[d, d]=0\\) inherited from the matrix \\(\\texttt{np.conj(a.T))}\\), while the same operator constructed in a different way, \\(M_2=\\texttt{np.conj(a.T) * a + np.eye(d)}\\), does not. This can be avoided by constructing this operator explicitly. Note that this type of truncation artifacts are related to the fact that in a infinite Hilbert space \\(\\mathrm{Tr}([a, a^{\\dagger}])\\neq 0\\) (actually, striclty speaking, \\(=\\infty\\)) as a consequence of the canonical commutation relation. On the contrary, in a finite Hilbert space for any two operators \\(O_1, O_2\\), \\(\\mathrm{Tr}([O_1, O_2])= 0\\). Taking a dimension \\(d\\) large enough allows to make these artifacts a negligible error in the whole computation.\n\n7.2.1 Action of the Destroy Operator on a Fock State\nThe action of the destroy operator \\(a\\) on a Fock state \\(|n\\rangle\\) lowers the state by one quantum number, multiplied by a factor \\(\\sqrt{n}\\). For example, applying \\(a\\) to the state \\(|3\\rangle\\) yields:\n\\[\n\\hat{a}|3\\rangle = \\sqrt{3}|2\\rangle\n\\]\nThis demonstrates the lowering action of the destroy operator with a specific factor, dependent on the quantum number of the state being acted upon.\n\n\n# Apply the destroy operator on the one state\nresult_state = np.dot(destroy_operator, three_state)\n\nprint(\"Resulting State:\")\nresult_state\n\nResulting State:\n\n\narray([0.        , 0.        , 1.41421356, 0.        , 0.        ,\n       0.        , 0.        ])",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Representing Quantum States and Operators with NumPy</span>"
    ]
  },
  {
    "objectID": "lecture1/quantum_objects_numpy.html#partial-trace",
    "href": "lecture1/quantum_objects_numpy.html#partial-trace",
    "title": "7  Representing Quantum States and Operators with NumPy",
    "section": "7.3 Partial Trace",
    "text": "7.3 Partial Trace\nIn Section 3.1.4, we have already discussed the concept of tensor products. Here we will introduce the partial trace, a crucial operation in quantum mechanics that allows us to focus on a subsystem of a larger composite system.\nThe partial trace over a subsystem, say \\(B\\), of a composite system \\(AB\\), mathematically expresses as “tracing out” \\(B\\), leaving the reduced state of \\(A\\). For a bipartite state \\(\\rho_{AB}\\), the partial trace over \\(B\\) is:\n\\[\n\\text{Tr}_B(\\hat{\\rho}_{AB}) = \\sum_{i \\in \\mathcal{H}_B} \\langle i| \\hat{\\rho}_{AB} |i\\rangle\n\\]\nwhere \\(\\{|i\\rangle\\}\\) forms a complete basis for subsystem \\(B\\).\nLet’s try it with an entangled Bell’s state between two qubits:\n\\[\n\\vert \\phi^+ \\rangle = \\frac{1}{\\sqrt{2}} \\left( \\vert 0, 0 \\rangle + \\vert 1, 1 \\rangle \\right)\n\\]\n\n\ndef ptrace(psi, subspace_to_keep, dim_subspace):\n    dim1, dim2 = dim_subspace\n\n    rho = np.outer(psi, psi.conj())\n\n    # Reshape rho to separate the subsystems' degrees of freedom\n    rho_reshaped = rho.reshape(dim1, dim2, dim1, dim2)\n\n    if subspace_to_keep == 1:\n        # Perform the trace over the second subsystem\n        traced_out = np.trace(rho_reshaped, axis1=1, axis2=3)\n    elif subspace_to_keep == 2:\n        # Perform the trace over the first subsystem\n        traced_out = np.trace(rho_reshaped, axis1=0, axis2=2)\n    else:\n        raise ValueError(\"subspace_to_keep must be either 1 or 2.\")\n\n    return traced_out\n\n# Bell state between two qubits\nphi_plus = ( np.kron(fock(2, 1), fock(2, 1)) + np.kron(fock(2, 0), fock(2, 0)) ) / np.sqrt(2)\n\n# Reduced density matrix of the first qubit\nrho_1 = ptrace(phi_plus, 1, (2, 2))\nrho_1\n\narray([[0.5, 0. ],\n       [0. , 0.5]])",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Representing Quantum States and Operators with NumPy</span>"
    ]
  },
  {
    "objectID": "lecture1/quantum_objects_numpy.html#why-qutip",
    "href": "lecture1/quantum_objects_numpy.html#why-qutip",
    "title": "7  Representing Quantum States and Operators with NumPy",
    "section": "7.4 Why QuTiP?",
    "text": "7.4 Why QuTiP?\nWhile NumPy and SciPy are powerful tools for numerical computations, they lack specific functionalities for efficiently handling complex quantum systems. QuTiP is designed to fill this gap, offering features such as:\n\nEasy manipulation and visualization of quantum objects.\nSupport for operations on states and operators in different Hilbert spaces.\nTools for dealing with composite systems, partial traces, and superoperators. It is like to have the book “Quantum noise” (by Gardiner and Zoller) already implemented in your laptop!\n\nIn the next chapters, we’ll explore how QuTiP simplifies these tasks, making it an invaluable tool for quantum optics simulations.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Representing Quantum States and Operators with NumPy</span>"
    ]
  },
  {
    "objectID": "lecture1/qutip_introduction.html",
    "href": "lecture1/qutip_introduction.html",
    "title": "8  Introduction to QuTiP",
    "section": "",
    "text": "8.1 Quantum Operators\nThe QuTiP package can be imported with\nIt can also be imported with the command from qutip import *, that automatically imports all the QuTiP functions. However, here we use the first method, in order to explicitly see the QuTiP functions.\nQuantum operators play a crucial role in the formulation of quantum mechanics, representing physical observables and operations on quantum states. In QuTiP, operators are represented as Qobj instances, just like quantum states. This section introduces the creation and manipulation of quantum operators.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to QuTiP</span>"
    ]
  },
  {
    "objectID": "lecture1/qutip_introduction.html#quantum-operators",
    "href": "lecture1/qutip_introduction.html#quantum-operators",
    "title": "8  Introduction to QuTiP",
    "section": "",
    "text": "8.1.1 Creating Operators\nOperators in quantum mechanics can represent measurements, such as position or momentum, and transformations, such as rotation. Let’s see how we can define some common operators in QuTiP.\n\nThe Annihilation Operator of the Quantum Harmonic oscillator\nThe harmonic oscillator is a fundamental model in quantum mechanics for understanding various physical systems. Its quantization leads to the concept of creation and annihilation operators, which respectively increase and decrease the energy of the system by one quantum of energy.\nThe annihilation operator, often denoted by \\(\\hat{a}\\), acts on a quantum state to reduce its quantum number. The action of \\(\\hat{a}\\) on a state \\(|n\\rangle\\) is defined as:\n\\[\n\\hat{a} |n\\rangle = \\sqrt{n} |n-1\\rangle\n\\]\nHere, \\(|n\\rangle\\) represents a quantum state with \\(n\\) quanta of energy (also known as a Fock state), and \\(\\sqrt{n}\\) is the normalization factor. The matrix representation of the annihilation operator in an \\(d\\)-dimensional Hilbert space is given by an upper triangular matrix with the square roots of natural numbers as its off-diagonal elements.\n\n\n# Define the annihilation operator for d-dimensional Hilbert space\nd = 7\n\na = qutip.destroy(d)\n\nprint(\"Annihilation operator (a) for d=7:\")\na\n\nAnnihilation operator (a) for d=7:\n\n\nQuantum object: dims=[[7], [7]], shape=(7, 7), type='oper', dtype=Dia, isherm=False\nQobj data =\n[[0.         1.         0.         0.         0.         0.\n  0.        ]\n [0.         0.         1.41421356 0.         0.         0.\n  0.        ]\n [0.         0.         0.         1.73205081 0.         0.\n  0.        ]\n [0.         0.         0.         0.         2.         0.\n  0.        ]\n [0.         0.         0.         0.         0.         2.23606798\n  0.        ]\n [0.         0.         0.         0.         0.         0.\n  2.44948974]\n [0.         0.         0.         0.         0.         0.\n  0.        ]]\n\n\n\n\n\nPauli Matrices\nThe Pauli matrices are fundamental in the study of quantum mechanics, representing the spin operators for a spin-1/2 particle and quantum two-level systems.\n\\[\n\\sigma_x = \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}, \\ \\sigma_y = \\begin{pmatrix} 0 & -i \\\\ i & 0 \\end{pmatrix}, \\ \\sigma_z = \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix}\n\\]\nWe can define these matrices in QuTiP as follows:\n\n\nsigma_x = qutip.sigmax()\nsigma_y = qutip.sigmay()\nsigma_z = qutip.sigmaz()\n\nprint(\"Sigma X:\")\ndisplay(sigma_x)\nprint(\"\\n\")\nprint(\"Sigma Y:\")\ndisplay(sigma_y)\nprint(\"\\n\")\nprint(\"Sigma Z:\")\nsigma_z\n\nSigma X:\n\n\nQuantum object: dims=[[2], [2]], shape=(2, 2), type='oper', dtype=CSR, isherm=True\nQobj data =\n[[0. 1.]\n [1. 0.]]\n\n\n\n\nSigma Y:\n\n\nQuantum object: dims=[[2], [2]], shape=(2, 2), type='oper', dtype=CSR, isherm=True\nQobj data =\n[[0.+0.j 0.-1.j]\n [0.+1.j 0.+0.j]]\n\n\n\n\nSigma Z:\n\n\nQuantum object: dims=[[2], [2]], shape=(2, 2), type='oper', dtype=CSR, isherm=True\nQobj data =\n[[ 1.  0.]\n [ 0. -1.]]\n\n\n\n\n\n\n8.1.2 Operator Functions and Operations\nQuTiP supports various operations on operators, including addition, multiplication (both scalar and matrix), and the commutator. These operations are essential for constructing Hamiltonians, calculating observables, and more.\n\nExample: Commutator of Pauli Matrices\nThe commutator of two operators \\(A\\) and \\(B\\) is defined as \\([A, B] = AB - BA\\). Let’s calculate the commutator of \\(\\sigma_x\\) and \\(\\sigma_y\\).\n\n\ncommutator_xy = qutip.commutator(sigma_x, sigma_y)\nprint(\"Commutator of Sigma X and Sigma Y:\")\ndisplay(commutator_xy)\ncommutator_xy == 2j * sigma_z\n\nCommutator of Sigma X and Sigma Y:\n\n\nQuantum object: dims=[[2], [2]], shape=(2, 2), type='oper', dtype=CSR, isherm=False\nQobj data =\n[[0.+2.j 0.+0.j]\n [0.+0.j 0.-2.j]]\n\n\nTrue",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to QuTiP</span>"
    ]
  },
  {
    "objectID": "lecture1/qutip_introduction.html#quantum-states",
    "href": "lecture1/qutip_introduction.html#quantum-states",
    "title": "8  Introduction to QuTiP",
    "section": "8.2 Quantum States",
    "text": "8.2 Quantum States\nQuantum states describe the state of a quantum system. In QuTiP, states are represented again as Qobj instances. This section focuses on the representation and manipulation of quantum states.\n\n8.2.1 Fock States\nThe most basic quantum states are the fock states, often denoted as \\(|n\\rangle\\) (with \\(n \\in \\mathbb{N}\\)). Let’s see how we can create these in QuTiP.\n\n\n8.2.2 Superposition States\nQuantum mechanics allows particles to be in a superposition of states. Let’s create a superposition state.\n\\[\n\\vert \\psi \\rangle = \\frac{1}{\\sqrt{2}} \\left( \\vert 0 \\rangle + \\vert 1 \\rangle \\right)\n\\]\n\n\nfock_0 = qutip.fock(d, 0)  # Fock state |0&gt;\nfock_1 = qutip.fock(d, 1)  # Fock state |1&gt;\n\n# Creating a superposition state\nsuperposition_state = (fock_0 + fock_1).unit()  # Normalize the state\n\nprint(\"Superposition state:\")\nsuperposition_state\n\nSuperposition state:\n\n\nQuantum object: dims=[[7], [1]], shape=(7, 1), type='ket', dtype=Dense\nQobj data =\n[[0.70710678]\n [0.70710678]\n [0.        ]\n [0.        ]\n [0.        ]\n [0.        ]\n [0.        ]]\n\n\n\n\n\n8.2.3 Coherent States\nCoherent states in QuTiP represent quantum states closest to classical waves, defined as \\[\n|\\alpha\\rangle = e^{-|\\alpha|^2/2} \\sum_{n=0}^{\\infty} \\frac{\\alpha^n}{\\sqrt{n!}} |n\\rangle \\, ,\n\\]\nwith minimal uncertainty.\nThe coherent state is an eigenstate of the annihilation operator \\[\n\\hat{a} \\vert \\alpha \\rangle = \\alpha \\vert \\alpha \\rangle\n\\]\n\n\n\n\n\n\nWarning!\n\n\n\nRemember that every Qobj lives in a truncated Hilbert space. If the \\(\\alpha\\) value is too large, the state will become a non-physical state because it will touch the high energy levels of the truncated Hilbert space.\n\n\n\n\nalpha = 0.8\ncoherent_state = qutip.coherent(d, alpha)\n\ncoherent_state\n\nQuantum object: dims=[[7], [1]], shape=(7, 1), type='ket', dtype=Dense\nQobj data =\n[[0.72614904]\n [0.58091919]\n [0.32861796]\n [0.1517784 ]\n [0.06073653]\n [0.02159193]\n [0.00767346]]\n\n\n\nLet’s compute the fidelity between \\(\\vert \\alpha \\rangle\\) and \\(\\hat{a} \\vert \\alpha \\rangle / \\alpha\\).\n\n\nqutip.fidelity(a * coherent_state / alpha, coherent_state)\n\nnp.float64(0.9999661194274998)\n\n\n\n\n\n8.2.4 Spin States\n\n\nqutip.spin_state(0.5, -1)\n\nQuantum object: dims=[[2], [1]], shape=(2, 1), type='ket', dtype=Dense\nQobj data =\n[[0.]\n [1.]]\n\n\n\n\n\n8.2.5 Density Matrices\nQuantum states can also be represented using density matrices, which are useful for describing mixed states.\n\nCreating a Density Matrix\nLet’s convert our superposition state into a density matrix.\n\n\n# Creating a density matrix from a state\ndensity_matrix = superposition_state * superposition_state.dag()  # Outer product\n\nprint(\"Density matrix of the superposition state:\")\ndensity_matrix\n\nDensity matrix of the superposition state:\n\n\nQuantum object: dims=[[7], [7]], shape=(7, 7), type='oper', dtype=Dense, isherm=True\nQobj data =\n[[0.5 0.5 0.  0.  0.  0.  0. ]\n [0.5 0.5 0.  0.  0.  0.  0. ]\n [0.  0.  0.  0.  0.  0.  0. ]\n [0.  0.  0.  0.  0.  0.  0. ]\n [0.  0.  0.  0.  0.  0.  0. ]\n [0.  0.  0.  0.  0.  0.  0. ]\n [0.  0.  0.  0.  0.  0.  0. ]]\n\n\n\n\n\n\n8.2.6 Partial Trace\nThe partial trace over a subsystem, say \\(B\\), of a composite system \\(AB\\), mathematically expresses as “tracing out” \\(B\\), leaving the reduced state of \\(A\\). For a bipartite state \\(\\rho_{AB}\\), the partial trace over \\(B\\) is:\n\\[\n\\text{Tr}_B(\\hat{\\rho}_{AB}) = \\sum_{i \\in \\mathcal{H}_B} \\langle i| \\hat{\\rho}_{AB} |i\\rangle\n\\]\nwhere \\(\\{|i\\rangle\\}\\) forms a complete basis for subsystem \\(B\\).\nLet’s try it with an entangled Bell’s state between two qubits:\n\\[\n\\vert \\phi^+ \\rangle = \\frac{1}{\\sqrt{2}} \\left( \\vert 0, 0 \\rangle + \\vert 1, 1 \\rangle \\right)\n\\]\n\n\n# Bell state between two qubits\nphi_plus = ( qutip.tensor(qutip.spin_state(1/2, -1), qutip.spin_state(1/2, -1)) + qutip.tensor(qutip.spin_state(1/2, 1), qutip.spin_state(1/2, 1)) ).unit()\n\n# Reduced density matrix of the first qubit\nrho_1 = qutip.ptrace(phi_plus, 1)\nrho_1\n\nQuantum object: dims=[[2], [2]], shape=(2, 2), type='oper', dtype=Dense, isherm=True\nQobj data =\n[[0.5 0. ]\n [0.  0.5]]\n\n\n\nWe now apply the partial trace to a more complicated state, that is composed by two bosonic modes and two spins \\(\\vert j_1, m_1 \\rangle\\) and \\(\\vert j_2, m_2 \\rangle\\), with \\(j_1 = 1\\) and \\(j_2 = \\frac{1}{2}\\), \\(m_1 = 0\\), and \\(m_2 = 1\\).\n\n\nj1 = 1\nj2 = 1/2\nm1 = 0\nm2 = 1\n\n\npsi = qutip.tensor(qutip.fock(d, 3), qutip.fock(d, 1), qutip.spin_state(j1, 0), qutip.spin_state(j2, 1))\n\n# Trace only the second spin state\nrho_0 = qutip.ptrace(psi, [0, 1, 2])\ndisplay(rho_0)\n\n# Trace only the first bosonic mode and the second spin state\nrho_1 = qutip.ptrace(psi, [1, 2])\ndisplay(rho_1)\n\n# Trace all except the second bosonic mode\nrho_2 = qutip.ptrace(psi, [1])\nrho_2\n\nQuantum object: dims=[[7, 7, 3], [7, 7, 3]], shape=(147, 147), type='oper', dtype=Dense, isherm=True\nQobj data =\n[[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]]\n\n\nQuantum object: dims=[[7, 3], [7, 3]], shape=(21, 21), type='oper', dtype=Dense, isherm=True\nQobj data =\n[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n\n\nQuantum object: dims=[[7], [7]], shape=(7, 7), type='oper', dtype=Dense, isherm=True\nQobj data =\n[[0. 0. 0. 0. 0. 0. 0.]\n [0. 1. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0.]]",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to QuTiP</span>"
    ]
  },
  {
    "objectID": "lecture1/qutip_introduction.html#eigenstates-and-eigenvalues",
    "href": "lecture1/qutip_introduction.html#eigenstates-and-eigenvalues",
    "title": "8  Introduction to QuTiP",
    "section": "8.3 Eigenstates and Eigenvalues",
    "text": "8.3 Eigenstates and Eigenvalues\nThe eigenstates and eigenvalues of a system or an operator provide crucial insights into its properties. Let’s explore how to calculate these in QuTiP.\n\n\n# Example: Eigenstates and eigenvalues of Pauli Z\neigenvalues, eigenstates = sigma_z.eigenstates()\n\nprint(\"Eigenvalues of Sigma Z:\")\ndisplay(eigenvalues)\nprint(\"\\n\")\nprint(\"Eigenstates of Sigma Z:\")\ndisplay(eigenstates)\n\nEigenvalues of Sigma Z:\n\n\narray([-1.,  1.])\n\n\n\n\nEigenstates of Sigma Z:\n\n\narray([Quantum object: dims=[[2], [1]], shape=(2, 1), type='ket', dtype=Dense\n       Qobj data =\n       [[ 0.]\n        [-1.]]                                                               ,\n       Quantum object: dims=[[2], [1]], shape=(2, 1), type='ket', dtype=Dense\n       Qobj data =\n       [[-1.]\n        [-0.]]                                                               ],\n      dtype=object)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to QuTiP</span>"
    ]
  },
  {
    "objectID": "lecture1/qutip_introduction.html#computing-expectation-values",
    "href": "lecture1/qutip_introduction.html#computing-expectation-values",
    "title": "8  Introduction to QuTiP",
    "section": "8.4 Computing Expectation Values",
    "text": "8.4 Computing Expectation Values\nThe expectation value of an operator provides insight into the average outcome of a quantum measurement. For a quantum state \\(|\\psi\\rangle\\) and an operator \\(\\hat{O}\\), the expectation value is given by:\n\\[\n\\langle \\hat{O} \\rangle = \\langle\\psi| \\hat{O} |\\psi\\rangle\n\\]\nExpectation values are crucial for predicting measurable quantities in quantum mechanics. Let’s compute the expectation value of the number operator \\(\\hat{n} = \\hat{a}^\\dagger \\hat{a}\\) for a coherent state, which represents a quantum state closest to a classical harmonic oscillator.\n\n\n# Define the coherent state |psi&gt; with alpha=2\nalpha = 0.8\npsi = qutip.coherent(d, alpha)\n\n# Define the number operator n = a.dag() * a\nn = a.dag() * a\n\n# Compute the expectation value of n for the state |psi&gt;\nexpectation_value_n = qutip.expect(n, psi)\n\nprint(\"Expectation value of the number operator for |psi&gt;:\")\ndisplay(expectation_value_n)\nprint(\"\\n\")\nprint(\"The squared modulus of alpha is:\")\ndisplay(abs(alpha) ** 2)\n\nExpectation value of the number operator for |psi&gt;:\n\n\n0.639996733025295\n\n\n\n\nThe squared modulus of alpha is:\n\n\n0.6400000000000001",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to QuTiP</span>"
    ]
  },
  {
    "objectID": "lecture1/qutip_introduction.html#a-complete-example-the-quantum-harmonic-oscillator",
    "href": "lecture1/qutip_introduction.html#a-complete-example-the-quantum-harmonic-oscillator",
    "title": "8  Introduction to QuTiP",
    "section": "8.5 A complete example: the Quantum Harmonic Oscillator",
    "text": "8.5 A complete example: the Quantum Harmonic Oscillator\nIn Appendix A we have already defined the quantum harmonic oscillator, which is a fundamental model in quantum mechanics. In the energy eigenbasis, the quantum harmonic oscillator is described by the Hamiltonian\n\\[\n\\hat{H} = \\omega \\hat{a}^\\dagger \\hat{a} \\, ,\n\\]\nwhere \\(\\omega\\) is the resonance frequency and \\(\\hat{a}\\) is the bosonic annihilation operator. In this basis, the Hamiltonian is indeed diagonal, with eigenvalues \\(E_n = \\omega n\\), where \\(n \\in \\mathbb{N}\\).\n\n\nN = 120 # Hilbert space cutoff\nw = 1 # Resonance frequency of the harmonic oscillator\n\na = qutip.destroy(N)\n\nH = w * a.dag() * a\n\nH\n\nQuantum object: dims=[[120], [120]], shape=(120, 120), type='oper', dtype=Dia, isherm=True\nQobj data =\n[[  0.   0.   0. ...   0.   0.   0.]\n [  0.   1.   0. ...   0.   0.   0.]\n [  0.   0.   2. ...   0.   0.   0.]\n ...\n [  0.   0.   0. ... 117.   0.   0.]\n [  0.   0.   0. ...   0. 118.   0.]\n [  0.   0.   0. ...   0.   0. 119.]]",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to QuTiP</span>"
    ]
  },
  {
    "objectID": "lecture1/qutip_introduction.html#passing-in-the-position-basis",
    "href": "lecture1/qutip_introduction.html#passing-in-the-position-basis",
    "title": "8  Introduction to QuTiP",
    "section": "8.6 Passing in the position basis",
    "text": "8.6 Passing in the position basis\nFrom the classical point of view, we are used to describe the harmonic oscillator in terms of position and momentum. In quantum mechanics, we can also express the system in terms of the position and momentum operators, which are related to the annihilation and creation operators as follows (\\(\\hbar = 1\\)):\n\\[\\begin{align*}\n\\hat{x} &= \\frac{1}{\\sqrt{2 m \\omega}} \\left( \\hat{a} + \\hat{a}^\\dagger \\right) \\, , \\\\\n\\hat{p} &= i \\sqrt{\\frac{m \\omega}{2}} \\left( \\hat{a}^\\dagger - \\hat{a} \\right) \\, .\n\\end{align*}\\]\nWe first check that\n\\[\n\\left[ \\hat{x}, \\hat{p} \\right] = i\n\\]\n\n\nm = 0.5\n\nx = (a + a.dag()) / np.sqrt(2 * m * w)\np = - 1j * (a - a.dag()) * np.sqrt(m * w / 2)\n\nqutip.commutator(x, p)\n\nQuantum object: dims=[[120], [120]], shape=(120, 120), type='oper', dtype=Dia, isherm=False\nQobj data =\n[[0.  +1.j 0.  +0.j 0.  +0.j ... 0.  +0.j 0.  +0.j 0.  +0.j]\n [0.  +0.j 0.  +1.j 0.  +0.j ... 0.  +0.j 0.  +0.j 0.  +0.j]\n [0.  +0.j 0.  +0.j 0.  +1.j ... 0.  +0.j 0.  +0.j 0.  +0.j]\n ...\n [0.  +0.j 0.  +0.j 0.  +0.j ... 0.  +1.j 0.  +0.j 0.  +0.j]\n [0.  +0.j 0.  +0.j 0.  +0.j ... 0.  +0.j 0.  +1.j 0.  +0.j]\n [0.  +0.j 0.  +0.j 0.  +0.j ... 0.  +0.j 0.  +0.j 0.-119.j]]\n\n\n\nWe now numerically diagonalize the position operator \\(\\hat{x}\\), such that \\(\\hat{U}^\\dagger \\hat{x} \\hat{U}\\) is diagonal. Then we plot the eigenstates of the quantum harmonic oscillator in the new basis, obtained with\n\\[\n\\vert \\phi_n (x) \\rangle = \\hat{U}^\\dagger \\vert \\psi_n \\rangle\n\\]\n\n\nimport matplotlib.pyplot as plt\n\nE, T = x.eigenstates()\n\nU = np.zeros((N, N)).astype(np.complex128)\nfor i in range(N):\n    U[:,i] = T[i].full().flatten()\n\nU = qutip.Qobj(U)\n\nxlist = ( U.dag() * x * U ).diag()\ndx = xlist[1] - xlist[0]\n\n# Harmonic potential\nV = w**2 / 2 * xlist**2\n\nfig, ax = plt.subplots()\n\nax.plot(xlist, V, color=\"black\", ls=\"--\", lw=2)\nfor i in range(5):\n    factor = 5 # The purpose of this factor is to only make more visible the states\n    ax.plot(xlist, factor * np.abs( (U.dag() * qutip.fock(N, i)).full() )**2 + i * w + w/2, lw=2)\n\nax.set_xlabel(r\"$x$\")\nax.set_xlim(-5, 5)\nax.set_ylim(0, 5)\n\n# Show in Quarto\nplt.savefig(\"_tmp_fig.svg\")\nplt.close(fig)\nSVG(\"_tmp_fig.svg\")",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to QuTiP</span>"
    ]
  },
  {
    "objectID": "lecture1/closed_systems.html",
    "href": "lecture1/closed_systems.html",
    "title": "9  Closed Quantum Systems in QuTiP: The Schrödinger equation",
    "section": "",
    "text": "9.1 From wavefunctions to the Schrödinger equation in configuration space\nIn Chapter 6 we moved from Hamilton’s equations for a single point to Liouville’s equation for an ensemble. Quantum mechanics follows a similar path, replacing phase-space densities with wavefunctions (or density operators). We now show this parallelism in detail.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Closed Quantum Systems in QuTiP: The Schrödinger equation</span>"
    ]
  },
  {
    "objectID": "lecture1/closed_systems.html#from-wavefunctions-to-the-schrödinger-equation-in-configuration-space",
    "href": "lecture1/closed_systems.html#from-wavefunctions-to-the-schrödinger-equation-in-configuration-space",
    "title": "9  Closed Quantum Systems in QuTiP: The Schrödinger equation",
    "section": "",
    "text": "9.1.1 From a single wavepacket to the quantum continuity law\nA non-relativistic quantum system in one dimension is described by a complex wavefunction \\(\\psi(x,t)\\). Its probability density and current are\n\\[\n\\rho(x,t) \\;=\\; |\\psi(x,t)|^2, \\qquad\nj(x,t) \\;=\\; \\frac{\\hbar}{m}\\,\\Im\\!\\bigl[\\psi^{\\!*}(x,t)\\,\\partial_x \\psi(x,t)\\bigr].\n\\]\nStarting from the time-dependent Schrödinger equation\n\\[\ni\\hbar\\,\\partial_t \\psi(x,t) \\;=\\; \\hat H\\,\\psi(x,t) \\quad\\text{with}\\quad\n\\hat H \\;=\\; -\\frac{\\hbar^2}{2m}\\,\\partial_x^2 + V(x)\n\\tag{9.1}\\]\nand its complex conjugate, a short calculation yields the continuity equation\n\\[\n\\partial_t \\rho + \\partial_x j \\;=\\; 0,\n\\tag{9.2}\\]\nwhich plays the same role as Liouville’s incompressibility condition: probability is transported in configuration space without being created or destroyed.\n\n\n9.1.2 The Schrödinger equation as a linear ODE in Hilbert space\nWhile Equation 9.1 is a partial differential equation, it is linear in \\(\\psi\\), exactly like the matrix ODE \\(\\dot{\\mathbf y}=A\\mathbf y\\) used for classical linear systems. Written in Dirac notation\n\\[\ni\\hbar\\,\\frac{d}{dt}\\,|\\psi(t)\\rangle \\;=\\; \\hat H\\,|\\psi(t)\\rangle,\n\\]\nthe resemblance becomes explicit: replace the classical matrix \\(A\\) by the quantum Hamiltonian operator \\(\\hat H\\) and the state vector \\(\\mathbf y\\) by a ket \\(|\\psi\\rangle\\).\nFor statistical mixtures one introduces the density operator \\(\\hat\\rho\\) and obtains the von Neumann equation\n\\[\ni\\hbar\\,\\partial_t \\hat\\rho \\;=\\; [\\hat H,\\,\\hat\\rho],\n\\tag{9.3}\\]\na direct quantum counterpart of the Liouville equation in Equation 6.5.\nWith these ingredients we now possess a one-to-one map between the classical Liouville formulation and the quantum Schrödinger formulation, both expressible as sparse linear ODEs ready for numerical treatment. Let’s consider a simple example of a harmonic oscillator, which is described by the Hamiltonian\n\\[\n\\hat{H} = \\hbar \\omega \\hat{a}^\\dagger \\hat{a} \\, ,\n\\]\nwhere \\(\\hat{a}\\) and \\(\\hat{a}^\\dagger\\) are the annihilation and creation operators, respectively, and \\(\\omega\\) is the angular frequency of the oscillator.\n\n\nN = 120 # Number of Fock states\nw = 1 # Angular frequency of the oscillator\n\na = destroy(N) # Annihilation operator\nH = w * a.dag() * a # Hamiltonian of the harmonic oscillator\n\nH\n\nQuantum object: dims=[[120], [120]], shape=(120, 120), type='oper', dtype=Dia, isherm=True\nQobj data =\n[[  0.   0.   0. ...   0.   0.   0.]\n [  0.   1.   0. ...   0.   0.   0.]\n [  0.   0.   2. ...   0.   0.   0.]\n ...\n [  0.   0.   0. ... 117.   0.   0.]\n [  0.   0.   0. ...   0. 118.   0.]\n [  0.   0.   0. ...   0.   0. 119.]]\n\n\n\nIn QuTip, the Schrödinger equation is solved with the function sesolve, which stands for “Schrödinger equation solver”.\n\n\nalpha = 3 # Coherence of the initial state\n\n# We start from a coherent state the most classic-like state\npsi0 = coherent(N, alpha)\n\n# List of the times for the time evolution\ntlist = np.linspace(0, 2 * 2*np.pi/w, 100)\n\ne_ops = [H, a + a.dag()]\n\nsol = sesolve(H, psi0, tlist, e_ops=e_ops)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Closed Quantum Systems in QuTiP: The Schrödinger equation</span>"
    ]
  },
  {
    "objectID": "lecture1/closed_systems.html#plot-the-expectation-values",
    "href": "lecture1/closed_systems.html#plot-the-expectation-values",
    "title": "9  Closed Quantum Systems in QuTiP: The Schrödinger equation",
    "section": "9.2 Plot the expectation values",
    "text": "9.2 Plot the expectation values\nWe can access to the expectation values with the command sol.expect[i], where i is the index of the \\(i\\)-th operator for which we want to calculate te expectation value as a function of time.\n\n\nfig, ax = plt.subplots()\n\nax.plot(tlist, sol.expect[0], label=r\"$\\langle \\hat{H} \\rangle$\", lw=2)\nax.plot(tlist, sol.expect[1], label=r\"$\\langle \\hat{a} + \\hat{a}^\\dagger \\rangle$\", lw=2)\nax.legend()\nax.set_xlabel(r\"$t$\")\nax.set_xlim(tlist[0], tlist[-1])\nax.set_ylim(None, 16)\n\n# Show in Quarto\nplt.savefig(\"_tmp_fig.svg\")\nplt.close(fig)\nSVG(\"_tmp_fig.svg\")",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Closed Quantum Systems in QuTiP: The Schrödinger equation</span>"
    ]
  },
  {
    "objectID": "lecture1/closed_systems.html#access-directly-to-the-evolution-of-the-state",
    "href": "lecture1/closed_systems.html#access-directly-to-the-evolution-of-the-state",
    "title": "9  Closed Quantum Systems in QuTiP: The Schrödinger equation",
    "section": "9.3 Access directly to the evolution of the state",
    "text": "9.3 Access directly to the evolution of the state\nWe can also access directly to the wavefunction at each tlist. This can be done by simply calling sol.states to the solver without the e_ops operators.\nTo check this feature, let’s see if after \\(10\\) cycles we get still the initial state, by calculating the fidelity\n\\[\n\\langle \\psi \\left(t = 10 T\\right) \\vert \\psi \\left( t = 0 \\right) \\rangle\n\\]\nwhere \\(T = \\frac{2 \\pi}{\\omega}\\).\n\n\nsol = sesolve(H, psi0, tlist)\n\nsol.states[0].dag() * sol.states[-1]\n\n(0.999999999991156-6.853062359472892e-07j)\n\n\n\nWe then switch to the position eigenbasis. Thus, we need to diagonalize the position operator. This in general involves the Laguerre functions, but here we limit ourselves to numerically diagonalize the position operator.\nWe now show the wavefunctions in the position basis at three different times: \\(t_0\\), \\(t_1\\) and \\(t_2\\), with\n\\[\nt_0 = 0 \\, , \\quad t_1 = \\frac{\\pi}{\\omega} \\, , \\quad t_2 = \\frac{2 \\pi}{\\omega} \\, ,\n\\]\nshowing the exact periodicity of this system.\n\n\nidx_t0 = 0\nidx_t1 = np.where(tlist &gt;= np.pi/w)[0][0]\nidx_t2 = np.where(tlist &gt;= 2*np.pi/w)[0][0]\n\npsi0_x = U.dag() * sol.states[idx_t0]\npsi1_x = U.dag() * sol.states[idx_t1]\npsi2_x = U.dag() * sol.states[idx_t2]\n\n# We define the potential for the harmonic oscillator\nV = 0.5 * m * w**2 * xlist**2\n\nfig, ax = plt.subplots()\n\nax.plot(xlist, 700 * np.abs(psi0_x.full())**2, \n        label=r\"$\\vert \\langle \\psi (t_0) \\vert \\psi (t_0) \\rangle \\vert^2$\", lw=2)\nax.plot(xlist, 700 * np.abs(psi1_x.full())**2,\n        label=r\"$\\vert \\langle \\psi (t_1) \\vert \\psi (t_1) \\rangle \\vert^2$\", ls=\"--\", lw=2)\nax.plot(xlist, 700 * np.abs(psi2_x.full())**2,\n        label=r\"$\\vert \\langle \\psi (t_2) \\vert \\psi (t_2) \\rangle \\vert^2$\", ls=\"-.\", lw=2)\nax.plot(xlist, V, color=\"black\", ls=\"--\")\nax.legend()\nax.set_xlabel(r\"$x$\")\nax.set_xlim(xlist[0], xlist[-1])\nax.set_ylim(0, 120)\n\n# Show in Quarto\nplt.savefig(\"_tmp_fig.svg\")\nplt.close(fig)\nSVG(\"_tmp_fig.svg\")\n\n\n\n\n\n\n\n\n\nWe can also export an animation, showing more easily the time evolution of the state\n\n\nfrom matplotlib.animation import FuncAnimation\nfrom IPython.display import HTML\n\nplt.rcParams.update({'font.size': 8})\nfig, ax = plt.subplots(figsize=(4.6, 2.8))\n\nline, = ax.plot(xlist, 700 * np.abs(psi0_x.full())**2, lw=2)\nax.plot(xlist, V, color=\"black\", ls=\"--\")\nax.set_xlabel(r\"$x$\")\nax.set_xlim(xlist[0], xlist[-1])\nax.set_ylim(0, 120)\n\nplt.close(fig) # Otherwise the static figure also appears\n\ndef update(frame):\n    psi_t = U.dag() * sol.states[frame]\n    line.set_ydata( 700 * np.abs(psi_t.full())**2 )\n    return line,\n\nfps = 25\nani = FuncAnimation(fig, update, frames=len(tlist), blit=True, interval=1000 / fps)\n\nHTML(ani.to_jshtml())\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Closed Quantum Systems in QuTiP: The Schrödinger equation</span>"
    ]
  },
  {
    "objectID": "lecture2/master_equation_qutip.html",
    "href": "lecture2/master_equation_qutip.html",
    "title": "10  Open Dynamics in QuTiP: The Master Equation",
    "section": "",
    "text": "10.1 Example: the damped harmonic oscillator\nIn Chapter 9, we discussed the evolution of closed quantum systems, where the dynamics is governed by the Schrödinger equation. The evolution of the state vector \\(\\ket{\\psi(t)}\\) is given by:\n\\[\n\\frac{d \\ket{\\psi(t)}}{dt} = -\\frac{i}{\\hbar} \\hat{H} \\ket{\\psi(t)}\n\\]\nwhere \\(\\hat{H}\\) is the Hamiltonian operator of the system. The solution to this equation leads to unitary evolution. In contrast to the closed case, open quantum systems interact with their environment, leading to non-unitary evolution described by the Master equation:\n\\[\n\\frac{d \\hat{\\rho}}{dt} = -\\frac{i}{\\hbar}[\\hat{H}, \\hat{\\rho}] + \\sum_k \\left( \\hat{L}_k \\hat{\\rho} \\hat{L}_k^\\dagger - \\frac{1}{2} \\{\\hat{L}_k^\\dagger \\hat{L}_k, \\hat{\\rho}\\} \\right)\n\\]\nHere, \\(\\hat{\\rho}\\) represents the density matrix of the system, \\(\\hat{L}_k\\) are the Lindblad operators representing different dissipation processes. and \\(\\{\\hat{A}, \\hat{B} \\} = \\hat{A} \\hat{B} + \\hat{B} \\hat{A}\\) is the anti-commutator between the operators \\(\\hat{A}\\) and \\(\\hat{B}\\).\nWe can use the Master equation to study the dynamics of a damped harmonic oscillator. The Hamiltonian for a harmonic oscillator is given by:\n\\[\n\\hat{H} = \\omega_0 \\hat{a}^\\dagger \\hat{a}\n\\]\nwhere \\(\\hat{a}\\) and \\(\\hat{a}^\\dagger\\) are the annihilation and creation operators, respectively, and \\(\\omega_0\\) is the angular frequency of the oscillator.\nTo model the damping, we can introduce a Lindblad operator that represents the interaction with the environment. In the case of the interaction with a zero-temperature bath, the Lindblad operator can be defined as:\n\\[\n\\hat{L} = \\sqrt{\\gamma} \\hat{a}\n\\]\nwhere \\(\\gamma\\) is the damping rate. Let’s now initialize the system in a coherent state and evolve it using the Master equation. We will also visualize the evolution of the position and energy expectation values over time.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Open Dynamics in QuTiP: The Master Equation</span>"
    ]
  },
  {
    "objectID": "lecture2/master_equation_qutip.html#example-the-damped-harmonic-oscillator",
    "href": "lecture2/master_equation_qutip.html#example-the-damped-harmonic-oscillator",
    "title": "10  Open Dynamics in QuTiP: The Master Equation",
    "section": "",
    "text": "import numpy as np\nfrom qutip import *\n\n# Parameters\nN = 30  # Number of Fock states\nomega_0 = 1.0  # Angular frequency of the oscillator\n\na = destroy(N)  # Annihilation operator\n\n# Hamiltonian\nH = omega_0 * a.dag() * a\n\n\n\n\n\n\n\n# Damping rate\ngamma = 0.1\n\n# Lindblad operator\nL = np.sqrt(gamma) * a\n\n# Initial state: coherent state\nalpha = 3.0\npsi_0 = coherent(N, alpha)\n\ntlist = np.linspace(0, 50, 500)\n\n# Solve the Master equation\nresult = mesolve(H, psi_0, tlist, [L], e_ops=[a.dag() * a, a + a.dag()])\nresult\n\n&lt;Result\n  Solver: mesolve\n  Solver stats:\n    method: 'scipy zvode adams'\n    init time: 0.00017523765563964844\n    preparation time: 0.0002968311309814453\n    run time: 0.26783227920532227\n    solver: 'Master Equation Evolution'\n    num_collapse: 1\n  Time interval: [0.0, 50.0] (500 steps)\n  Number of e_ops: 2\n  State not saved.\n&gt;\n\n\n\n\n10.1.1 Plotting the results\n\n\nimport matplotlib.pyplot as plt\n\n# Plot the expectation values\nfig, ax = plt.subplots()\n\nax.plot(tlist, result.expect[0], label=r\"Energy $\\langle \\hat{H} \\rangle$\")\nax.plot(tlist, result.expect[1].real, label=r\"Position $\\langle \\hat{x} \\rangle$\")\nax.set_xlabel(r\"Time $t$\")\nax.set_ylabel(\"Expectation values\")\nax.legend()\nax.set_title(\"Damped Harmonic Oscillator Dynamics\")\n\n# Show in Quarto\nplt.savefig(\"_tmp_fig.svg\")\nplt.close()\nSVG(\"_tmp_fig.svg\")",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Open Dynamics in QuTiP: The Master Equation</span>"
    ]
  },
  {
    "objectID": "lecture2/master_equation_qutip.html#time-dependent-parameters-the-case-of-logical-quantum-gates",
    "href": "lecture2/master_equation_qutip.html#time-dependent-parameters-the-case-of-logical-quantum-gates",
    "title": "10  Open Dynamics in QuTiP: The Master Equation",
    "section": "10.2 Time-dependent parameters: the case of logical quantum gates",
    "text": "10.2 Time-dependent parameters: the case of logical quantum gates\nIn classical computing, logical gates such as AND, OR, and NOT process bits (0 or 1) to perform basic operations. For example:\n\nAND outputs 1 only if both inputs are 1.\nOR outputs 1 if at least one input is 1.\nNOT inverts its input bit.\n\nThese gates form the building blocks of all classical algorithms and digital circuits.\nQuantum computing generalizes this idea by using qubits, which can exist in superpositions of 0 and 1. Quantum gates act on these superposed states via time-dependent Hamiltonians, enabling phenomena like entanglement and interference. This richer behaviour unlocks powerful algorithms (e.g., Shor’s factoring, Grover’s search) that have no efficient classical equivalent.\nAs an example, we consider a combination of gates that generates a bell state\n\\[\n| \\Phi^+ \\rangle = \\frac{1}{\\sqrt{2}} (| 00 \\rangle + | 11 \\rangle).\n\\tag{10.1}\\]\nTo achieve this, we can use a sequence of quantum gates:\n\nHadamard gate: Applies a Hadamard gate to the first qubit, transforming \\(| 0 \\rangle\\) into \\(\\frac{1}{\\sqrt{2}} (| 0 \\rangle + | 1 \\rangle)\\).\nCNOT gate: Applies a controlled-NOT gate, where the first qubit controls the second. This flips the second qubit if the first is in state \\(| 1 \\rangle\\).\n\n\n\n\n\n\n\n\n\n\n\n\nEach gate can be represented by a time-dependent Hamiltonian. For example, the Hadamard gate can be implemented using a Hamiltonian\n\\[\n\\hat{H}_\\mathrm{H}^{(1)} (t_0, t) = \\Theta (t - t_0) \\Theta ( t_0 + \\pi / 2 - t) \\frac{1}{\\sqrt{2}} \\left( \\hat{\\sigma}_x^{(1)} - \\hat{\\sigma}_z^{(1)} \\right)\n\\]\nwhere \\(\\Theta(t)\\) is the Heaviside step function, and \\(\\hat{\\sigma}_x^{(1)}\\) and \\(\\hat{\\sigma}_z^{(1)}\\) are the Pauli operators acting on the first qubit. The Heaviside function ensures that the Hamiltonian is non-zero only during the time interval \\([t_0, t_0 + \\pi / 2]\\), where \\(t_0\\) is the time at which the gate is applied.\nThe CNOT gate can be implemented using a Hamiltonian that couples the two qubits\n\\[\n\\hat{H}_\\mathrm{CNOT} (t_0, t) = \\Theta (t - t_0) \\Theta ( t_0 + \\pi / 2 - t) \\ \\left(\\hat{\\mathbb{1}} + \\hat{\\sigma}_z^{(1)} \\right) \\otimes \\left( \\hat{\\mathbb{1}} - \\hat{\\sigma}_x^{(2)} \\right)\n\\]\nwhere \\(\\hat{\\sigma}_x^{(2)}\\) acts on the second qubit, and \\(\\hat{\\mathbb{1}}\\) is the identity operator. The Heaviside function again ensures that the Hamiltonian is non-zero only during the time interval \\([t_0, t_0 + \\pi / 2]\\).\nIn absence of losses, the output state after applying these gates is exactly the Bell state \\(|\\Phi^+\\rangle\\) defined in Equation 10.1. However, in a realistic scenario, we need to account for the effects of decoherence and dissipation, which can be modeled using the Master equation. This poses a challenge for quantum computing, as the coherence of the qubits must be maintained during the gate operations to ensure the correct output state.\nTo simulate the effects of decoherence, we consider the case where the qubits are in interaction with a finite-temperature environment, which can lead to energy dissipation and dephasing. In this case, the Master equation can be used to describe the dynamics of the system. We can introduce a Lindblad operator that represents the interaction with the environment. For example, we can use a Lindblad operator that describes the decay of the qubits:\n\\[\\begin{align*}\n\\hat{L}_1 &= \\sqrt{\\gamma (n_\\mathrm{th} + 1)} \\hat{\\sigma}_-^{(1)}, \\quad \\hat{L}_2 = \\sqrt{\\gamma (n_\\mathrm{th} + 1)} \\hat{\\sigma}_-^{(2)} \\\\\n\\hat{L}_3 &= \\sqrt{\\gamma n_\\mathrm{th}} \\hat{\\sigma}_+^{(1)}, \\quad \\hat{L}_4 = \\sqrt{\\gamma n_\\mathrm{th}} \\hat{\\sigma}_+^{(2)}\n\\end{align*}\\]\nwhere \\(\\hat{\\sigma}_+^{(i)}\\) and \\(\\hat{\\sigma}_-^{(i)}\\) are the raising and lowering operators for the \\(i\\)-th qubit, respectively, \\(n_\\mathrm{th}\\) is the average number of thermal excitations in the environment, and \\(\\gamma\\) is the decay rate. The Lindblad operators describe both energy dissipation (via \\(\\hat{\\sigma}_-\\)) and thermal excitation (via \\(\\hat{\\sigma}_+\\)) of the qubits.\nWe can use the ability of QuTiP to define time-dependent Hamiltonians to simulate the evolution of the system under the influence of these gates and the Lindblad operators.\n\n\ndef heaviside(t, t0):\n    return t &gt;= t0\n\ndef hadamard_coeff(t):\n    t0 = 0  # Start time of the Hadamard gate\n    return heaviside(t, t0) * heaviside(t0 + np.pi / 2, t)\n\ndef cnot_coeff(t):\n    t0 = np.pi / 2  # Start time of the CNOT gate\n    return heaviside(t, t0) * heaviside(t0 + np.pi / 2, t)\n\nsm1 = tensor(sigmam(), qeye(2))  # Lowering operator for qubit 1\nsm2 = tensor(qeye(2), sigmam())  # Lowering operator for qubit 2\nsx1 = tensor(sigmax(), qeye(2))  # Pauli X for qubit 1\nsz1 = tensor(sigmaz(), qeye(2))  # Pauli Z for qubit 1\nsx2 = tensor(qeye(2), sigmax())  # Pauli X for qubit 2\n\n# Parameters\ngamma = 0.1  # Decay rate\nT = 0.1  # Temperature (arbitrary units)\nn_th = 1 / (np.exp(1 / T) - 1)  # Average number of thermal excitations\n\n# Hadamard gate Hamiltonian\nH_hadamard = (sx1 - sz1) / np.sqrt(2)\n\n# CNOT gate Hamiltonian\nH_cnot = (1 + sz1) * (1 - sx2) / 2\n\n# Time-dependent Hamiltonian\nH = [[H_hadamard, hadamard_coeff],\n     [H_cnot, cnot_coeff]]\n\n# Lindblad dissipation operators\nL1 = np.sqrt(gamma * (n_th + 1)) * sm1\nL2 = np.sqrt(gamma * (n_th + 1)) * sm2\nL3 = np.sqrt(gamma * n_th) * sm1.dag()\nL4 = np.sqrt(gamma * n_th) * sm2.dag()\n\nc_ops = [L1, L2, L3, L4]\n\n# Initial state: |00&gt;\npsi_0 = tensor(basis(2, 1), basis(2, 1))\n\n# Time list\ntlist = np.linspace(0, np.pi, 500)\n\n# Solve the Master equation\nresult = mesolve(H, psi_0, tlist, c_ops)\nresult\n\n&lt;Result\n  Solver: mesolve\n  Solver stats:\n    method: 'scipy zvode adams'\n    init time: 8.869171142578125e-05\n    preparation time: 0.00020933151245117188\n    run time: 0.016063928604125977\n    solver: 'Master Equation Evolution'\n    num_collapse: 4\n  Time interval: [0.0, 3.141592653589793] (500 steps)\n  Number of e_ops: 0\n  States saved.\n&gt;\n\n\n\nWe now plot the following quantities as a function of time:\n\nThe probability of finding the first qubit in state \\(| 1 \\rangle\\).\nThe probability of finding the second qubit in state \\(| 1 \\rangle\\).\nThe fidelity with the target Bell state \\(|\\Phi^+\\rangle\\).\n\n\n\n# Bell state\nbell_state = (tensor(basis(2, 0), basis(2, 0)) + tensor(basis(2, 1), basis(2, 1))).unit()\n\nP0_1 = expect(sm1.dag() * sm1, result.states)\nP0_2 = expect(sm2.dag() * sm2, result.states)\nfid = [fidelity(bell_state, s) for s in result.states]\n\nfig, ax = plt.subplots()\n\nax.plot(tlist, P0_1, label=r\"$P_1^{(1)}$\")\nax.plot(tlist, P0_2, label=r\"$P_1^{(2)}$\")\nax.plot(tlist, fid, label=r\"Fidelity with $|\\Phi^+\\rangle$\")\nax.set_xlabel(r\"Time $t$\")\nax.set_ylabel(\"Probability / Fidelity\")\nax.set_ylim(0, 1)\nax.legend()\n\n# Show in Quarto\nplt.savefig(\"_tmp_fig.svg\")\nplt.close()\nSVG(\"_tmp_fig.svg\")",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Open Dynamics in QuTiP: The Master Equation</span>"
    ]
  },
  {
    "objectID": "lecture2/jaynes_cummings.html",
    "href": "lecture2/jaynes_cummings.html",
    "title": "11  The Jaynes–Cummings model: The Prove of Field Quantization",
    "section": "",
    "text": "11.1 Simulating the JC model with QuTiP\nAt the crossroads of quantum optics and cavity QED, the Jaynes–Cummings (JC) model stands out as the simplest non‑trivial arena in which a single photon mode interacts with a single two‑level atom. Despite – or perhaps because of – its austerity, it captures phenomena that range from vacuum Rabi oscillations to Schrödinger‑cat states. In what follows I weave together the story of the model, its mathematics, and its experimental vindication, culminating in Serge Haroche’s celebrated observation of field quantisation.\n## The Jaynes–Cummings Hamiltonian: more than a toy\nThe JC Hamiltonian was born amid a heated 1960s debate over whether maser radiation had to be quantised. Edward Jaynes and Frederick Cummings answered with a fully quantum description whose predictions flatly contradicted semi‑classical intuition. In the rotating‑wave approximation it reads\n\\[\n\\hat H = \\omega_c  \\hat a^{\\dagger}\\hat a + \\frac{\\omega_q}{2} \\hat\\sigma_z +  g\\bigl(\\hat a^{\\dagger}\\hat\\sigma_{-}+\\hat a \\hat\\sigma_{+}\\bigr).\n\\]\nwhere \\(\\hat a\\) is the cavity field’s annihilation operator, \\(\\hat\\sigma_z\\) is the atom’s Pauli operator, and \\(g\\) is the coupling strength. The first two terms describe the free evolution of cavity and atom, while the last term couples them.\nThe JC model conserves the total excitation number \\(N = \\hat a^{\\dagger}\\hat a + \\hat\\sigma_+\\hat\\sigma_-\\). This means that the Hilbert space decomposes into sectors of fixed \\(N\\), each spanned by the states\n\\[\n|\\psi_{n, 1}\\rangle = |n, e\\rangle \\quad \\text{and} \\quad |\\psi_{n, 2}\\rangle = |n+1, g\\rangle\n\\]\nThe JC Hamiltonian acts within each sector as a \\(2\\times2\\) matrix\n\\[\n\\hat H_{n} = \\begin{pmatrix}\n\\omega_c n + \\frac{\\omega_q}{2} & g\\sqrt{n+1} \\\\\ng\\sqrt{n+1} & \\omega_c (n+1) - \\frac{\\omega_q}{2}\n\\end{pmatrix}.\n\\]\nThe eigenvalues of this matrix are\n\\[\nE_{n, \\pm} = (n + \\frac{1}{2}) \\omega_c \\pm \\frac{1}{2} \\Omega_n\n\\tag{11.1}\\]\nwhere\n\\[\n\\Omega_n = \\sqrt{(\\omega_c - \\omega_q)^2 + 4g^2(n+1)}\n\\tag{11.2}\\]\nis the Rabi frequency.\n### Collapse and revival in plain words\nWhen the cavity field is prepared in a coherent state \\(|\\alpha\\rangle\\) and the atom in its ground state \\(|g\\rangle\\), the atomic inversion can be written as a superposition of Rabi oscillations at frequencies \\(\\Omega_n\\), each weighted by the Poisson probability \\(P_n=e^{-|α|^2}|α|^{2n}/n!\\). The resulting interference leads to an initial collapse of the oscillations, followed by a revival. The time evolution of the atomic inversion is given by\n\\[\n\\langle \\hat\\sigma_z(t) \\rangle = \\sum_{n=0}^{\\infty} P_n\\,\\cos\\bigl(2\\Omega_n\\,t\\bigr) \\, .\n\\]\n## The open-system perspective\nReal cavities leak, as well as electrons in the atom decay. To account for this, we can extend the JC Hamiltonian with a Lindblad term that describes the interaction with the environment. The master equation for the density operator \\(\\rho\\) reads\n\\[\n\\dot\\rho = -\\tfrac{i}{\\hbar}[\\hat H,\\rho] + \\kappa\\,\\mathcal D[\\hat a]\\rho + \\gamma\\,\\mathcal D[\\hat\\sigma_-]\\rho,\n\\]\nwith \\(\\mathcal D[\\hat O]\\rho = \\hat O\\rho\\hat O^{\\dagger} - \\tfrac12{\\hat O^{\\dagger}\\hat O,\\rho}\\) being the Lindblad dissipator. When \\(\\kappa\\) and \\(\\gamma\\) are small compared to the Rabi frequency, the JC model is in the strong-coupling regime, where the coherent oscillations are visible despite the dissipation. On the contrary, if \\(\\kappa\\) or \\(\\gamma\\) are large, the oscillations are damped and eventually disappear, which corresponds to the weak-coupling regime.\n## Damped vacuum Rabi oscillations: The hello‑world of cavity QED\nWe now simulate the open system dynamics of the JC model, starting from the \\(|\\psi(0)\\rangle = |0,e\\rangle\\) state and watch excitation watch the atomic population in time. Theory predicts a cosine at frequency \\(2g\\), blurred by an exponential envelope \\(e^{-\\kappa t/2}\\).",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>The Jaynes–Cummings model: The Prove of Field Quantization</span>"
    ]
  },
  {
    "objectID": "lecture2/jaynes_cummings.html#simulating-the-jc-model-with-qutip",
    "href": "lecture2/jaynes_cummings.html#simulating-the-jc-model-with-qutip",
    "title": "11  The Jaynes–Cummings model: The Prove of Field Quantization",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom qutip import *\n\n# Parameters\nwc    = 5.0   # cavity\nwq    = 5.0   # atom (on resonance)\ng     = 0.1   # coupling\nkappa = 0.01  # cavity decay\ngamma = 0.00  # atomic T1 (suppressed)\nN     = 20    # Fock cutoff\n\n# Operators\na  = tensor(destroy(N), qeye(2))\nsp = tensor(qeye(N), sigmap())\nsm = tensor(qeye(N), sigmam())\nsz = tensor(qeye(N), sigmaz())\n\nH = wc * a.dag() * a + 0.5*wq * sz + g * (a.dag() * sm + a * sp)\n\n# Dissipators\nc_ops = [np.sqrt(kappa) * a, np.sqrt(gamma) * sm]\n\n\n\n\n\n\nψ0 = tensor(basis(N, 0), basis(2, 0))\ntlist  = np.linspace(0, 200, 500)\n\nresult = mesolve(H, ψ0, tlist, c_ops, e_ops=[sp*sm])\nP_e = result.expect[0]\n\nfig, ax = plt.subplots()\nax.plot(tlist, P_e)\nax.set_xlabel(\"Time\")\nax.set_ylabel(r\"$P_{e}$\")\n\n# Show in Quarto\nplt.savefig(\"_tmp_fig.svg\")\nplt.close(fig)\nSVG(\"_tmp_fig.svg\")",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>The Jaynes–Cummings model: The Prove of Field Quantization</span>"
    ]
  },
  {
    "objectID": "lecture2/jaynes_cummings.html#demonstration-of-electromagnetic-field-quantization-the-haroche-experiment",
    "href": "lecture2/jaynes_cummings.html#demonstration-of-electromagnetic-field-quantization-the-haroche-experiment",
    "title": "11  The Jaynes–Cummings model: The Prove of Field Quantization",
    "section": "11.2 Demonstration of Electromagnetic Field Quantization: The Haroche Experiment",
    "text": "11.2 Demonstration of Electromagnetic Field Quantization: The Haroche Experiment\nA landmark experiment that unambiguously demonstrated the quantization of the electromagnetic field was conducted by Serge Haroche’s group in the 1990s (Brune et al. 1996). The experiment used high-Q superconducting microwave cavities prepared in a coherent state \\(|\\alpha\\rangle\\). Rydberg atoms were sent one at a time through the cavity, interacting dispersively with the quantized field. The population of the atomic excited state was measured as a function of the atom-cavity interaction time.\n\n11.2.1 Key Observations:\n\nThe atomic population exhibited collapse and revival dynamics.\nThese revivals correspond to the quantum interference between Rabi oscillations at different frequencies \\(\\Omega_n = \\Omega_0 \\sqrt{n+1}\\), each associated with a Fock state component \\(|n\\rangle\\) in the coherent state.\nA Fourier transform of the signal revealed multiple peaks, each corresponding to a discrete photon number.\n\nThis experiment cannot be explained using a classical field description. The observed dynamics and frequency components are direct evidence of the discrete (quantized) nature of the electromagnetic field. It remains one of the most compelling demonstrations of field quantization in quantum optics.\n### Preparing the coherent field\nTo simulate Haroche’s experiment, we need to prepare a coherent state \\(|\\alpha\\rangle\\) in the cavity and a two-level atom in its ground state.\n\n\nα = 2.0\nψ_fld = coherent(N, α)\nψ_atm = basis(2, 1)\nψ0 = tensor(ψ_fld, ψ_atm)\n\n\nWe can now use the mesolve function to evolve the system in time, while measuring the atomic population.\n\n\ntlist  = np.linspace(0, 1000, 500)\n\nout = mesolve(H, ψ0, tlist, c_ops, e_ops=[sp*sm])\npop = out.expect[0]\n\nfig, ax = plt.subplots()\nax.plot(tlist, pop)\nax.set_xlabel(\"Time\")\nax.set_ylabel(r\"$\\langle\\hat\\sigma_+ \\hat\\sigma_-\\rangle$\")\n\n# Show in Quarto\nplt.savefig(\"_tmp_fig.svg\")\nplt.close(fig)\nSVG(\"_tmp_fig.svg\")\n\n\n\n\n\n\n\n\n\n### Fourier spectroscopy: The signature of Fock states\nEach Fock state \\(|n\\rangle\\) drives the atom at \\(\\Omega_n = 2g\\sqrt{n+1}\\), so the Fourier transform of \\(\\langle \\hat\\sigma_z(t)\\rangle\\) must exhibit a comb at precisely those spacings.\n\n\nfrom numpy.fft import fft, fftfreq\n\n# We remove the exponential decay factor to make the peaks more visible\nexp_factor = np.exp(-kappa * tlist / 2)\nfft_sig = fft(pop / exp_factor)[0:len(pop)//2]\n\nfreq = fftfreq(len(pop), (tlist[1]-tlist[0]))[:len(pop)//2] * 2 * np.pi\n\nfig, ax = plt.subplots()\nax.plot(freq, np.abs(fft_sig))\nax.vlines(2*g*np.sqrt(np.arange(1, 5)), 20, 100, color=\"black\", linestyles=\"--\")\nax.text(2*g*np.sqrt(1), 110, r\"$2g\\sqrt{n+1}$\", fontsize=12, ha=\"left\", va=\"bottom\")\nax.set_yscale(\"log\")\nax.set_xlabel(\"Frequency\")\nax.set_ylabel(\"FFT\")\nax.set_title(\"Spectral lines at $2g\\sqrt{n+1}$\")\n\n# Show in Quarto\nplt.savefig(\"_tmp_fig.svg\")\nplt.close(fig)\nSVG(\"_tmp_fig.svg\")\n\n\n\n\n\n\n\n\n\nThis plot worths the Nobel prize in 2012, as it shows the quantization of the electromagnetic field. Indeed, the peaks at \\(2g\\sqrt{n+1}\\) are a direct signature of the Fock states \\(|n\\rangle\\) in the coherent state \\(|\\alpha\\rangle\\). Each peak corresponds to a different photon number, and their spacing reflects the quantized nature of the electromagnetic field.\n\n\n\nExperimental data from Haroche’s group showing the quantization of the electromagnetic field at different initial coherent state amplitudes (Brune et al. 1996).\n\n\n\n\n\n\nBrune, M., F. Schmidt-Kaler, A. Maali, J. Dreyer, E. Hagley, J. M. Raimond, and S. Haroche. 1996. “Quantum Rabi Oscillation: A Direct Test of Field Quantization in a Cavity.” Phys. Rev. Lett. 76 (March): 1800–1803. https://doi.org/10.1103/PhysRevLett.76.1800.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>The Jaynes–Cummings model: The Prove of Field Quantization</span>"
    ]
  },
  {
    "objectID": "lecture3/mollow_triplet.html",
    "href": "lecture3/mollow_triplet.html",
    "title": "12  Resonance Fluorescence and the Mollow Triplet",
    "section": "",
    "text": "12.1 Physical picture\nWhen a resonant laser drives a two‑level atom strongly, its resonance‑fluorescence spectrum splits into three Lorentzian peaks: a central line at the laser frequency and two symmetric sidebands. Predicted by B.R. Mollow in 1969 (Mollow (1969)) and first observed soon after, this Mollow triplet is a hallmark of light–matter interaction in the strong‑drive (dressed‑state) regime.\nConsider a two‑level atom with energy separation \\(\\omega_0\\) driven by a coherent laser at frequency \\(\\omega_L\\).\n\\[\nH = \\frac{\\omega_0}{2}\\,\\hat{\\sigma}_z + \\frac{\\Omega}{2}\\,(\\hat{\\sigma}_+e^{-i\\omega_L t} + \\hat{\\sigma}_-e^{+i\\omega_L t}).\n\\]\nTransforming to the laser rotating frame with \\(U(t)=\\exp [-i \\tfrac{\\omega_L t}{2}\\hat{\\sigma}_z]\\) removes the explicit time dependence and shifts the zero of energy, yielding the textbook Hamiltonian used below\n\\[\nH = \\frac{\\Delta}{2}\\,\\hat{\\sigma}_z + \\frac{\\Omega}{2}\\,(\\hat{\\sigma}_+ + \\hat{\\sigma}_-),\n\\]\nwith detuning \\(\\Delta = \\omega_0 - \\omega_L\\) and on‑resonance Rabi frequency \\(\\Omega = \\mu E_0/\\hbar\\). This semiclassical model, combined with a Lindblad dissipator for spontaneous emission at rate \\(\\gamma\\), fully captures the triplet.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Resonance Fluorescence and the Mollow Triplet</span>"
    ]
  },
  {
    "objectID": "lecture3/mollow_triplet.html#analytic-spectrum",
    "href": "lecture3/mollow_triplet.html#analytic-spectrum",
    "title": "12  Resonance Fluorescence and the Mollow Triplet",
    "section": "12.2 Analytic spectrum",
    "text": "12.2 Analytic spectrum\nIn order to analyse the properties of the emitted light, we can compute the power spectrum of the scattered photons, defined as the Fourier transform of the correlation function \\(\\langle \\hat{E}^{(-)}(t) \\hat{E}^{(+)}(0) \\rangle\\), where \\(\\hat{E}^{(-)}\\) and \\(\\hat{E}^{(+)}\\) are the negative and positive frequency parts of the electric field operator. In the rotating frame, these operators are related to the atom’s raising and lowering operators \\(\\hat{\\sigma}_+\\) and \\(\\hat{\\sigma}_-\\) as (Walls and Milburn 2008)\n\\[\n\\hat{E}^{(-)}(t) \\propto \\hat{\\sigma}_+(t) \\quad \\text{and} \\quad \\hat{E}^{(+)}(t) \\propto \\hat{\\sigma}_-(t) \\, .\n\\]\nThe power spectrum is then given by\n\\[\nS(\\omega) = \\int_{-\\infty}^{\\infty} e^{i\\omega t} \\langle \\hat{E}^{(-)}(t) \\hat{E}^{(+)}(0) \\rangle dt \\, .\n\\tag{12.1}\\]\nTreating spontaneous emission at rate \\(\\gamma\\) with a Lindblad term \\(\\hat{L} = \\sqrt{\\gamma},\\hat{\\sigma}_-\\), the power spectrum of scattered photons is (Walls and Milburn 2008)\n\\[\nS(\\omega) \\propto \\frac{\\tfrac{\\gamma}{2}}{(\\omega-\\omega_L)^2 + (\\tfrac{\\gamma}{2})^2}\n            + \\frac{3\\gamma/8}{(\\omega-\\omega_L-\\Omega_R)^2 + (3\\gamma/4)^2}\n            + \\frac{3\\gamma/8}{(\\omega-\\omega_L+\\Omega_R)^2 + (3\\gamma/4)^2}.\n\\]\nwith a central peak at \\(\\omega_L\\) and two sidebands at \\(\\omega_L \\pm \\Omega_R\\), where \\(\\Omega_R = \\sqrt{\\Delta^2 + \\Omega^2}\\) is the Rabi frequency in the rotating frame.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Resonance Fluorescence and the Mollow Triplet</span>"
    ]
  },
  {
    "objectID": "lecture3/mollow_triplet.html#numerical-spectrum-in-qutip",
    "href": "lecture3/mollow_triplet.html#numerical-spectrum-in-qutip",
    "title": "12  Resonance Fluorescence and the Mollow Triplet",
    "section": "12.3 Numerical spectrum in QuTiP",
    "text": "12.3 Numerical spectrum in QuTiP\nBelow is a minimal QuTiP script that reproduces the triplet for a resonantly driven atom (\\(\\Delta = 0\\)). The code computes the emission spectrum \\(S(\\omega)\\) in Equation 12.1 by using the spectrum function to compute the Fourier transform of the correlation function of the emission operators.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom qutip import *\n\ngamma = 1.0          # spontaneous emission rate\nOmega = 5.0 * gamma   # Rabi frequency (drive strength)\nDelta = 0.0           # laser detuning\n\nsm = sigmam()\nsp = sigmap()\nsz = sigmaz()\n\nH = 0.5 * Delta * sz + 0.5 * Omega * (sp + sm)\nc_ops = [np.sqrt(gamma) * sm]\n\nrho_ss = steadystate(H, c_ops)\n\nwlist = np.linspace(-10 * gamma, 10 * gamma, 2000)\nS = spectrum(H, wlist, c_ops, sp, sm)\nS /= np.max(S)\n\nplt.plot(wlist / gamma, S)\nplt.xlabel(r\"$(\\omega - \\omega_L)/\\gamma$\")\nplt.ylabel(r\"$S(\\omega)$ (arb. units)\")\nplt.title(\"Mollow triplet\")\nplt.xlim(-10, 10)\n\n# Show in Quarto\nplt.savefig(\"_tmp_fig.svg\")\nplt.close()\nSVG(\"_tmp_fig.svg\")\n\n\n\n\n\n\n\n\n\nRunning the code with \\(\\Omega = 5,\\gamma\\) reproduces the canonical spectrum: a narrow central line and two broader sidebands at \\(\\pm\\Omega\\).",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Resonance Fluorescence and the Mollow Triplet</span>"
    ]
  },
  {
    "objectID": "lecture3/mollow_triplet.html#sec-photon-stats",
    "href": "lecture3/mollow_triplet.html#sec-photon-stats",
    "title": "12  Resonance Fluorescence and the Mollow Triplet",
    "section": "12.4 Photon statistics and antibunching",
    "text": "12.4 Photon statistics and antibunching\nGiven the emission spectrum, we may ask if the emitted light is classical or quantum. The answer can be found in the photon statistics of the resonance fluorescence, which can be probed by measuring the second‑order correlation function\n\\[\ng^{(2)}(\\tau) \\;=\\; \\frac{\\langle\\hat{E}^{(-)}(0)\\,\\hat{E}^{(-)}(\\tau)\\,\\hat{E}^{(+)}(\\tau)\\,\\hat{E}^{(+)}(0)\\rangle}{\\langle\\hat{E}^{(-)}\\hat{E}^{(+)}\\rangle^{2}} \\;\\propto\\; \\frac{\\langle\\sigma^\\dagger(0)\\,\\sigma^\\dagger(\\tau)\\,\\sigma^-(\\tau)\\,\\sigma^-(0)\\rangle}{\\langle\\sigma^\\dagger\\sigma^-\\rangle^{2}} .\n\\tag{12.2}\\]\nwhere \\(\\hat{E}^{(+)}\\) and \\(\\hat{E}^{(-)}\\) are the positive and negative frequency parts of the electric field operator, and \\(\\sigma^\\dagger\\), \\(\\sigma^-\\) are the raising and lowering operators of the two‑level atom. The quantity \\(g^{(2)}(\\tau)\\) measures the probability of detecting one photon at time \\(0\\) and another at time \\(\\tau\\), normalised by the square of the average number of photons emitted.\nFor classical light \\(g^{(2)}(0) \\ge 1\\) (photon bunching), whereas a single quantum emitter produces antibunching with \\(g^{(2)}(0)=0\\): once a photon is emitted, the atom is in its ground state and cannot emit another immediately, so the probability of detecting two photons with zero delay vanishes.\nUnder strong driving \\(g^{(2)}(\\tau)\\) also displays damped Rabi oscillations at \\(\\Omega_R\\)—a direct time‑domain analogue of the sidebands.\n\n12.4.1 QuTiP example\n\n\ntau_list = np.linspace(0, 10 / gamma, 400)  # time delays\n\nrho_ss = steadystate(H, c_ops)\n\ncorr = correlation_3op_1t(H, None, tau_list, c_ops, sp, sp*sm, sm)\n\nn_ss = expect(sp * sm, rho_ss) # steady‑state population\ng2 = np.real(corr) / (n_ss ** 2) # normalized\n\nplt.plot(tau_list, g2)\nplt.xlabel(r\"$\\tau$\")\nplt.ylabel(r\"$g^{(2)}(\\tau)$\")\nplt.title(\"Photon antibunching and Rabi oscillations\")\nplt.ylim(0, None)\n\n# Show in Quarto\nplt.savefig(\"_tmp_fig.svg\")\nplt.close()\nSVG(\"_tmp_fig.svg\")\n\n\n\n\n\n\n\n\n\nThe plot shows \\(g^{(2)}(0) \\approx 0\\) (perfect antibunching in the ideal model). As \\(\\tau\\) increases the function overshoots above 1 and undergoes damped oscillations at the Rabi frequency before relaxing to the Poissonian value \\(g^{(2)}(\\infty)=1\\).\n\n\n\nThe second order correlation function of the fluorescent light form a single mercury ion in a trap versus delay time \\(\\tau\\). The antibunching at \\(\\tau=0\\) is clearly visible, as well as the Rabi oscillations at longer delays. Figure taken from (Walther 1998).",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Resonance Fluorescence and the Mollow Triplet</span>"
    ]
  },
  {
    "objectID": "lecture3/mollow_triplet.html#discussion",
    "href": "lecture3/mollow_triplet.html#discussion",
    "title": "12  Resonance Fluorescence and the Mollow Triplet",
    "section": "12.5 Discussion",
    "text": "12.5 Discussion\n\nAntibunching ⇒ unambiguously indicates a single quantum emitter or sub‑Poissonian light.\nRabi oscillations in \\(g^{(2)}\\) ⇒ time‑domain fingerprint of the dressed‑state splitting that generates the Mollow sidebands.\nTechnological relevance ⇒ Resonance‑fluorescence photons combine single‑photon purity (antibunching) with high brightness and tunable frequency via the drive laser.\n\n\n\n\n\nMollow, B. R. 1969. “Power Spectrum of Light Scattered by Two-Level Systems.” Phys. Rev. 188 (December): 1969–75. https://doi.org/10.1103/PhysRev.188.1969.\n\n\nWalls, DF, and Gerard J Milburn. 2008. Quantum Optics. Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-540-28574-8.\n\n\nWalther, H. 1998. “Single atom experiments in cavities and traps.” Proceedings of the Royal Society of London. Series A: Mathematical, Physical and Engineering Sciences 454 (1969): 431–45. https://doi.org/10.1098/rspa.1998.0169.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Resonance Fluorescence and the Mollow Triplet</span>"
    ]
  },
  {
    "objectID": "lecture3/jaynes_cummings_spectrum.html",
    "href": "lecture3/jaynes_cummings_spectrum.html",
    "title": "13  The Emission Spectrum of the Jaynes–Cummings Model",
    "section": "",
    "text": "13.1 Analytic Analysis\nIn Chapter 11, we introduced the Jaynes–Cummings (JC) model as a cornerstone of quantum optics and cavity QED. Here we focus on the emission spectrum of the JC system, exploring how the interaction between a single atom and a single cavity mode manifests in the frequency domain. We will see how the spectral lines reflect the dressed–state structure of the system and how dissipation modifies this picture.\nUnder the condition of zero detuning \\(\\Delta = \\omega_c - \\omega_q = 0\\), the Jaynes–Cummings dressed‐state energies simplify. For the \\(n\\)-th excitation manifold, the two eigenvalues become:\n\\[\nE_{n,\\pm} = n\\,\\omega_c \\pm g \\sqrt{n+1} \\,.\n\\]\nHere:\nWhenever the system loses a photon, it jumps from the \\(n\\)-th manifold down to the \\((n-1)\\)-th. Each of the two states at level \\(n\\) (\\(E_{n,+}\\) or \\(E_{n,-}\\)) can decay into either of the two states at level \\(n-1\\). Concretely, the four allowed decay channels are:\nEach channel corresponds to a distinct emission line in the spectrum, with its frequency given by the energy difference between initial and final states.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>The Emission Spectrum of the Jaynes–Cummings Model</span>"
    ]
  },
  {
    "objectID": "lecture3/jaynes_cummings_spectrum.html#analytic-analysis",
    "href": "lecture3/jaynes_cummings_spectrum.html#analytic-analysis",
    "title": "13  The Emission Spectrum of the Jaynes–Cummings Model",
    "section": "",
    "text": "\\(\\omega_c\\) is the cavity frequency.\n\\(g\\) is the single‐photon coupling strength.\n\\(n = 0,1,2,\\dots\\) counts the total number of excitations shared between atom and field.\n\n\n\n\\(E_{n,+} \\to E_{n-1,+}\\)\n\\(E_{n,+} \\to E_{n-1,-}\\)\n\\(E_{n,-} \\to E_{n-1,+}\\)\n\\(E_{n,-} \\to E_{n-1,-}\\)\n\n\n\n13.1.1 Case of Small Population\nWhen the system is weakly excited, it rarely climbs above the first excitation manifold. Practically, only \\(n=0\\) (the vacuum) and \\(n=1\\) are occupied. In this regime:\n\nManifolds populated: Only \\(n=0\\) and \\(n=1\\).\nTransitions: From \\(E_{1,+}\\) and \\(E_{1,-}\\) down to the vacuum at \\(E_{0}=0\\).\nObserved lines: Exactly two, located at:\n\\[\n\\omega = \\omega_c \\pm g\n\\]\n\nThese two peaks form the well‐known vacuum Rabi splitting. As soon as you start populating higher manifolds, the spectrum becomes richer, with more lines appearing at frequencies \\(\\omega_c \\pm g(\\sqrt{n+1} \\pm \\sqrt{n})\\) for \\(n=0,1,2,\\dots\\).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>The Emission Spectrum of the Jaynes–Cummings Model</span>"
    ]
  },
  {
    "objectID": "lecture3/jaynes_cummings_spectrum.html#numerical-simulation-with-qutip",
    "href": "lecture3/jaynes_cummings_spectrum.html#numerical-simulation-with-qutip",
    "title": "13  The Emission Spectrum of the Jaynes–Cummings Model",
    "section": "13.2 Numerical simulation with QuTiP",
    "text": "13.2 Numerical simulation with QuTiP\nWe now simulate the emission spectrum of the JC model using QuTiP, focusing on the weak and strong coupling regimes. We will compute the emission spectrum \\(S (\\omega)\\) of the cavity field using the spectrum function. We first define the system and its parameters.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom qutip import *\n\n# Parameters\nwc    = 5.0   # cavity\nwq    = 5.0   # atom (on resonance)\ng     = 0.1   # coupling\nkappa = 0.03  # cavity decay\ngamma = 0.00  # atomic T1 (suppressed)\nn_th  = 0.01  # thermal photons of the environment\nN     = 20    # Fock cutoff\n\n# Operators\na  = tensor(destroy(N), qeye(2))\nsp = tensor(qeye(N), sigmap())\nsm = tensor(qeye(N), sigmam())\nsz = tensor(qeye(N), sigmaz())\n\nH = wc * a.dag() * a + 0.5*wq * sz + g * (a.dag() * sm + a * sp)\n\n# Dissipators\nc_ops = [np.sqrt(kappa * (n_th + 1)) * a, np.sqrt(gamma * (n_th + 1)) * sm,\n        np.sqrt(kappa * n_th) * a.dag(), np.sqrt(gamma * n_th) * sp]\n\n\nAnd we finally compute the emission spectrum.\n\n\nw_list = np.linspace(wc-6*g, wc+6*g, 1000)\n\nspec = spectrum(H, w_list, c_ops, a.dag(), a)\n\nfig, ax = plt.subplots()\n\nax.plot(w_list, spec.real)\nax.set_xlabel(\"Frequency (MHz)\")\nax.set_ylabel(r\"$S(\\omega)$ (arb. units)\")\n\n# Show in Quarto\nplt.savefig(\"_tmp_fig.svg\")\nplt.close(fig)\nSVG(\"_tmp_fig.svg\")",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>The Emission Spectrum of the Jaynes–Cummings Model</span>"
    ]
  },
  {
    "objectID": "bibliography.html",
    "href": "bibliography.html",
    "title": "Bibliography",
    "section": "",
    "text": "Brune, M., F. Schmidt-Kaler, A. Maali, J. Dreyer, E. Hagley, J. M.\nRaimond, and S. Haroche. 1996. “Quantum Rabi\nOscillation: A Direct Test of Field Quantization in a\nCavity.” Phys. Rev. Lett. 76 (March): 1800–1803.\nhttps://doi.org/10.1103/PhysRevLett.76.1800.\n\n\nCampaioli, Francesco, Jared H. Cole, and Harini Hapuarachchi. 2024.\n“Quantum Master Equations: Tips and Tricks for Quantum Optics,\nQuantum Computing, and Beyond.” PRX Quantum 5 (June):\n020202. https://doi.org/10.1103/PRXQuantum.5.020202.\n\n\nD’Alessandro, Domenico. 2021. Introduction to\nQuantum Control and Dynamics. Chapman; Hall/CRC. https://doi.org/10.1201/9781003051268.\n\n\nJohansson, J. R., P. D. Nation, and Franco Nori. 2012. “QuTiP: An open-source Python framework for the dynamics\nof open quantum systems.” Computer Physics\nCommunications 183 (8): 1760–72. https://doi.org/10.1016/j.cpc.2012.02.021.\n\n\nKhaneja, Navin, Timo Reiss, Cindie Kehlet, Thomas Schulte-Herbrüggen,\nand Steffen J. Glaser. 2005. “Optimal Control of Coupled Spin\nDynamics: Design of NMR Pulse Sequences by Gradient Ascent\nAlgorithms.” Journal of Magnetic Resonance 172 (2):\n296–305. https://doi.org/10.1016/j.jmr.2004.11.004.\n\n\nLambert, Neill, Eric Giguère, Paul Menczel, Boxi Li, Patrick Hopf,\nGerardo Suárez, Marc Gali, et al. 2024. “QuTiP 5: The Quantum Toolbox in Python.”\narXiv:2412.04705. https://arxiv.org/abs/2412.04705.\n\n\nLvovsky, A. I., and M. G. Raymer. 2009. “Continuous-Variable\nOptical Quantum-State Tomography.” Rev. Mod. Phys. 81\n(March): 299–332. https://doi.org/10.1103/RevModPhys.81.299.\n\n\nMercurio, Alberto, Yi-Te Huang, Li-Xun Cai, Yueh-Nan Chen, Vincenzo\nSavona, and Franco Nori. 2025. “QuantumToolbox.jl: An Efficient Julia\nFramework for Simulating Open Quantum Systems.” arXiv\nPreprint arXiv:2504.21440. https://doi.org/10.48550/arXiv.2504.21440.\n\n\nMollow, B. R. 1969. “Power Spectrum of Light Scattered by\nTwo-Level Systems.” Phys. Rev. 188 (December): 1969–75.\nhttps://doi.org/10.1103/PhysRev.188.1969.\n\n\nPeruzzo, Alberto, Jarrod McClean, Peter Shadbolt, Man-Hong Yung, Xiao-Qi\nZhou, Peter J. Love, Alán Aspuru-Guzik, and Jeremy L. O’Brien. 2014.\n“A Variational Eigenvalue Solver on a Photonic Quantum\nProcessor.” Nature Communications 5 (1). https://doi.org/10.1038/ncomms5213.\n\n\nWalls, DF, and Gerard J Milburn. 2008. Quantum\nOptics. Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-540-28574-8.\n\n\nWalther, H. 1998. “Single atom experiments in\ncavities and traps.” Proceedings of the Royal Society\nof London. Series A: Mathematical, Physical and Engineering\nSciences 454 (1969): 431–45. https://doi.org/10.1098/rspa.1998.0169.",
    "crumbs": [
      "Bibliography"
    ]
  },
  {
    "objectID": "appendices/quantum-harmonic-oscillator.html",
    "href": "appendices/quantum-harmonic-oscillator.html",
    "title": "Appendix A — The quantum harmonic oscillator",
    "section": "",
    "text": "As an example, here we consider the quantum harmonic oscillator. Classically, the harmonic oscillator is defined as a system subject to the force \\(\\mathbf{F} = - k \\mathbf{r}\\), where \\(k\\) is the elastic constant. In other words, the force is proportional to the displacement from a stable point (in this case the origin).\nFollowing the relation \\(\\mathbf{F} = - \\nabla V(\\mathbf{r})\\), we can say that the corresponding potential is \\(V(\\mathbf{r}) = k/2 \\ \\mathbf{r}^2\\). The solution of the Schrodinger equation\n\\[\n    i \\hbar \\frac{\\partial}{\\partial t} \\Psi(\\mathbf{r}, t) = -\\frac{\\hbar^2}{2 m} \\nabla^2 \\Psi(\\mathbf{r}, t) + V(\\mathbf{r}) \\Psi(\\mathbf{r}, t) \\, ,\n\\]\nwhere \\(\\hbar\\) is the reduced Planck constant, \\(m\\) is the mass of the particle, and \\(\\nabla^2\\) is the Laplacian operator, gives us the eigenstates of the system. Considering only the one-dimensional case, we obtain the following eigenstates for the quantum harmonic oscillator:\n\\[\n\\psi_n(x) = \\frac{1}{\\sqrt{2^n n!}} \\left(\\frac{m\\omega}{\\pi \\hbar}\\right)^{1/4} e^{-\\frac{m\\omega x^2}{2\\hbar}} H_n\\left(\\sqrt{\\frac{m\\omega}{\\hbar}} x\\right) \\, ,\n\\tag{A.1}\\]\nwhere \\(\\omega = \\sqrt{k / m}\\) is the resonance frequency of the oscillator and \\(H_n\\) is the \\(n\\)-th Hermite polynomial.\nA useful way to describe the quantum harmonic oscillator is by using the ladder operators\n\\[\\begin{eqnarray}\n    \\hat{a} &=& \\sqrt{\\frac{m \\omega}{2 \\hbar}} \\left( \\hat{x} + i \\frac{1}{m \\omega} \\hat{p} \\right) \\\\\n    \\hat{a}^\\dagger &=& \\sqrt{\\frac{m \\omega}{2 \\hbar}} \\left( \\hat{x} - i \\frac{1}{m \\omega} \\hat{p} \\right) \\, ,\n\\end{eqnarray}\\]\nnote that the position \\(\\hat{x}\\) and conjugate momentum \\(\\hat{p}\\) are operators too. If we now write the eigenstates in Equation 5.1 in the bra-ket notation (\\(\\psi_n \\to \\ket{n}\\)), the ladder operators allow us to move from one eigenstate to the next or previous one:\n\\[\\begin{eqnarray}\n    \\label{eq1: destroy operator action}\n    \\hat{a} \\ket{n} &=& \\sqrt{n} \\ket{n-1} \\\\\n    \\label{eq1: creation operator action}\n    \\hat{a}^\\dagger \\ket{n} &=& \\sqrt{n+1} \\ket{n+1} \\, ,\n\\end{eqnarray}\\]\nand it is straightforward to recognize the creation (\\(\\hat{a}^\\dagger\\)) and annihilation (\\(\\hat{a}\\)) operators. In this framework the system Hamiltonian of the quantum harmonic oscillator becomes \\[\n\\hat{H} = \\hbar \\omega \\left( \\hat{a}^\\dagger \\hat{a} + \\frac{1}{2} \\right) \\, .\n\\]\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import eval_hermite, factorial\n\n# Physical parameters\nm = 1.0\nk = 1.0\nw = np.sqrt(k/m)\nalpha = -np.sqrt(2)   # coherent‐state parameter\n\n# Grid\nbounds = 6.0\nx = np.linspace(-bounds, bounds, 1000)\n\n# n-th eigenfunction of the HO (hbar=1)\ndef psi(n, x):\n    Hn = eval_hermite(n, np.sqrt(m*w) * x)\n    norm = (m*w/np.pi)**0.25 / np.sqrt(2**n * factorial(n))\n    return norm * Hn * np.exp(-m*w*x**2/2)\n\n# Build the first six eigenstates and energies\npsi_n = [psi(n, x) for n in range(6)]\nE_n   = [(n + 0.5) * w for n in range(6)]\n\n# Coherent-state wavefunction (real alpha ⇒ no overall phase)\npsi_coh = (m*w/np.pi)**0.25 * np.exp(- (x - np.sqrt(2)*alpha)**2 / 2)\n\n# Plotting\nfig, ax = plt.subplots()\n\n# 1) potential\nax.plot(x, 0.5*k*x**2, 'k--', lw=2, label=r'$V(x)=\\tfrac12 k x^2$')\n\n# 2) coherent state\nax.fill_between(x, psi_coh, color='gray', alpha=0.5)\nax.plot(x, psi_coh, color='gray', lw=2, label='Coherent state')\n\n# 3) eigenstates offset by E_n\nlines = []\nfor n in range(6):\n    y = psi_n[n] + E_n[n]\n    line, = ax.plot(x, y, lw=2, label=fr'$|{n}\\rangle$')\n    lines.append(line)\n\n# Cosmetics\nax.set_ylim(0, 7)\nax.set_xlabel(r'$x$')\n\n# State labels on the right\nfor n, line in enumerate(lines):\n    ax.text(4.5, E_n[n] + 0.2, rf'$|{n}\\rangle$', color=line.get_color())\n\nax.text(-3.5, 6.5, r\"$V(x)$\")\nax.text(-3, 0.9, r\"$\\ket{\\alpha}$\", color=\"grey\")\nax.annotate(\"\", xy=(-5.5,3.5), xytext=(-5.5,2.5), arrowprops=dict(arrowstyle=\"&lt;-&gt;\"))\nax.text(-5.4, 3, r\"$\\hbar \\omega$\", ha=\"left\", va=\"center\")\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure A.1: First eigenstates of one-dimensional the quantum harmonic oscillator, each of them vertically shifted by the corresponding eigenvalue. The grey-filled curve corresponds to a coherent state with \\(\\alpha=-\\sqrt{2}\\). The used parameter are \\(m=1\\), \\(\\omega=1\\), and \\(\\hbar=1\\).\n\n\n\n\n\nIt is worth introducing the coherent state \\(\\ket{\\alpha}\\) of the harmonic oscillator, defined as the eigenstate of the destroy operator, with eigenvalue \\(\\alpha\\), in other words, \\(\\hat{a} \\ket{\\alpha} = \\alpha \\ket{\\alpha}\\). It can be expressed analytically in terms of the eigenstates of the quantum harmonic oscillator \\[\n\\begin{aligned}\n    \\ket{\\alpha} = e^{-\\frac{1}{2} \\abs{\\alpha}^2} \\sum_{n=0}^{\\infty} \\frac{\\alpha^n}{\\sqrt{n!}} \\ket{n} \\, ,\n\\end{aligned}\n\\]\nand it can be seen as the most classic-like state since it has the minimum uncertainty \\(\\Delta x \\Delta p = \\hbar / 2\\).\nFigure A.1 shows the first eigenstates of the quantum harmonic oscillator, each of them vertically shifted by the respective energy, while the grey-filled curve is a coherent state with \\(\\alpha = - \\sqrt{2}\\). The black dashed curve is the potential, choosing \\(k = 1\\), \\(m = 1\\), and \\(\\hbar = 1\\). It is worth noting that also the groundstate \\(\\ket{0}\\) has a nonzero energy (\\(E_0 = \\hbar \\omega / 2\\)).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The quantum harmonic oscillator</span>"
    ]
  },
  {
    "objectID": "appendices/wigner_function.html",
    "href": "appendices/wigner_function.html",
    "title": "Appendix B — From classical densities to the Wigner function",
    "section": "",
    "text": "B.1 Definition\nSo far we have:\nThe Wigner function builds a bridge between these two pictures.\nIt lives in phase space like \\(\\rho(x,p)\\), yet it is derived from \\(\\hat\\rho\\) and keeps all quantum information.\nFor a one–dimensional system the Wigner function is\n\\[\nW(x,p,t) \\;=\\;\n\\frac{1}{2\\pi\\hbar}\\,\n\\int_{-\\infty}^{+\\infty}\ndy\\;\ne^{-i p y/\\hbar}\\;\n\\bigl\\langle x+\\tfrac{y}{2}\\bigl|\\hat\\rho(t)\\bigr|x-\\tfrac{y}{2}\\bigr\\rangle.\n\\tag{B.1}\\]\nA quick checklist:",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>From classical densities to the Wigner function</span>"
    ]
  },
  {
    "objectID": "appendices/wigner_function.html#definition",
    "href": "appendices/wigner_function.html#definition",
    "title": "Appendix B — From classical densities to the Wigner function",
    "section": "",
    "text": "\\(x\\) and \\(p\\) are simultaneous variables (even though they do not commute quantum mechanically).\n\\(W\\) is real, but it can take negative values – an unmistakable quantum signature.\nThe normalisation matches that of \\(\\rho\\): \\(\\displaystyle\\int dx\\,dp\\,W(x,p,t)=1\\).\n\n\n\n\n\n\n\nClassical ↔︎ Quantum analogy\n\n\n\n\n\\(\\rho(x,p,t)\\) is always non-negative.\n\\(W(x,p,t)\\) can be negative, revealing quantum interference.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>From classical densities to the Wigner function</span>"
    ]
  },
  {
    "objectID": "appendices/wigner_function.html#marginals-and-expectation-values",
    "href": "appendices/wigner_function.html#marginals-and-expectation-values",
    "title": "Appendix B — From classical densities to the Wigner function",
    "section": "B.2 Marginals and expectation values",
    "text": "B.2 Marginals and expectation values\nDespite possible negativity, \\(W\\) returns the correct probability densities for position and momentum:\n\\[\n\\int_{-\\infty}^{+\\infty} dp\\;W(x,p,t) \\;=\\; \\langle x|\\hat\\rho|x\\rangle \\;=\\; |\\psi(x,t)|^{2},\n\\]\n\\[\n\\int_{-\\infty}^{+\\infty} dx\\;W(x,p,t) \\;=\\; \\langle p|\\hat\\rho|p\\rangle .\n\\]\nExpectation values of symmetrised operators follow the phase-space average rule\n\\[\n\\langle \\hat A \\rangle \\;=\\; \\int dx\\,dp\\;A_{\\text{W}}(x,p)\\;W(x,p,t),\n\\]\nwhere \\(A_{\\text{W}}\\) is the Weyl symbol of \\(\\hat A\\) (the phase-space version of the operator).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>From classical densities to the Wigner function</span>"
    ]
  },
  {
    "objectID": "appendices/wigner_function.html#properties-worth-remembering",
    "href": "appendices/wigner_function.html#properties-worth-remembering",
    "title": "Appendix B — From classical densities to the Wigner function",
    "section": "B.3 Properties worth remembering",
    "text": "B.3 Properties worth remembering\n\n\n\nProperty\nClassical \\(\\rho\\)\nQuantum \\(W\\)\n\n\n\n\nReal\n✓\n✓\n\n\nNon-negative\n✓\n✗ possible negativity\n\n\nNormalised\n✓\n✓\n\n\nObeys continuity / Liouville\n✓\n✓, plus \\(\\hbar\\)-corrections\n\n\nSupports interference fringes\n✗\n✓",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>From classical densities to the Wigner function</span>"
    ]
  },
  {
    "objectID": "appendices/wigner_function.html#example-gaussian-wavepacket",
    "href": "appendices/wigner_function.html#example-gaussian-wavepacket",
    "title": "Appendix B — From classical densities to the Wigner function",
    "section": "B.4 Example : Gaussian wavepacket",
    "text": "B.4 Example : Gaussian wavepacket\nFor a minimum-uncertainty Gaussian wavepacket\n\\[\n\\psi(x) \\;=\\;\n\\frac{1}{(2\\pi\\sigma_x^2)^{1/4}}\n\\exp\\!\\Bigl[-\\frac{(x-x_0)^2}{4\\sigma_x^2} + i\\,p_0(x-x_0)/\\hbar\\Bigr],\n\\]\nthe Wigner function is also Gaussian and everywhere positive:\n\\[\nW(x,p) \\;=\\;\n\\frac{1}{\\pi\\hbar}\\,\n\\exp\\!\\Bigl[-\\frac{(x-x_0)^2}{2\\sigma_x^2}\n-\\frac{2\\sigma_x^2}{\\hbar^2}(p-p_0)^2\\Bigr].\n\\]\nNegativity appears only when the state contains quantum interference, for example in superpositions of spatially separated Gaussians.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>From classical densities to the Wigner function</span>"
    ]
  },
  {
    "objectID": "appendices/wigner_function.html#why-the-wigner-function-matters",
    "href": "appendices/wigner_function.html#why-the-wigner-function-matters",
    "title": "Appendix B — From classical densities to the Wigner function",
    "section": "B.5 Why the Wigner function matters",
    "text": "B.5 Why the Wigner function matters\n\nIt lets us visualise quantum states in the familiar \\((x,p)\\) plane.\n\nMany semiclassical techniques expand around \\(W\\) and truncate the series.\n\nIn quantum optics the Wigner function of an electromagnetic mode can be measured with homodyne tomography (see later lectures).\n\nUsing \\(W(x,p,t)\\) we now have a complete trio:\n\nClassical ensemble — \\(\\rho(x,p,t)\\) (Liouville).\n\nQuantum wavefunction/density operator — \\(\\psi(x,t)\\) / \\(\\hat\\rho(t)\\) (Schrödinger/von Neumann).\n\nQuantum phase-space picture — \\(W(x,p,t)\\) (Moyal evolution).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>From classical densities to the Wigner function</span>"
    ]
  },
  {
    "objectID": "appendices/wigner_function.html#example-the-nonlinear-oscillator",
    "href": "appendices/wigner_function.html#example-the-nonlinear-oscillator",
    "title": "Appendix B — From classical densities to the Wigner function",
    "section": "B.6 Example: the nonlinear oscillator",
    "text": "B.6 Example: the nonlinear oscillator\nIn Chapter 6 we solved the Liouville equation for a classical nonlinear oscillator with Hamiltonian\n\\[\nH = \\frac{p^2}{2m} + \\frac{1}{2} k x^2 + g x^4.\n\\tag{B.2}\\]\nMoreover, in Chapter 9 we solved the Schrödinger equation using QuTiP for aq simple harmonic oscillator. We now combine these two approaches to study the quantum nonlinear oscillator with Hamiltonian Equation B.2, but with the quantum operator \\(\\hat x\\) and \\(\\hat p\\) instead of the classical variables \\(x\\) and \\(p\\).\n\n\nimport numpy as np\nfrom qutip import *\n\nN = 120\nm = 0.5 # Mass of the particle\nk = 2.0 # Spring constant\nG = 0.15 # Nonlinear constant\nw = np.sqrt(k/m) # Angular frequency\n\na = destroy(N)\n\nx = np.sqrt(m * w / 2) * (a + a.dag())\np = 1j * np.sqrt(m * w / 2) * (a.dag() - a)\n\nH = p**2 / (2 * m) + k * x**2 / 2 + G * x**4\n\n# Initial state: coherent state\nalpha = np.sqrt(1 / (2 * m * w)) * 1 + 1j * np.sqrt(m * w / 2) * 0.1\npsi_0 = coherent(N, alpha)\n\ntlist = np.linspace(0, 2, 500)\nresult = sesolve(H, psi_0, tlist)\nresult\n\n&lt;Result\n  Solver: sesolve\n  Solver stats:\n    method: 'scipy zvode adams'\n    init time: 0.00017714500427246094\n    preparation time: 0.00014591217041015625\n    run time: 0.9353423118591309\n    solver: 'Schrodinger Evolution'\n  Time interval: [0.0, 2.0] (500 steps)\n  Number of e_ops: 0\n  States saved.\n&gt;\n\n\n\nThe Wigner function can be computed from the resulting state using QuTiP’s wigner function. Let’s plot its evolution over time:\n\n\nimport matplotlib.pyplot as plt\nfrom IPython.display import HTML\nfrom matplotlib.animation import FuncAnimation\n\n# Phase space grid\nx_list = np.linspace(-5, 5, 120)\npx_list = np.linspace(-5, 5, 120)\n\nplt.rcParams.update({\"font.size\": 8})\nfig, ax = plt.subplots(figsize=(4.6, 2.8))\n\nfig.suptitle(r\"Wigner function evolution of a nonlinear oscillator\")\n\nimg = ax.pcolormesh(x_list, px_list, wigner(result.states[0], x_list, px_list),\n                    shading=\"gouraud\", rasterized=True,\n                    vmin=-0.3, vmax=0.3, cmap=\"PuOr\")\nax.set_xlabel(\"Position $x$\")\nax.set_ylabel(\"Momentum $p_x$\")\n\ndef animate(i):\n  wig_i = wigner(result.states[i], x_list, px_list)\n  img.set_array(wig_i.ravel())\n  return img,\n\nani = FuncAnimation(fig, animate, frames=range(0, len(tlist), 10), interval=200, blit=True)\n\nplt.close(fig)\nHTML(ani.to_jshtml())\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>From classical densities to the Wigner function</span>"
    ]
  }
]