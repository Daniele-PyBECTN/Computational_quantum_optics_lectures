[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Numerical Methods for Quantum Optics and Open Quantum Systems",
    "section": "",
    "text": "Numerical Methods for Quantum Optics and Open Quantum Systems is a hands-on course that shows you how to model and simulate open quantum systems in quantum optics with Python and QuTiP. The notes mix concise explanations, essential equations, and runnable code cells that work both on your computer and in Google Colab. Everything lives in a Quarto project on GitHub and is published in HTML and PDF for easy reading and collaboration. By the end, you will be able to set up and explore standard problems—such as photon cavities, two-level atoms, and open-system dynamics—using tools you can reuse in research and projects.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Home Page</span>"
    ]
  },
  {
    "objectID": "lecture1/introduction.html",
    "href": "lecture1/introduction.html",
    "title": "2  Introduction",
    "section": "",
    "text": "2.1 Why simulate open quantum systems?\nThe experimental frontier of quantum optics increasingly targets systems that cannot be described by perfectly isolated, unitary dynamics. Photons leak from cavities, solid‑state qubits couple to phonons, and measurement back‑action reshapes quantum states in real time. In these scenarios the open character of the system—the interplay between coherent evolution and irreversible processes—becomes the defining feature, not a perturbation. Analytical solutions exist only for a handful of toy models; to design devices, interpret data, and test conceptual ideas we therefore rely on numerical simulation of open quantum dynamics.\nNumerical methods allow us to:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "lecture1/introduction.html#why-simulate-open-quantum-systems",
    "href": "lecture1/introduction.html#why-simulate-open-quantum-systems",
    "title": "2  Introduction",
    "section": "",
    "text": "Predict observables such as spectra, correlation functions, or entanglement measures before running an experiment.\nPrototype control protocols (e.g., pulse shaping or feedback) that can stabilize fragile quantum states.\nExplore parameter regimes that are inaccessible analytically, revealing new phenomena like dissipative phase transitions or non‑Markovian memory effects.\n\n\n\n\n\n\n\nFigure 2.1: Description of an open quantum system and its practical applications. A quantum system interacts with a macroscopic environment, leading to decoherence and dissipation. The evolution of the system is described the master equation \\(\\dot{\\hat{\\rho}} = \\mathcal{L}_T(t) [\\hat{\\rho}]\\), where \\(\\hat{\\rho}\\) is the density matrix and \\(\\mathcal{L}_T(t)\\) is the Liouville superoperator. The solution can be used to study the steady state and non-equilibrium properties of the system. The theoretical study of open quantum systems offers several tools for modeling spin resonance, optical spectra, and quantum information processing, and their use is certainly not limited to these fields and applications. Reproduced from (Campaioli, Cole, and Hapuarachchi 2024) under a CC BY 4.0 license.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "lecture1/introduction.html#why-python",
    "href": "lecture1/introduction.html#why-python",
    "title": "2  Introduction",
    "section": "2.2 Why Python?",
    "text": "2.2 Why Python?\nPython is not the fastest language for floating‑point arithmetic—compiled languages like C or Fortran still win raw speed benchmarks—but it has become the lingua franca of modern scientific computing. Three qualities make it particularly compelling for our purposes:\n\nExpressiveness – A succinct, readable syntax lowers cognitive overhead and lets us translate mathematical ideas into code quickly.\nRich ecosystem – Numpy, SciPy, Jupyter, Matplotlib, and data‑analysis libraries coexist seamlessly, providing everything from linear algebra kernels to publication‑quality plots.\nCommunity & portability – Tutorials, StackOverflow answers, CI pipelines, and cloud platforms such as Google Colab enable beginners to run the same notebooks locally or on GPUs in the cloud with negligible setup.\n\nMost importantly, Python hosts QuTiP (Quantum Toolbox in Python)(Johansson, Nation, and Nori 2012; Lambert et al. 2024) the de‑facto standard library for simulating open quantum systems. QuTiP wraps efficient C and Fortran back‑ends behind a high‑level interface: you manipulate Qobj instances instead of raw matrices, and you call solvers such as mesolve or mcsolve for Lindblad‑master equations and quantum trajectory simulations, respectively. The package is actively maintained, well documented, and battle‑tested across thousands of research papers.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "lecture1/introduction.html#how-does-python-differ-from-other-mainstream-languages",
    "href": "lecture1/introduction.html#how-does-python-differ-from-other-mainstream-languages",
    "title": "2  Introduction",
    "section": "2.3 How does Python differ from other mainstream languages?",
    "text": "2.3 How does Python differ from other mainstream languages?\n\n\n\n\n\n\n\n\n\nLanguage\nParadigm\nTypical strength\nTypical weakness\n\n\n\n\nC / C++\nCompiled, low‑level\nMaximal performance, fine‑grained memory control\nVerbose, higher barrier to entry, manual parallelization\n\n\nFortran\nCompiled, array‑oriented\nLegacy HPC codes, excellent BLAS/LAPACK bindings\nLimited modern features, smaller community\n\n\nMATLAB\nProprietary, array‑oriented\nIntegrated IDE, built‑in plotting, domain‑specific toolboxes\nLicense cost, closed ecosystem\n\n\nPython\nInterpreted, multi‑paradigm\nReadability, vast open‑source libraries, rapid prototyping\nOverhead of interpreter, GIL limits naive multithreading\n\n\n\nPython balances high‑level productivity with the option to call compiled extensions (via Cython, Numba, or Rust bindings) whenever performance matters.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "lecture1/introduction.html#a-glance-at-julia-and-quantumtoolbox.jl",
    "href": "lecture1/introduction.html#a-glance-at-julia-and-quantumtoolbox.jl",
    "title": "2  Introduction",
    "section": "2.4 A glance at Julia and QuantumToolbox.jl",
    "text": "2.4 A glance at Julia and QuantumToolbox.jl\nWhile Python dominates current scientific computing, it is not the only contender. In recent years, researchers and engineers have been exploring the need for a new programming language—one that combines the performance of compiled languages like C or Fortran with the ease of use and readability of scripting languages like Python or MATLAB. This is the motivation behind Julia.\nJulia promises “C‑like speed with Python‑like syntax” by using just‑in‑time (JIT) compilation and a multiple‑dispatch programming model. Within this language, the package QuantumToolbox.jl(Mercurio et al. 2025) has emerged as a high‑performance analog to QuTiP. It mirrors QuTiP’s API but benefits from Julia’s performance model and native automatic differentiation. Benchmarks already demonstrate significant speed‑ups, especially for large Hilbert spaces and GPU‑accelerated workloads.\nNevertheless, Julia’s ecosystem is still maturing. Its tooling, package stability, and IDE support are evolving rapidly but are not yet as robust as Python’s. Similarly, QuantumToolbox.jl, while powerful, has a smaller user base and fewer educational resources compared to QuTiP. For a course focused on accessibility and broad applicability, we therefore choose to prioritize Python and QuTiP as the more mature and stable learning platform.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "lecture1/introduction.html#course-scope",
    "href": "lecture1/introduction.html#course-scope",
    "title": "2  Introduction",
    "section": "2.5 Course scope",
    "text": "2.5 Course scope\nIn this course we therefore focus on Python + QuTiP. You will learn to:\n\nBuild Hamiltonians and collapse operators in a composable way.\nIntegrate master equations and unravel them into quantum trajectories.\nCompute expectation values, spectra, and correlation functions.\nCouple simulations to optimisation or machine‑learning workflows within the wider Python ecosystem.\n\nWhere Julia can offer useful perspective we will point out parallels, but all hands‑on examples will run in Python notebooks that you can execute locally or on Colab.\n\nTake‑away: Numerical simulation is the microscope of modern quantum optics. Python and QuTiP give us a practical, accessible, and well‑supported platform for that microscope—letting us peer into the dynamics of open quantum systems without getting lost in low‑level details.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "lecture1/introduction.html#first-steps-in-python-lists-loops-and-functions",
    "href": "lecture1/introduction.html#first-steps-in-python-lists-loops-and-functions",
    "title": "2  Introduction",
    "section": "2.6 First steps in Python: lists, loops, and functions",
    "text": "2.6 First steps in Python: lists, loops, and functions\n\n2.6.1 Creating and using lists\nBefore diving into numerical simulations, it’s useful to get acquainted with the basic syntax and features of Python. One of the simplest and most commonly used data structures is the list, which stores a sequence of elements. Lists are flexible—they can contain numbers, strings, or even other lists.\nHere’s how to create and access elements in a list:\n\n\nfruits = ['apple', 'banana', 'cherry']\nprint(f'First fruit: {fruits[0]}')\n\nFirst fruit: apple\n\n\n\n\n\n2.6.2 For loops\nA for loop allows us to iterate through each item in a collection and execute the same block of code for every element. You will use loops constantly—whether you are sweeping parameter values, accumulating results, or analysing datasets—so it is worth seeing the syntax early.\n\n\nfor fruit in fruits:\n    print(f'I like {fruit}')\n\nI like apple\nI like banana\nI like cherry\n\n\n\n\n\n2.6.3 Defining functions\nFunctions bundle reusable logic behind a descriptive name. In quantum‑optics simulations, well‑structured functions help keep notebooks tidy—for instance, collecting the code that builds a Hamiltonian or evaluates an observable in one place. Below is a minimal example that squares a number.\n\n\ndef square(x):\n    return x * x\n\nprint(square(5))\n\n25\n\n\n\n\n\n2.6.4 Lambda (anonymous) functions\nOccasionally we only need a small, throw‑away function—say, as a callback or key in a sort operation. Python’s lambda syntax lets us declare such anonymous functions in a single line, without the ceremony of def.\n\n\nsquare_lambda = lambda x: x * x\nprint(square_lambda(5))\n\n25\n\n\n\n\n\n2.6.5 Complex numbers\nPython has built‑in support for complex numbers, which are represented as a + bj, where a is the real part and b is the imaginary part. This is particularly useful in quantum mechanics, where complex numbers are ubiquitous.\n\n\nz = 1 + 2j\nprint(f'Complex number: {z}')\nprint(f'Real part: {z.real}')\nprint(f'Magnitude: {abs(z)}')\n\nComplex number: (1+2j)\nReal part: 1.0\nMagnitude: 2.23606797749979\n\n\n\n\n\n2.6.6 Why plain Python lists can be slow\nPython lists store references to arbitrary Python objects. Each element carries its own type information and reference count. When you perform arithmetic on list elements, the interpreter must\n\nLook up the byte‑code for each operation.\nResolve types at runtime.\nDispatch to the correct C implementation.\n\nThis per‑element overhead dominates runtime in numerical workloads.\n\n\n2.6.7 Enter numpy\nTo overcome the performance limits of pure‑Python lists, we turn to NumPy, which stores data in contiguous, fixed‑type arrays and dispatches mathematical operations to highly‑optimised C (and often SIMD/GPU) kernels. The example below shows how you can express a million‑element computation in just two vectorised lines.\nnumpy provides fixed‑type, contiguous arrays backed by efficient C (or SIMD/GPU) loops. Operations are dispatched once for the whole array, eliminating Python‑level overhead and unlocking BLAS/LAPACK acceleration.\nAs an example, we can compute the sum of all the elements of a python list, comparing the performance with a numpy array.\n\n\nimport numpy as np\nimport time # Only for benchmarking\n\nmy_list = [i / 1_000_000 for i in range(1_000_000)]\n\nstart = time.time() # start timer\nsum_list = sum(my_list)  # sum using Python list\nend = time.time()  # end timer\nprint(f'Sum using list: {sum_list}, '\n      f'Time taken: {1e3*(end - start):.4f} milliseconds')\n\nmy_list_numpy = np.array(my_list)\nstart = time.time()  # start timer\nsum_numpy = np.sum(my_list_numpy)  # sum using numpy array\nend = time.time()  # end timer\nprint(f'Sum using numpy: {sum_numpy}, '\n      f'Time taken: {1e3*(end - start):.4f} milliseconds')\n\nSum using list: 499999.5, Time taken: 7.7283 milliseconds\nSum using numpy: 499999.5, Time taken: 0.4094 milliseconds\n\n\n\nNumPy is also able to perform vectorized operations, which let us express complex computations in a few lines of code. For example, we can compute a function of all elements in an array without writing explicit loops. This is not only more readable but also significantly faster, as the underlying C code can be optimised for performance.\n\n\n# Vectorized array operations\nx = np.linspace(0, 100, 1_000_000)\ny = np.sin(x) + 0.5 * x**2\nprint(y[:5])  # show first five results\n\n[0.         0.00010001 0.00020002 0.00030005 0.00040008]\n\n\n\nOne line performs a million floating‑point operations in compiled code—often orders of magnitude faster than an explicit Python loop.\n\n\n\n\nCampaioli, Francesco, Jared H. Cole, and Harini Hapuarachchi. 2024. “Quantum Master Equations: Tips and Tricks for Quantum Optics, Quantum Computing, and Beyond.” PRX Quantum 5 (June): 020202. https://doi.org/10.1103/PRXQuantum.5.020202.\n\n\nJohansson, J. R., P. D. Nation, and Franco Nori. 2012. “QuTiP: An open-source Python framework for the dynamics of open quantum systems.” Computer Physics Communications 183 (8): 1760–72. https://doi.org/10.1016/j.cpc.2012.02.021.\n\n\nLambert, Neill, Eric Giguère, Paul Menczel, Boxi Li, Patrick Hopf, Gerardo Suárez, Marc Gali, et al. 2024. “QuTiP 5: The Quantum Toolbox in Python.” arXiv:2412.04705. https://arxiv.org/abs/2412.04705.\n\n\nMercurio, Alberto, Yi-Te Huang, Li-Xun Cai, Yueh-Nan Chen, Vincenzo Savona, and Franco Nori. 2025. “QuantumToolbox.jl: An Efficient Julia Framework for Simulating Open Quantum Systems.” arXiv Preprint arXiv:2504.21440. https://doi.org/10.48550/arXiv.2504.21440.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "lecture1/linear_algebra.html",
    "href": "lecture1/linear_algebra.html",
    "title": "3  Linear Algebra with NumPy and SciPy",
    "section": "",
    "text": "3.1 NumPy: The Foundation of Dense Linear Algebra\nQuantum systems are described by vectors and operators in complex Hilbert spaces. States \\(\\vert \\psi \\rangle\\) correspond to column vectors, and observables—like the Hamiltonian \\(\\hat{H}\\) or spin operators—are represented by matrices. Tasks such as finding energy spectra via eigenvalue decompositions, simulating time evolution through operator exponentials, and building composite systems with tensor (Kronecker) products all reduce to core linear‐algebra operations.\nIn this chapter, we will leverage NumPy’s and SciPy’s routines (backed by optimized BLAS/LAPACK) to perform matrix–matrix products, eigen-decompositions, vector norms, and more. When system size grows, SciPy’s sparse data structures and Krylov‐subspace solvers will let us handle very large, structured operators efficiently.\nBy blending physical intuition (Schrödinger’s equation, expectation values, operator algebra) with hands‐on Python code, you’ll see how powerful and intuitive modern linear‐algebra libraries can be for quantum‐mechanics simulations. Let’s get started!\nNumPy provides the ndarray type, an efficient, N-dimensional array stored in contiguous memory. This layout makes vectorized operations and low-level BLAS calls blazing fast. At its simplest, a 2D ndarray represents a matrix:\n\\[\nA = \\begin{pmatrix}a_{11} & a_{12}\\\\ a_{21} & a_{22}\\end{pmatrix},\n\\]\nand a 1D ndarray represents a column vector:\n\\[\n\\mathbf{v} = \\begin{pmatrix}v_1\\\\ v_2\\end{pmatrix}.\n\\]\nNumPy’s dense arrays form the backbone of many quantum‐simulation tasks—building Hamiltonians, computing overlaps, and propagating states all reduce to these core operations. Having a quick reference for them can speed up both writing and reading simulation code.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Algebra with NumPy and SciPy</span>"
    ]
  },
  {
    "objectID": "lecture1/linear_algebra.html#numpy-the-foundation-of-dense-linear-algebra",
    "href": "lecture1/linear_algebra.html#numpy-the-foundation-of-dense-linear-algebra",
    "title": "3  Linear Algebra with NumPy and SciPy",
    "section": "",
    "text": "3.1.1 Summary of Core Functions\n\n\n\n\n\n\n\n\nOperation\nEquation\nNumPy call\n\n\n\n\nMatrix–matrix product\n\\(C = A B\\)\nC = A.dot(B) or A @ B\n\n\nMatrix–vector product\n\\(\\mathbf{w} = A \\mathbf{v}\\)\nw = A.dot(v)\n\n\nEigenvalues and eigenvectors\n\\(A \\mathbf{x} = \\lambda \\mathbf{x}\\)\nw, v = np.linalg.eig(A)\n\n\nDeterminant\n\\(\\det(A)\\)\nnp.linalg.det(A)\n\n\nInverse\n\\(A^{-1}\\)\nnp.linalg.inv(A)\n\n\nNorm (Frobenius)\n\\(\\|A\\|_F = \\sqrt{\\sum_{ij} \\vert a_{ij} \\vert^2}\\)\nnp.linalg.norm(A)\n\n\nKronecker product\n\\(A \\otimes B\\)\nnp.kron(A, B)\n\n\n\nIn the table above, each abstract operation is paired with its NumPy call. Notice how intuitive the syntax is: the @ operator reads like the usual linear-algebra notation.\n\n\n3.1.2 Matrix–Matrix and Matrix–Vector Multiplication\nLet’s consider a simple example of a 2×2 matrix \\(A\\) and a 2-vector \\(\\mathbf{v}\\). This captures key ideas: operator composition via matrix–matrix products and state evolution via matrix–vector products. Indeed, in quantum mechanics, applying one operator after another corresponds to a matrix–matrix product, while acting on a quantum state uses a matrix–vector product. Consider the following:\n\n\nimport numpy as np\n\n# Define a 2×2 matrix and a 2-vector\nA = np.array([[1, 2], [3, 4]])\nv = np.array([5, 6])\n\n# Matrix–matrix product\nc = A @ A  # same as A.dot(A)\ndisplay(\"A @ A =\", c)\n\n# Matrix–vector product\nw = A @ v  # same as A.dot(v)\ndisplay(\"A @ v =\", w)\n\n'A @ A ='\n\n\narray([[ 7, 10],\n       [15, 22]])\n\n\n'A @ v ='\n\n\narray([17, 39])\n\n\n\nHere, A @ A computes \\(A^2\\), and A @ v computes \\(A\\mathbf{v}\\).\n\n\n3.1.3 Diagonalization\nThe eigenvalue problem is one of the cornerstones of both applied mathematics and quantum mechanics. Given a square matrix \\(A \\in \\mathbb{C}^{n\\times n}\\), we seek scalars \\(\\lambda\\in\\mathbb{C}\\) (eigenvalues) and nonzero vectors \\(\\mathbf{x}\\in\\mathbb{C}^n\\) (eigenvectors) such that\n\\[\nA \\,\\mathbf{x} = \\lambda\\,\\mathbf{x}.\n\\]\nPhysically, in quantum mechanics, \\(A\\) might be the Hamiltonian operator \\(\\hat H\\), its eigenvalues \\(\\lambda\\) correspond to allowed energy levels, and the eigenvectors \\(\\mathbf{x}\\) represent stationary states. Mathematically, diagonalizing \\(A\\) transforms it into a simple form\n\\[\nA = V \\,\\Lambda\\, V^{-1},\n\\]\nwhere \\(\\Lambda\\) is the diagonal matrix of eigenvalues and the columns of \\(V\\) are the corresponding eigenvectors. Once in diagonal form, many operations—such as computing matrix exponentials for time evolution, powers of \\(A\\), or resolving a system of differential equations—become trivial:\n\n\\[\nf(A) = V\\,f(\\Lambda)\\,V^{-1},\\quad\nf(\\Lambda) = \\mathrm{diag}\\bigl(f(\\lambda_1),\\dots,f(\\lambda_n)\\bigr).\n\\]\nIn practice, NumPy’s np.linalg.eig calls optimized LAPACK routines to compute all eigenpairs of a dense matrix:\n\n\nw, v = np.linalg.eig(A)\ndisplay(\"Eigenvalues:\", w)\ndisplay(\"Eigenvectors (as columns):\\n\", v)\n\n'Eigenvalues:'\n\n\narray([-0.37228132,  5.37228132])\n\n\n'Eigenvectors (as columns):\\n'\n\n\narray([[-0.82456484, -0.41597356],\n       [ 0.56576746, -0.90937671]])\n\n\n\nUnder the hood, NumPy calls optimized LAPACK routines to diagonalize dense matrices.\n\n\n3.1.4 Kronecker Product\nIn quantum mechanics, the state space of a composite system is the tensor product of the state spaces of its subsystems. If system 1 has Hilbert space \\(\\mathcal H_A\\) of dimension \\(m\\) and system 2 has \\(\\mathcal H_B\\) of dimension \\(p\\), then the joint space is \\(\\mathcal H_A\\otimes\\mathcal H_B\\) of dimension \\(m p\\). Operators on the composite system factorize as tensor (Kronecker) products of subsystem operators. For example, if \\(A\\) acts on system 1 and \\(B\\) on system 2, then \\[\nA\\otimes B\\;:\\;\\mathcal H_A\\otimes\\mathcal H_B\\to \\mathcal H_A\\otimes\\mathcal H_B\n\\] has matrix elements \\[\n(A\\otimes B)_{(i\\,,\\,\\alpha),(j\\,,\\,\\beta)}\n    = A_{ij}\\,B_{\\alpha\\beta},\n\\] and in block form \\[\nA \\otimes B\n= \\begin{pmatrix}\n    a_{11}\\,B & a_{12}\\,B & \\cdots & a_{1n}\\,B \\\\\n    \\vdots    &        &        & \\vdots     \\\\\n    a_{m1}\\,B & a_{m2}\\,B & \\cdots & a_{mn}\\,B\n\\end{pmatrix},\n\\] yielding an \\(mp\\times nq\\) matrix when \\(A\\in\\mathbb C^{m\\times n}\\) and \\(B\\in\\mathbb C^{p\\times q}\\).\nWhy is this useful? In later chapters we will build multi‐qubit gates (e.g. CNOT, controlled-phase), couple different oscillators, and assemble large Hamiltonians by taking tensor products of single‐mode operators. The Kronecker product lets us lift any local operator into the full, composite Hilbert space.\nIn NumPy, the Kronecker product is computed with np.kron:\n\n\nB = np.array([[0, 1], [1, 0]])  # Pauli-X matrix\nkron = np.kron(A, B)\ndisplay(\"A ⊗ B =\", kron)\n\n'A ⊗ B ='\n\n\narray([[0, 1, 0, 2],\n       [1, 0, 2, 0],\n       [0, 3, 0, 4],\n       [3, 0, 4, 0]])\n\n\n\nKronecker products build composite quantum-system operators from single-subsystem operators.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Algebra with NumPy and SciPy</span>"
    ]
  },
  {
    "objectID": "lecture1/linear_algebra.html#scipy-advanced-algorithms-and-sparse-data",
    "href": "lecture1/linear_algebra.html#scipy-advanced-algorithms-and-sparse-data",
    "title": "3  Linear Algebra with NumPy and SciPy",
    "section": "3.2 SciPy: Advanced Algorithms and Sparse Data",
    "text": "3.2 SciPy: Advanced Algorithms and Sparse Data\nWhile NumPy covers dense linear algebra, SciPy complements it with:\n\n\n\n\n\n\n\nModule\nPurpose\n\n\n\n\nscipy.linalg\nAlternative LAPACK-based routines for dense ops\n\n\nscipy.sparse\nData structures (COO, CSR, CSC) for sparse matrices\n\n\nscipy.sparse.linalg\nIterative solvers (e.g. Arnoldi, Lanczos)\n\n\nscipy.integrate\nODE and quadrature routines\n\n\nscipy.optimize\nRoot-finding and minimization\n\n\nscipy.special\nSpecial mathematical functions\n\n\n\nCompared to NumPy, SciPy’s routines often expose extra options (e.g. choosing solvers) and can handle very large, sparse systems efficiently.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Algebra with NumPy and SciPy</span>"
    ]
  },
  {
    "objectID": "lecture1/linear_algebra.html#some-useful-functions",
    "href": "lecture1/linear_algebra.html#some-useful-functions",
    "title": "3  Linear Algebra with NumPy and SciPy",
    "section": "3.3 Some Useful Functions",
    "text": "3.3 Some Useful Functions\nBelow are a few handy SciPy routines:\n\nDeterminant: scipy.linalg.det\nInverse: scipy.linalg.inv\nFrobenius norm: scipy.linalg.norm\n\n\n\nimport scipy.linalg as la\n\ndet = la.det(A)\ninv = la.inv(A)\nnorm_f = la.norm(A)\ndisplay(det, inv, norm_f)\n\nnp.float64(-2.0)\n\n\narray([[-2. ,  1. ],\n       [ 1.5, -0.5]])\n\n\nnp.float64(5.477225575051661)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Algebra with NumPy and SciPy</span>"
    ]
  },
  {
    "objectID": "lecture1/linear_algebra.html#solving-linear-systems",
    "href": "lecture1/linear_algebra.html#solving-linear-systems",
    "title": "3  Linear Algebra with NumPy and SciPy",
    "section": "3.4 Solving Linear Systems",
    "text": "3.4 Solving Linear Systems\nA linear system has the form\n\\[\nA\\,\\mathbf{x} = \\mathbf{b},\n\\]\nwhere \\(A\\in\\mathbb R^{n\\times n}\\) and \\(\\mathbf b\\in\\mathbb R^n\\) is known. For small \\(n\\) you can even solve by hand. For example, consider the \\(2\\times2\\) system\n\\[\n\\begin{cases}\nx_1 + 2x_2 = 5,\\\\\n3x_1 + 4x_2 = 11.\n\\end{cases}\n\\quad\\Longrightarrow\\quad\nA = \\begin{pmatrix}1 & 2\\\\ 3 & 4\\end{pmatrix},\\;\n\\mathbf b = \\begin{pmatrix}5\\\\11\\end{pmatrix}.\n\\]\nWe can reproduce this with NumPy:\n\n\nA = np.array([[1, 2], [3, 4]])\nb = np.array([5, 11])\nx = np.linalg.solve(A, b)\ndisplay(\"Solution x=\", x)\n\n'Solution x='\n\n\narray([1., 2.])\n\n\n\nSciPy’s sparse module also offers scipy.sparse.linalg.spsolve for large, sparse \\(A\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Algebra with NumPy and SciPy</span>"
    ]
  },
  {
    "objectID": "lecture1/linear_algebra.html#sparse-matrices",
    "href": "lecture1/linear_algebra.html#sparse-matrices",
    "title": "3  Linear Algebra with NumPy and SciPy",
    "section": "3.5 Sparse Matrices",
    "text": "3.5 Sparse Matrices\nAs quantum systems scale to many degrees of freedom, the underlying operators—such as Hamiltonians or Liouvillian superoperators—grow exponentially in dimension but often remain highly structured and sparse. Instead of storing dense arrays with mostly zeros, sparse-matrix formats only record nonzero entries and their indices, dramatically reducing memory requirements. Common physical models, like spin chains with nearest-neighbor couplings or lattice Hamiltonians, have only \\(\\mathcal{O}(N)\\) or \\(\\mathcal{O}(N \\log N)\\) nonzero elements, making sparse representations essential for large-scale simulations.\nIn the following sections, we will:\n\nConstruct sparse matrices in COO formats with SciPy.\nIllustrate basic sparse-matrix operations (matrix–vector products, format conversions).\nUse scipy.sparse.linalg.eigs (Arnoldi) to compute a few eigenvalues of a sparse Hamiltonian.\n\nThe Coordinate (COO) format is a simple way to store sparse matrices. Instead of storing all entries, the COO format only keeps nonzero entries of the form \\((i, j, a_{ij})\\), which saves memory and speeds up computations. Graphically, a 5×5 example with 4 nonzeros might look like:\n\\[\nA = \\begin{pmatrix}\n7 & \\cdot & \\cdot & \\cdot & 1 \\\\\n\\cdot & \\cdot & 2      & \\cdot & \\cdot \\\\\n\\cdot & 3      & \\cdot & \\cdot & \\cdot \\\\\n\\cdot & \\cdot & \\cdot & \\cdot & \\cdot \\\\\n4      & \\cdot & \\cdot & \\cdot & \\cdot\n\\end{pmatrix}\n\\]\nHere each number shows a location and its value. COO is very simple and intuitive, but not the most efficient. For larger matrices, we can use the Compressed Sparse Row (CSR) or Compressed Sparse Column (CSC) formats, which store the nonzero entries in a more compact way. The CSR format is very efficient for matrix–vector products.\nSuch matrix can be created in SciPy using the coo_matrix class:\n\n\nfrom scipy import sparse\n\n# Create a sparse COO matrix\ni = [0, 0, 1, 2, 4] # row indices\nj = [0, 4, 2, 1, 0] # column indices\ndata = [7, 1, 2, 3, 4] # nonzero values\ncoo = sparse.coo_matrix((data, (i, j)), shape=(5, 5))\ncoo\n\n&lt;COOrdinate sparse matrix of dtype 'int64'\n    with 5 stored elements and shape (5, 5)&gt;\n\n\n\nIt is also possible to convert between different sparse formats. For example, to convert a COO matrix to CSR format, you can use the tocsc() method:\n\n\n# Convert COO to CSR format\ncsr = coo.tocsr()\ncsr\n\n&lt;Compressed Sparse Row sparse matrix of dtype 'int64'\n    with 5 stored elements and shape (5, 5)&gt;\n\n\n\nAnd the matrix–vector product is as simple as:\n\n\n# Matrix–vector product\nv = np.array([1, 2, 3, 4, 5])\nw = coo @ v  # same as coo.dot(v)\nw\n\narray([12,  6,  6,  0,  4])\n\n\n\n\n3.5.1 Eigenvalues of Sparse Matrices\nEven with sparse storage, direct methods (dense diagonalization or full factorization) become intractable when the matrix dimension exceeds millions. To extract a few extremal eigenvalues or approximate time evolution, Krylov-subspace approaches (like the Arnoldi algorithm) build a low-dimensional orthonormal basis that captures the action of the operator on a subspace. By repeatedly applying the sparse matrix to basis vectors and orthogonalizing, Arnoldi produces a small Hessenberg matrix whose eigenpairs approximate those of the full operator. This hybrid strategy leverages both memory-efficient storage and iterative linear algebra to access spectral properties of huge quantum systems.\nTo approximate a few eigenvalues of a large, sparse matrix \\(A\\), SciPy’s eigs implements the Arnoldi algorithm. Under the hood it builds an \\(m\\)-dimensional Krylov basis. More precisely, given a starting vector \\(v_1\\) with \\(\\|v_1\\|_2 = 1\\), the \\(m\\)‑dimensional Krylov subspace is\n\\[\n\\mathcal{K}_m(A, v_1) = \\operatorname{span}\\{v_1, Av_1, A^{2}v_1, \\dots, A^{m-1}v_1\\}.\n\\]\nThe Arnoldi iteration produces the decomposition\n\\[\nA V_m = V_m H_m + h_{m+1,m}\\, v_{m+1} e_m^{\\top},\n\\]\nwhere\n\n\\(V_m = [v_1, \\dots, v_m]\\) has orthonormal columns,\n\\(H_m\\) is an \\(m \\times m\\) upper‑Hessenberg matrix,\n\\(e_m\\) is the \\(m\\)‑th canonical basis vector.\n\nThe eigenvalues of \\(H_m\\) are called Ritz values; they approximate eigenvalues of \\(A\\). As \\(m\\) grows, the approximation improves. In practice we combine Arnoldi with a restart strategy (after reaching a given \\(m\\) we keep the most accurate Ritz vectors and build a fresh Krylov basis). SciPy’s scipy.sparse.linalg.eigs wrapper uses the implicitly restarted Arnoldi method from ARPACK.\nAs a pseudo-code, the Arnoldi algorithm can be summarized as follows:\n\nPick a random vector \\(v\\) and normalize it.\nFor \\(j = 1, \\dots, m\\)\n\n\\(w = A v_j\\)\nOrthogonalize: \\[h_{i,j} = v_i^{\\dagger} w, \\quad w \\leftarrow w - h_{i,j} v_i \\quad (i = 1, \\dots, j)\\]\n\\(h_{j+1,j} = \\|w\\|_2\\).\nIf \\(h_{j+1,j} = 0\\), stop (the Krylov subspace is invariant).\n\\(v_{j+1} = w / h_{j+1,j}\\).\n\n\nThe cost is \\(m\\) sparse matrix–vector products and \\(\\mathcal{O}(m^2 n)\\) scalar operations for orthogonalization (which stays moderate when \\(m \\ll n\\)).\nHere’s a concrete example:\n\n\nfrom scipy.sparse.linalg import eigs\n\n# Compute the 2 largest-magnitude eigenvalues of coo\nvals, vecs = eigs(coo, k=2)\ndisplay(\"Sparse eigenvalues:\", vals)\n\n'Sparse eigenvalues:'\n\n\narray([7.53112887+0.j, 2.44948974+0.j])",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Algebra with NumPy and SciPy</span>"
    ]
  },
  {
    "objectID": "lecture1/numba_and_jax.html",
    "href": "lecture1/numba_and_jax.html",
    "title": "4  Speeding up Python for Linear Algebra Tasks",
    "section": "",
    "text": "4.1 Numba: Just-In-Time Compilation\nPython is easy to read, but pure-Python loops can be slow if you do not leverage optimized libraries (BLAS, LAPACK). Here we explore two tools—Numba and JAX—to accelerate common linear algebra operations.\nNumba uses LLVM to compile Python functions to machine code at runtime. Key points:\nExample: Matrix–Vector Multiplication\nIn practice, Numba can speed up this looped version by 10×–100× compared to pure Python, approaching the speed of NumPy’s optimized routines. The reader is encouraged to try the code without the @njit decorator to see the difference in performance.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Speeding up Python for Linear Algebra Tasks</span>"
    ]
  },
  {
    "objectID": "lecture1/numba_and_jax.html#numba-just-in-time-compilation",
    "href": "lecture1/numba_and_jax.html#numba-just-in-time-compilation",
    "title": "4  Speeding up Python for Linear Algebra Tasks",
    "section": "",
    "text": "Decorators: Use @njit (nopython mode) for best speed.\n\nType inference: Numba infers types on first run, then compiles specialized code.\n\nCompilation overhead: The first call incurs compilation time; subsequent calls are fast.\n\nObject mode vs nopython mode: Always aim for nopython mode to avoid Python object overhead.\n\n\nJIT Workflow 1. Call function → type inference → LLVM IR generation.\n2. LLVM IR → machine code (cached).\n3. Subsequent calls use cached machine code.\n\n\n\n\nfrom numba import njit\nimport numpy as np\nimport time # for timing\n\n@njit\ndef matvec(A, x):\n    m, n = A.shape\n    y = np.zeros(m)\n    for i in range(m):\n        temp = 0.0\n        for j in range(n):\n            temp += A[i, j] * x[j]\n        y[i] = temp\n    return y\n\n# Prepare data\ndim = 500\nA = np.random.rand(dim, dim)\nx = np.random.rand(dim)\n\n# Using NumPy's dot product\nstart = time.time()\ny0 = A @ x\nend = time.time()\nprint(\"NumPy time (ms): \", 1e3*(end - start))\n\n# Using Numba's compiled function\ny0 = matvec(A, x) # First call for compilation\n\nstart = time.time()\ny1 = matvec(A, x)\nend = time.time()\nprint(\"Numba time (ms): \", 1e3*(end - start))\n\nNumPy time (ms):  0.3445148468017578\nNumba time (ms):  0.29969215393066406",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Speeding up Python for Linear Algebra Tasks</span>"
    ]
  },
  {
    "objectID": "lecture1/numba_and_jax.html#jax-xla-compilation-and-automatic-differentiation",
    "href": "lecture1/numba_and_jax.html#jax-xla-compilation-and-automatic-differentiation",
    "title": "4  Speeding up Python for Linear Algebra Tasks",
    "section": "4.2 JAX: XLA Compilation and Automatic Differentiation",
    "text": "4.2 JAX: XLA Compilation and Automatic Differentiation\nJAX is a high-performance library from Google Research that extends NumPy with just-in-time compilation and automatic differentiation. It - Compiles array operations via XLA, fusing kernels and reducing Python overhead. - Supports GPU and TPU backends with minimal code changes. - Provides grad for gradients of scalar functions, enabling optimisation and machine-learning tasks. - Offers advanced transformations like vmap (vectorisation) and pmap (parallelism on multiple devices).\nJAX is widely used in deep learning frameworks (e.g. Flax, Haiku), reinforcement learning, and scientific research (including physics simulations), thanks to its blend of speed and flexibility.\n\n4.2.1 A Quick Overview of Automatic Differentiation\nAutomatic differentiation (AD) is a family of techniques to compute exact derivatives of functions defined by computer programs. Unlike symbolic differentiation (which can lead to expression swell) or numerical finite-difference (which suffers from truncation and round-off error), AD exploits the fact that any complex function is ultimately composed of a finite set of elementary operations (addition, multiplication, sin, exp, …) whose derivatives are known exactly.\n\nLimitations of Finite Differences\nA common finite-difference formula for a scalar function \\(f(x)\\) is the central difference\n\\[\n\\frac{df}{dx}(x)\\approx \\frac{f(x+h)-f(x-h)}{2h},\n\\]\nwith local truncation error \\(\\mathcal{O}(h^2)\\). However, this approach has important limitations:\n\nTruncation vs. round-off: If \\(h\\) is too large, the \\(\\mathcal{O}(h^2)\\) term dominates. If \\(h\\) is too small, floating-point cancellation makes the numerator \\(f(x+h)-f(x-h)\\) inaccurate.\nCost with many parameters: For \\(f:\\mathbb{R}^n\\to\\mathbb{R}\\), the gradient component \\(i\\) is\n\\[\n\\frac{\\partial f}{\\partial x_i}(\\mathbf{x})\\approx \\frac{f(\\mathbf{x}+h\\mathbf{e}_i)-f(\\mathbf{x}-h\\mathbf{e}_i)}{2h}.\n\\]\nComputing all \\(n\\) components requires \\(2n\\) evaluations of \\(f\\), so the cost scales as \\(\\mathcal{O}(n)\\) in \\(f\\)-calls. For large \\(n\\) (many parameters), this becomes prohibitive.\nNon-smooth or branching code: When \\(f\\) contains control flow or non-differentiable operations, finite differences may give misleading or undefined results.\n\n\n\nAutomatic Differentiation and the Chain Rule\nAutomatic differentiation (AD) applies the chain rule to each elementary operation in code (addition, multiplication, sin, exp, etc.), yielding exact derivatives up to floating-point precision. For a composition\n\\[\nu = g(x),\\quad y = f(u),\n\\]\nAD uses the chain rule:\n\\[\n\\frac{dy}{dx} = \\frac{df}{du}\\frac{dg}{dx}.\n\\]\nIn more complex nests, e.g.\n\\[\nv = h(u),\\quad u = g(x),\\quad y = f(v),\n\\]\nwe get\n\\[\n\\frac{dy}{dx} = \\frac{df}{dv}\\frac{dh}{du}\\frac{dg}{dx}.\n\\]\nAD comes in two modes:\n\nForward mode (propagate derivatives from inputs to outputs).\nReverse mode (propagate sensitivities from outputs back to inputs).\n\nJAX implements both and selects the most efficient strategy automatically.\n\n\nComparing Accuracy: AD vs Finite Differences\nBelow is a Quarto code cell that plots the error of finite differences (varying step size \\(h\\)) and automatic differentiation against the true derivative of \\(f(x) = e^{\\sin(x)}\\) at \\(x=1.0\\).\n\n\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt # for plotting\n\n# Set JAX to use 64-bit floats\njax.config.update(\"jax_enable_x64\", True)\n\n# Define function and true derivative\ndef f_np(x):\n    return np.exp(np.sin(x))\n\ndef df_true(x):\n    return np.cos(x) * np.exp(np.sin(x))\n\n# Point of evaluation\nx0 = 1.0\n\n# Finite-difference errors for varying h\nhs = np.logspace(-8, -1, 50)\nerrors_fd = []\nfor h in hs:\n    df_fd = (f_np(x0 + h) - f_np(x0 - h)) / (2 * h)\n    errors_fd.append(abs(df_fd - df_true(x0)))\n\n# Automatic differentiation error (constant)\ndf_ad = jax.grad(lambda x: jnp.exp(jnp.sin(x)))(x0)\nerror_ad = abs(np.array(df_ad) - df_true(x0))\n\nprint(f\"AD error: {error_ad}\")\nprint(f\"FD minimum error: {min(errors_fd)}\")\n\n# Plot\nfig, ax = plt.subplots()\nax.loglog(hs, errors_fd, marker=\"o\")\nax.set_xlabel(\"Step size $h$\")\nax.set_ylabel(\"Error of Finite Differences\")\n\n# Show in Quarto\nplt.savefig('_tmp_fig.svg')\nplt.close(fig)\nSVG(filename='_tmp_fig.svg')\n\nAD error: 0.0\nFD minimum error: 7.006839553014288e-12\n\n\n\n\n\n\n\n\n\n\nThis plot illustrates that finite differences achieve minimal error at an optimal \\(h\\), but degrade for too large or too small \\(h\\), while AD remains accurate to machine precision regardless of step size.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Speeding up Python for Linear Algebra Tasks</span>"
    ]
  },
  {
    "objectID": "lecture1/numba_and_jax.html#why-computing-gradients-is-important-in-quantum-physics",
    "href": "lecture1/numba_and_jax.html#why-computing-gradients-is-important-in-quantum-physics",
    "title": "4  Speeding up Python for Linear Algebra Tasks",
    "section": "4.3 Why Computing Gradients Is Important in Quantum Physics",
    "text": "4.3 Why Computing Gradients Is Important in Quantum Physics\nIn quantum physics, many problems reduce to optimizing parameters in a model or a control protocol. Computing gradients of a cost function with respect to these parameters is essential for efficient and reliable optimization.\n\nVariational quantum algorithms: In methods like the variational quantum eigensolver (VQE)(Peruzzo et al. 2014), a parametrised quantum state \\(|\\psi(\\boldsymbol{\\theta})\\rangle\\) depends on parameters \\(\\boldsymbol{\\theta}=(\\theta_1,\\dots,\\theta_n)\\). One minimises the expectation \\[\nE(\\boldsymbol{\\theta}) = \\langle\\psi(\\boldsymbol{\\theta})|\\hat H|\\psi(\\boldsymbol{\\theta})\\rangle.\n\\]\nGradient-based methods require\n\\[\n\\frac{\\partial E}{\\partial \\theta_i} = \\frac{\\partial}{\\partial \\theta_i} \\langle\\psi(\\boldsymbol{\\theta})|\\hat H|\\psi(\\boldsymbol{\\theta})\\rangle.\n\\]\nAD enables exact evaluation of these derivatives through the quantum circuit parameters, improving convergence compared to gradient-free methods.\nQuantum optimal control(D’Alessandro 2021; Khaneja et al. 2005): One shapes control fields \\(u(t)\\) in the Hamiltonian \\[\n\\hat H(t; u) = \\hat H_0 + \\sum_i u_i(t) \\hat H_i\n\\]\nto drive the system from an initial state \\(|\\psi_0\\rangle\\) to a target \\(|\\psi_T\\rangle\\). A typical cost function is\n\\[\nJ[u] = 1 - |\\langle\\psi_T|\\mathcal U_T[u]|\\psi_0\\rangle|^2,\n\\]\nwhere \\(\\mathcal U_T[u]\\) is the time-ordered evolution. Computing gradients \\(\\delta J/\\delta u_i(t)\\) is needed for gradient-ascent pulse engineering (GRAPE) algorithms. AD can differentiate through time-discretised propagators and ODE solvers, automating derivation of \\(\\delta J/\\delta u_i(t)\\) and providing machine-precision gradients for faster convergence.\nParameter estimation and tomography(Lvovsky and Raymer 2009): Maximum-likelihood estimation for quantum states or processes often involves maximising a log-likelihood \\(L(\\boldsymbol{\\theta})\\). Gradients speed up estimation and enable standard optimisers (e.g. L-BFGS).\n\nBy providing exact, efficient gradients even through complex quantum simulations (time evolution, measurement models, noise), automatic differentiation (via JAX or similar frameworks) has become a key tool in modern quantum physics research.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Speeding up Python for Linear Algebra Tasks</span>"
    ]
  },
  {
    "objectID": "lecture1/numba_and_jax.html#summary",
    "href": "lecture1/numba_and_jax.html#summary",
    "title": "4  Speeding up Python for Linear Algebra Tasks",
    "section": "4.4 Summary",
    "text": "4.4 Summary\n\nNumba: Best for speeding up existing NumPy loops with minimal code changes. Ideal when you do not need gradients or accelerators.\nJAX: Ideal for optimisation tasks requiring gradients, large-scale batch operations, or GPU/TPU acceleration. The XLA compiler often outperforms loop-based JIT for fused kernels.\n\n\n\n\n\nD’Alessandro, Domenico. 2021. Introduction to Quantum Control and Dynamics. Chapman; Hall/CRC. https://doi.org/10.1201/9781003051268.\n\n\nKhaneja, Navin, Timo Reiss, Cindie Kehlet, Thomas Schulte-Herbrüggen, and Steffen J. Glaser. 2005. “Optimal Control of Coupled Spin Dynamics: Design of NMR Pulse Sequences by Gradient Ascent Algorithms.” Journal of Magnetic Resonance 172 (2): 296–305. https://doi.org/10.1016/j.jmr.2004.11.004.\n\n\nLvovsky, A. I., and M. G. Raymer. 2009. “Continuous-Variable Optical Quantum-State Tomography.” Rev. Mod. Phys. 81 (March): 299–332. https://doi.org/10.1103/RevModPhys.81.299.\n\n\nPeruzzo, Alberto, Jarrod McClean, Peter Shadbolt, Man-Hong Yung, Xiao-Qi Zhou, Peter J. Love, Alán Aspuru-Guzik, and Jeremy L. O’Brien. 2014. “A Variational Eigenvalue Solver on a Photonic Quantum Processor.” Nature Communications 5 (1). https://doi.org/10.1038/ncomms5213.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Speeding up Python for Linear Algebra Tasks</span>"
    ]
  },
  {
    "objectID": "lecture1/ordinary_differential_equations.html",
    "href": "lecture1/ordinary_differential_equations.html",
    "title": "5  Ordinary Differential Equations",
    "section": "",
    "text": "5.1 General Definition and Examples\nAn ordinary differential equation (ODE) is an equation involving functions of one independent variable (for instance, time) and its derivatives. In the simplest scenario, suppose we have an unknown function \\(y(t)\\). A first-order ODE can be written as:\n\\[\n\\frac{dy(t)}{dt} = f\\bigl(y(t), t\\bigr),\n\\]\nwhere \\(f\\) is a known function, and \\(y(t)\\) is the unknown to be determined. Higher-order ODEs can often be recast as systems of first-order ODEs by introducing additional variables for the higher derivatives.\nTo see how ODEs arise in physical scenarios, consider Newton’s second law, \\(m\\,\\frac{d^2 x}{dt^2} = F(x,t)\\). This second-order ODE can be reduced to a system of two first-order ODEs by introducing an auxiliary variable for velocity \\(v(t) = \\frac{dx}{dt}\\). Then we have:\n\\[\n\\begin{cases}\n    \\frac{dx}{dt} = v,\\\\\n    \\frac{dv}{dt} = \\frac{F(x,t)}{m}.\n  \\end{cases}\n\\]\nIn quantum mechanics, the time-dependent Schrödinger equation\n\\[\ni\\hbar \\,\\frac{d}{dt}\\ket{\\psi(t)} = \\hat{H}\\,\\ket{\\psi(t)}\n\\] can be viewed as a first-order ODE in the Hilbert space: the role of \\(\\ket{\\psi(t)}\\) is analogous to \\(y(t)\\), and \\(-\\frac{i}{\\hbar}\\,\\hat{H}\\) plays the role of \\(f(\\,\\cdot\\,,t)\\) (assuming a time-independent \\(\\hat{H}\\)). This analogy suggests that the Schrödinger equation can be treated using standard ODE solution techniques or, in more complicated cases, numerical integration.\nA linear ODE has the form: \\(\\frac{d\\mathbf{y}(t)}{dt} = A\\,\\mathbf{y}(t) + \\mathbf{b}(t),\\) where \\(\\mathbf{y}(t)\\) is a vector function of time, \\(A\\) is a constant (or possibly time-dependent) matrix, and \\(\\mathbf{b}(t)\\) is a known inhomogeneous term. If \\(\\mathbf{b}(t) = \\mathbf{0}\\), the equation is said to be homogeneous.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Ordinary Differential Equations</span>"
    ]
  },
  {
    "objectID": "lecture1/ordinary_differential_equations.html#solving-linear-odes-by-diagonalizing-the-system-matrix",
    "href": "lecture1/ordinary_differential_equations.html#solving-linear-odes-by-diagonalizing-the-system-matrix",
    "title": "5  Ordinary Differential Equations",
    "section": "5.2 Solving Linear ODEs by Diagonalizing the System Matrix",
    "text": "5.2 Solving Linear ODEs by Diagonalizing the System Matrix\nA common case in quantum mechanics and in classical physics is the linear homogeneous system:\n\\[\n\\frac{d\\mathbf{y}(t)}{dt} = A\\,\\mathbf{y}(t),\n\\quad\n\\mathbf{y}(0) = \\mathbf{y}_0,\n\\tag{5.1}\\]\nwhere \\(A\\) is a constant \\(n\\times n\\) matrix, and \\(\\mathbf{y}_0\\) is the initial condition.\n\n5.2.1 Eigenvalue Decomposition\nIf \\(A\\) is diagonalizable, we can write: \\[A = V \\, D \\, V^{-1},\\] where \\(D\\) is a diagonal matrix whose entries are the eigenvalues \\(\\lambda_i\\) of \\(A\\), and the columns of \\(V\\) are the corresponding eigenvectors. Define: \\[\\mathbf{z}(t) = V^{-1}\\,\\mathbf{y}(t).\\] Then, plugging this into Equation 5.1, we get\n\\[\n\\frac{d\\mathbf{z}(t)}{dt}\n  = V^{-1}\\, \\frac{d\\mathbf{y}(t)}{dt}\n  = V^{-1}\\, A\\, \\mathbf{y}(t)\n  = V^{-1}\\, (V \\, D \\, V^{-1})\\, \\mathbf{y}(t)\n  = D \\, \\mathbf{z}(t).\n\\]\nHence, in the \\(\\mathbf{z}\\)-coordinates, the system becomes a set of \\(n\\) uncoupled first-order ODEs:\n\\[\n\\frac{dz_i}{dt} = \\lambda_i\\, z_i(t),\n  \\quad\n  \\text{for } i = 1,\\dots,n.\n\\]\nThese have the well-known solutions:\n\\[\nz_i(t) = z_i(0)\\,e^{\\lambda_i t}.\n\\] To enforce the initial condition \\(\\mathbf{y}(0) = \\mathbf{y}_0\\), we note that \\(\\mathbf{z}(0) = V^{-1}\\,\\mathbf{y}_0\\). Hence, transforming back, we get:\n\\[\n\\mathbf{y}(t) = V\\, \\mathbf{z}(t)\n  = V\n  \\begin{pmatrix}\n    z_1(0) \\, e^{\\lambda_1 t}\\\\\n    z_2(0) \\, e^{\\lambda_2 t}\\\\\n    \\vdots\\\\\n    z_n(0) \\, e^{\\lambda_n t}\n  \\end{pmatrix}\n  = V \\, e^{D t} \\, V^{-1} \\, \\mathbf{y}_0.\n\\]\nTherefore, we obtain the compact form:\n\\[\n\\mathbf{y}(t) = e^{A t}\\,\\mathbf{y}_0,\n\\]\nor, equivalently,\n\\[\n\\mathbf{y}(t) = V\n  \\begin{pmatrix}\n  e^{\\lambda_1} & & \\\\\n  & \\ddots & \\\\\n  & & e^{\\lambda_n}\n  \\end{pmatrix}\n  V^{-1}\n  \\mathbf{y}_0 \\, .\n\\]\nIn the case of \\(A\\) Hermitian, the time evolution can be expanded as\n\\[\n\\mathbf{y}(t) = \\sum_i (\\mathbf{v}_i^\\dagger \\cdot \\mathbf{y}_0) \\mathbf{v}_i \\, e^{\\lambda_i t} \\, ,\n\\]\nwhere \\(\\mathbf{v}_i\\) are the eigenvectors of the matrix.\n\n\n5.2.2 Relation to the Schrödinger Equation\nWhen dealing with the time-dependent Schrödinger equation for a time-independent Hamiltonian \\(\\hat{H}\\), we can represent \\(\\ket{\\psi(t)}\\) in a certain basis, turning the Schrödinger equation into:\n\\[\ni \\hbar \\,\\frac{d}{dt} \\mathbf{c}(t) = H \\,\\mathbf{c}(t),\n\\]\nor equivalently,\n\\[\n\\frac{d\\mathbf{c}(t)}{dt} = -\\frac{i}{\\hbar} \\, H \\,\\mathbf{c}(t).\n\\]\nWe can identify \\(A = -\\frac{i}{\\hbar}\\, H\\). If \\(H\\) is diagonalizable (e.g., Hermitian matrices always have a complete set of orthonormal eigenvectors), then the above solution technique via diagonalization applies. The resulting exponential solution corresponds to the usual \\(e^{-\\frac{i}{\\hbar} H t}\\) operator that defines unitary time evolution in quantum mechanics.\n\nExample: Harmonic Oscillator\nThe harmonic oscillator is described by the second-order ODE:\n\\[\n\\frac{d^2 x}{dt^2} + \\omega^2 x = 0,\n\\]\nwhich can be rewritten as a first-order system:\n\\[\n\\begin{cases}\n\\frac{dx}{dt} = v, \\\\\n\\frac{dv}{dt} = -\\omega^2 x.\n\\end{cases}\n\\]\nor, in matrix form:\n\\[\n\\frac{d}{dt} \\begin{pmatrix} x \\\\ v \\end{pmatrix} = \\begin{pmatrix} 0 & 1 \\\\ -\\omega^2 & 0 \\end{pmatrix} \\begin{pmatrix} x \\\\ v \\end{pmatrix}.\n\\]\nBy diagonalizing the matrix, we can find the solution to this system.\n\n\nimport numpy as np\n\nomega = 2.0\n\n# Define the 2x2 matrix A\nA = np.array([[0.0,  1.0],\n              [-omega**2, 0.0]])\n\n# Initial condition: x(0) = 1, v(0) = 0.5\nx0 = np.array([1.0, 0.5])\n\n# Diagonalize A\neigs, V = np.linalg.eig(A)\nV_inv = np.linalg.inv(V)\n\nz0 = V_inv @ x0\n\n# Define a time array\nt_points = np.linspace(0, 2, 200)\n\nX_t = []\nfor t in t_points:\n    # Compute the solution at time t\n    z_t = np.diag(np.exp(eigs * t)) @ z0\n    x_t = V @ z_t # Transform back to original coordinates\n    X_t.append(x_t)\n\nX_t = np.array(X_t).real\n\nprint(\"x(2) = \", X_t[-1])\n\nx(2) =  [-0.84284424  1.18678318]\n\n\n\nWe have: - A: the system matrix. - y0: initial condition \\(\\mathbf{y}(0)\\). - We diagonalize \\(A\\) to find \\(A = V D V^{-1}\\). - Then \\(\\exp(A t) = V \\exp(D t) V^{-1}\\).\nIf you run the code, you’ll see the final value of \\(\\mathbf{y}(2)\\).\nWe could also visualize the time evolution:\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nax.plot(t_points, X_t[:, 0], label=\"$x(t)$\")\nax.plot(t_points, X_t[:, 1], label=\"$v(t)$\")\nax.plot(t_points, 0.5 * omega**2 * X_t[:, 0]**2 + 0.5 * X_t[:, 1]**2,\n        label=\"$E(t)$\")\nax.set_xlabel(\"Time\")\nax.set_ylabel(\"$y(t)$\")\nax.legend()\n\n# Show in Quarto\nplt.savefig('_tmp_fig.svg')\nplt.close(fig)\nSVG(filename='_tmp_fig.svg')",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Ordinary Differential Equations</span>"
    ]
  },
  {
    "objectID": "lecture1/ordinary_differential_equations.html#sec-ode-euler-method",
    "href": "lecture1/ordinary_differential_equations.html#sec-ode-euler-method",
    "title": "5  Ordinary Differential Equations",
    "section": "5.3 Numerical Solution via the Euler Method",
    "text": "5.3 Numerical Solution via the Euler Method\nIn many realistic situations (e.g., time-dependent Hamiltonians, nonlinear effects, large dissipative systems described by master equations), finding an exact analytic solution can be very challenging or impossible. We then rely on numerical methods to solve ODEs.\n\n5.3.1 Forward Euler Method\nOne of the simplest methods is the forward Euler method. Suppose we want to solve: \\[\n\\frac{d\\mathbf{y}(t)}{dt} = \\mathbf{f}(\\mathbf{y}(t), t),\n  \\quad\n  \\mathbf{y}(0) = \\mathbf{y}_0.\n\\]\nWe discretize time into steps \\(t_n = n\\,h\\) with step size \\(h\\). The Euler method approximates the derivative at \\(t_n\\) by a difference quotient:\n\\[\n\\frac{d\\mathbf{y}(t_n)}{dt} \\approx \\frac{\\mathbf{y}_{n+1} - \\mathbf{y}_n}{h}.\n\\]\nHence, the system becomes the algebraic update:\n\\[\n\\mathbf{y}_{n+1} = \\mathbf{y}_n + h\\,\\mathbf{f}(\\mathbf{y}_n, t_n),\n\\]\nwith \\(\\mathbf{y}_0\\) known. After iterating this rule for \\(n=0,1,2,\\dots\\), we obtain an approximate solution at discrete times \\(t_n\\).\n\n\n5.3.2 Stability Criterion for the Euler Method\nWhile the Euler method is straightforward, it can be susceptible to numerical instability when the system has rapidly decaying or oscillatory modes. For example, consider the test equation \\(\\frac{dy}{dt} = \\lambda\\, y,\\) where \\(\\lambda\\) is a (possibly complex) constant. The exact solution is \\(y(t) = y(0)\\,e^{\\lambda t}\\). In the Euler scheme, we get\n\\[\ny_{n+1} = y_n + h\\,\\lambda\\,y_n = (1 + h\\,\\lambda)\\, y_n.\n\\]\nThus,\n\\[\ny_n = (1 + h\\,\\lambda)^n\\, y_0.\n\\]\nFor the method to be stable (i.e., for \\(y_n\\) to remain bounded in the limit \\(n\\to\\infty\\) when the exact solution is stable), we require: \\[|1 + h\\,\\lambda| &lt; 1,\\] when the real part of \\(\\lambda\\) is negative (dissipative system). If this condition is not met, the numerical solution may diverge even though the true solution decays exponentially. In practice, one must choose the time step \\(h\\) small enough to satisfy such stability constraints.\n\nExample: Harmonic Oscillator with Euler Method\nLet’s now implement the forward Euler method for a simpler ODE. Consider the same harmonic oscillator, Euler’s method approximates the evolution as:\n\\[\n\\begin{pmatrix}\n  x_{n+1} \\\\ v_{n+1}\n\\end{pmatrix} \\simeq\n\\begin{pmatrix}\n  x_n \\\\ v_n\n\\end{pmatrix} + h \\,\n\\begin{pmatrix}\n  0 & 1 \\\\ -\\omega^2 & 0\n\\end{pmatrix} \\,\n\\begin{pmatrix}\n  x_n \\\\ v_n\n\\end{pmatrix},\n\\]\nwhere \\(h\\) is the time step.\n\n\nh = 0.01\n\nX_t_euler = np.zeros((len(t_points), 2))\nX_t_euler[0] = x0\nfor n in range(len(t_points) - 1):\n    X_t_euler[n+1] = X_t_euler[n] + h * A @ X_t_euler[n]\n\nfig, ax = plt.subplots()\nax.plot(t_points, X_t[:, 0], label=\"$x(t)$ (exact)\")\nax.plot(t_points, X_t_euler[:, 0], label=\"$x(t)$ (Euler)\", linestyle='--')\nax.plot(t_points, X_t[:, 1], label=\"$v(t)$ (exact)\")\nax.plot(t_points, X_t_euler[:, 1], label=\"$v(t)$ (Euler)\", linestyle='--')\nax.set_xlabel(\"Time\")\nax.set_ylabel(\"$y(t)$\")\nax.legend()\n\n# Show in Quarto\nplt.savefig('_tmp_fig.svg')\nplt.close(fig)\nSVG(filename='_tmp_fig.svg')\n\n\n\n\n\n\n\n\n\nHere we see how the Euler solution compares to the exact solution obtained via diagonalization. Notice that using a large time step \\(h\\) can cause the Euler solution to deviate significantly from the exact decay (and may even diverge if \\(|1 - \\lambda h| \\ge 1\\)).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Ordinary Differential Equations</span>"
    ]
  },
  {
    "objectID": "lecture1/ordinary_differential_equations.html#applying-these-methods-to-the-schrödinger-equation",
    "href": "lecture1/ordinary_differential_equations.html#applying-these-methods-to-the-schrödinger-equation",
    "title": "5  Ordinary Differential Equations",
    "section": "5.4 Applying These Methods to the Schrödinger Equation",
    "text": "5.4 Applying These Methods to the Schrödinger Equation\n\nTime-Independent Hamiltonian\nFor a time-independent Hamiltonian \\(\\hat{H}\\), the Schrödinger equation in vector form reads:\n\\[\ni\\hbar \\,\\frac{d\\mathbf{c}(t)}{dt} = H\\,\\mathbf{c}(t).\n\\]\nBy setting \\(A = -\\tfrac{i}{\\hbar} H\\), we recognize that this is a linear ODE. If \\(H\\) (or \\(A\\)) is diagonalizable, its eigen-decomposition yields an analytic solution. In quantum optics, these solutions describe unitary time evolution of a closed system, often expressed as:\n\\[\n\\mathbf{c}(t) = e^{-\\tfrac{i}{\\hbar}H t} \\,\\mathbf{c}(0).\n\\]\n\n\n5.4.1 Time-Dependent Hamiltonian\nWhen \\(\\hat{H}(t)\\) varies explicitly with time, one no longer has a simple exponential solution. Instead, one can divide the time interval of interest into many small sub-intervals and approximate \\(\\hat{H}(t)\\) as constant in each interval. This procedure is related to the time-ordered exponential, but from a numerical perspective, we can simply implement a step-by-step integration (e.g., Euler, Runge–Kutta, or other higher-order methods) to construct \\(\\ket{\\psi(t_{n+1})}\\) from \\(\\ket{\\psi(t_n)}\\).\n\n\n5.4.2 Open Quantum Systems\nIn open quantum systems, the evolution of the density matrix \\(\\rho(t)\\) is often governed by the master equation:\n\\[\n\\frac{d\\rho(t)}{dt} = \\mathcal{L}[\\rho(t)],\n\\] where \\(\\mathcal{L}\\) is the so-called Liouvillian superoperator, which could contain both Hamiltonian (coherent) parts and dissipative terms. Numerically, one can vectorize \\(\\rho(t)\\) (flattening the matrix into a vector) and represent \\(\\mathcal{L}\\) as a matrix \\(\\mathcal{L}_{\\mathrm{mat}}\\). Then, the equation again has the familiar linear form:\n\\[\n\\frac{d\\mathbf{r}(t)}{dt} = \\mathcal{L}_{\\mathrm{mat}}\\;\\mathbf{r}(t).\n\\]\nHence, the same techniques (matrix diagonalization for analytical solutions, or time stepping methods like Euler, Runge–Kutta, etc. for numerical solutions) remain valid.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Ordinary Differential Equations</span>"
    ]
  },
  {
    "objectID": "lecture1/ordinary_differential_equations.html#conclusion",
    "href": "lecture1/ordinary_differential_equations.html#conclusion",
    "title": "5  Ordinary Differential Equations",
    "section": "5.5 Conclusion",
    "text": "5.5 Conclusion\nIn summary:\n\nAn Ordinary Differential Equation (ODE) involves a function of one variable and its derivatives.\nWhen an ODE is linear and time-independent, one can analytically solve it by diagonalizing the system matrix.\nFor more complicated (time-dependent or nonlinear) problems, numerical integration methods such as the Euler method can be applied.\nThe Euler method is conceptually simple but demands careful choice of time step to ensure stability, particularly when the system matrix has eigenvalues with large negative real parts or when fast decaying/oscillatory modes are present.\nThese ideas are directly applicable to quantum mechanical systems such as the Schrödinger equation or master equations for open systems. In the Schrödinger equation, diagonalization corresponds to finding energy eigenstates and frequencies, while in open quantum systems, vectorization plus diagonalization or numerical iteration handles both coherent and dissipative dynamics.\n\nThroughout the course, we will leverage these fundamental methods—both analytical techniques (e.g., diagonalization) and numerical approaches (e.g., Euler and more sophisticated solvers)—to simulate quantum systems efficiently and accurately.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Ordinary Differential Equations</span>"
    ]
  },
  {
    "objectID": "lecture1/phase_space_ode.html",
    "href": "lecture1/phase_space_ode.html",
    "title": "6  From Hamilton’s equations to the Liouville equation in phase space",
    "section": "",
    "text": "6.1 From Hamilton’s equations to Liouville’s continuity law\nIn the previous chapter we showed that many linear ordinary differential equations (ODEs) that appear in quantum mechanics can be solved elegantly by writing them in matrix form and diagonalising the matrix. In classical mechanics, however, the equations of motion\n\\[\n\\begin{cases}\n\\dot x \\,=\\, \\dfrac{p}{m},\\\\\n\\dot p \\,=\\, -\\dfrac{\\partial V (x)}{\\partial x}\n\\end{cases}\n\\tag{6.1}\\]\nbecome non‑linear as soon as the potential \\(V(x)\\) is non‑quadratic. Consequently the state vector \\(\\mathbf{y} = (x,p)^{\\mathsf T}\\) no longer satisfies a linear system \\(\\dot{\\mathbf{y}} = A\\,\\mathbf{y}\\). As an example, we will consider the Duffing-like oscillator with a quartic potential \\(V(x) = \\frac{1}{2} k x^2 + g x^4\\).\nIn Hamiltonian mechanics we usually track a single phase‑space point \\((x(t),p(t))\\) by solving the Hamilton equations in Equation 6.1. Yet many physical questions are statistical:\nAnswering these requires an equation for a phase‑space density \\(\\rho(x(t),p(t), t)\\), not individual trajectories. The Liouville equation supplies precisely that.\nThe Liouville equation describes how a classical probability density function in phase space evolves over time. It is a fundamental result in classical statistical mechanics and emerges directly from Hamilton’s equations.\nWe aim to derive: \\[\n\\frac{\\partial \\rho}{\\partial t} + \\{ \\rho, H \\} = 0\n\\]\nwhere:\nWe begin with the canonical equations of motion for a 1D system: \\[\n\\begin{cases}\n\\dot{x} = \\frac{\\partial H}{\\partial p}\\\\\n\\dot{p} = -\\frac{\\partial H}{\\partial x}\n\\end{cases}\n\\]\nThese equations describe the deterministic evolution of a point \\((x(t), p(t))\\) in phase space.\nLet \\(\\rho(x, p, t)\\) be the density of an ensemble of classical systems in phase space. To study how this density evolves along the flow of the system, we compute the total derivative: \\[\n\\frac{d}{dt} \\rho(x(t), p(t), t) =\n\\frac{\\partial \\rho}{\\partial t}\n+ \\frac{\\partial \\rho}{\\partial x} \\frac{dx}{dt}\n+ \\frac{\\partial \\rho}{\\partial p} \\frac{dp}{dt}\n\\]\nSubstituting Hamilton’s equations: \\[\n\\frac{d\\rho}{dt} =\n\\frac{\\partial \\rho}{\\partial t}\n+ \\frac{\\partial \\rho}{\\partial x} \\frac{\\partial H}{\\partial p}\n- \\frac{\\partial \\rho}{\\partial p} \\frac{\\partial H}{\\partial x}\n= \\frac{\\partial \\rho}{\\partial t} + \\{ \\rho, H \\}\n\\]\nIn Hamiltonian mechanics, the phase space flow is incompressible: it preserves the volume element \\(dx \\wedge dp\\). This implies that the density \\(\\rho\\) remains constant along each trajectory:\n\\[\n\\frac{d}{dt} \\rho(x(t), p(t), t) = 0\n\\]\nHence, we obtain: \\[\n\\frac{\\partial \\rho}{\\partial t} + \\{ \\rho, H \\} = 0 \\qquad \\text{or} \\qquad \\frac{\\partial \\rho}{\\partial t} = \\{H, \\rho\\}\n\\]\nThis is the Liouville equation.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>From Hamilton's equations to the Liouville equation in phase space</span>"
    ]
  },
  {
    "objectID": "lecture1/phase_space_ode.html#from-hamiltons-equations-to-liouvilles-continuity-law",
    "href": "lecture1/phase_space_ode.html#from-hamiltons-equations-to-liouvilles-continuity-law",
    "title": "6  From Hamilton’s equations to the Liouville equation in phase space",
    "section": "",
    "text": "Given ignorance about the exact initial state, how does a whole ensemble of points evolve?\nWhich quantities remain constant under the flow, and why?\n\n\n\n\n\n\n\\(\\rho(x, p, t)\\) is the probability density in phase space,\n\\(H(x, p)\\) is the Hamiltonian of the system,\n\\(\\{f, g\\} = \\frac{\\partial f}{\\partial x} \\frac{\\partial g}{\\partial p} - \\frac{\\partial f}{\\partial p} \\frac{\\partial g}{\\partial x}\\) denotes the Poisson bracket.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>From Hamilton's equations to the Liouville equation in phase space</span>"
    ]
  },
  {
    "objectID": "lecture1/phase_space_ode.html#physical-interpretation",
    "href": "lecture1/phase_space_ode.html#physical-interpretation",
    "title": "6  From Hamilton’s equations to the Liouville equation in phase space",
    "section": "6.2 Physical Interpretation",
    "text": "6.2 Physical Interpretation\n\nThe equation describes how a probability distribution in phase space flows under Hamiltonian evolution.\nThe term \\(\\{ \\rho, H \\}\\) encodes the flow of the distribution due to the system’s dynamics.\nThe total number of systems is conserved, and the phase-space density is transported without compression.\n\nIn short: Liouville’s theorem states that the probability density is constant along the trajectories of the system in phase space.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>From Hamilton's equations to the Liouville equation in phase space</span>"
    ]
  },
  {
    "objectID": "lecture1/phase_space_ode.html#discretizing-the-liouville-operator-with-finite-differences",
    "href": "lecture1/phase_space_ode.html#discretizing-the-liouville-operator-with-finite-differences",
    "title": "6  From Hamilton’s equations to the Liouville equation in phase space",
    "section": "6.3 Discretizing the Liouville Operator with Finite Differences",
    "text": "6.3 Discretizing the Liouville Operator with Finite Differences\nIn classical statistical mechanics, the Liouville equation governs the time evolution of a probability density in phase space. To simulate this numerically, we can discretize phase space and rewrite the Liouville operator as a sparse matrix, using finite difference approximations for derivatives.\nFor a 1D system with Hamiltonian \\(H(x, p) = \\frac{p^2}{2m} + V(x)\\), we compute:\n\\[\n\\{ H, \\rho \\} = \\frac{\\partial H}{\\partial x} \\frac{\\partial \\rho}{\\partial p} - \\frac{\\partial H}{\\partial p} \\frac{\\partial \\rho}{\\partial x} = \\frac{\\partial V}{\\partial x} \\frac{\\partial \\rho}{\\partial p} - \\frac{p}{m} \\frac{\\partial \\rho}{\\partial x}\n\\]\n\n6.3.1 Discretizing Phase Space\nWe define a uniform grid of \\(N_x\\) points over \\(x\\) and \\(N_p\\) points over \\(p\\):\n\n\\(x_i = x_0 + i \\cdot \\Delta x\\), for \\(i = 0, \\dots, N_x - 1\\)\n\\(p_j = p_0 + j \\cdot \\Delta p\\), for \\(j = 0, \\dots, N_p - 1\\)\n\nThe phase space density \\(\\rho(x_i, p_j)\\) is stored as a 2D array or flattened into a vector \\(\\vec{\\rho} \\in \\mathbb{R}^{N_x N_p}\\).\nWe now define central difference matrices for the derivatives. Using second-order central differences: \\[\n\\left. \\frac{\\partial \\rho}{\\partial x} \\right|_{x_i} \\approx\n\\frac{\\rho(x_{i+1}) - \\rho(x_{i-1})}{2\\Delta x}\n\\]\nThis corresponds to a matrix \\(D_x \\in \\mathbb{R}^{N_x \\times N_x}\\) with the stencil: \\[\nD_x = \\frac{1}{2 \\Delta x}\n\\begin{pmatrix}\n0 & 1 & 0 & \\cdots & 0 \\\\\n-1 & 0 & 1 & \\cdots & 0 \\\\\n0 & -1 & 0 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\ddots & 1 \\\\\n0 & 0 & \\cdots & -1 & 0 \\\\\n\\end{pmatrix}\n\\tag{6.2}\\]\nAnalogously: \\[\nD_p = \\frac{1}{2 \\Delta p}\n\\begin{pmatrix}\n0 & 1 & 0 & \\cdots & 0 \\\\\n-1 & 0 & 1 & \\cdots & 0 \\\\\n0 & -1 & 0 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\ddots & 1 \\\\\n0 & 0 & \\cdots & -1 & 0 \\\\\n\\end{pmatrix}\n\\tag{6.3}\\]\nBoth matrices are sparse, antisymmetric, and can be constructed with sparse matrix tools of scipy.sparse.\n\n\n6.3.2 Building the Liouville Matrix Operator\nOnce we flatten the 2D array \\(\\rho(x_i, p_j)\\) into a vector \\(\\vec{\\rho} \\in \\mathbb{R}^{N_x N_p}\\), we define:\n\n\\(P = \\text{diag}(p_j / m)\\) of shape \\(N_p \\times N_p\\)\n\\(\\partial_x V = \\text{diag}(\\partial_x V(x_i))\\) of shape \\(N_x \\times N_x\\)\n\nThen the full Liouville matrix \\(L\\) becomes:\n\\[\nL = (I_x \\otimes D_p) \\cdot (\\partial_x V \\otimes I_p) - (D_x \\otimes I_p) \\cdot (I_x \\otimes P)\n\\tag{6.4}\\]\nHere:\n\n\\(I_x\\), \\(I_p\\): identity matrices on position and momentum spaces\n\\(\\otimes\\): Kronecker product\n\nThis is a sparse matrix acting on \\(\\vec{\\rho}\\), and encodes the total effect of the classical flow in phase space.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>From Hamilton's equations to the Liouville equation in phase space</span>"
    ]
  },
  {
    "objectID": "lecture1/phase_space_ode.html#time-evolution",
    "href": "lecture1/phase_space_ode.html#time-evolution",
    "title": "6  From Hamilton’s equations to the Liouville equation in phase space",
    "section": "6.4 Time Evolution",
    "text": "6.4 Time Evolution\nWe can evolve the discretized density using an ODE solver: \\[\n\\frac{d \\vec{\\rho}}{dt} = L \\vec{\\rho}\n\\tag{6.5}\\]\nThus, we have reduced the problem to a linear ordinary differential equation (ODE) system, which can be solved using the standard tools discussed in Chapter 5.\nBefore concluding this section, let us summarize some important points:\n\nThe Liouville operator can be expressed as a sparse matrix using finite differences.\nPosition and momentum derivatives are replaced by central difference matrices.\nThe discretized Liouville equation is a linear ODE system for the phase-space density vector.\nThe phase space grid must be fine enough to resolve the flow.\nBoundary conditions (periodic, reflecting, absorbing) must be chosen according to the physics.\nThis approach is analogous to how quantum Hamiltonians are discretized into matrices using finite differences.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>From Hamilton's equations to the Liouville equation in phase space</span>"
    ]
  },
  {
    "objectID": "lecture1/phase_space_ode.html#running-example-a-quartic-non-linear-oscillator",
    "href": "lecture1/phase_space_ode.html#running-example-a-quartic-non-linear-oscillator",
    "title": "6  From Hamilton’s equations to the Liouville equation in phase space",
    "section": "6.5 Running example: a quartic non-linear oscillator",
    "text": "6.5 Running example: a quartic non-linear oscillator\nLet us keep the algebra to a minimum and pick the potential\n\\[\nV(x)=\\tfrac 12 k x^{2}+g x^{4},\n\\]\nwith \\(k&gt;0\\) (harmonic part) and \\(g&gt;0\\) (hardening quartic term). Its Hamiltonian reads\n\\[\nH(x,p)=\\frac{p^{2}}{2m}+V(x).\n\\]\nAlthough the equations of motion are non‑linear, we could still integrate them numerically using scipy.integrate.solve_ivp. However, this goes out of the scope of this course, as we are interested in linear ordinary differential equations and their matrix equivalents. To restore linearity we have to take a step back and study phase‑space functions rather than individual trajectories.\nWe can now construct the matrix operators for the Liouville equation following Equation 6.2, Equation 6.3, and Equation 6.4. We can take advantage of the tools provided by scipy.sparse to create the sparse matrices efficiently. We start by importing the necessary libraries\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy as sp\nimport scipy.sparse as sparse\nimport functools as ft\n\n# Define a Gaussian function, useful for initial conditions\ndef gaussian(x, mu, sigma):\n    \"\"\"Generate a Gaussian function.\"\"\"\n    norm_factor = 1 / (sigma * np.sqrt(2 * np.pi))\n    return np.exp(-0.5 * ((x - mu) / sigma) ** 2) * norm_factor\n\n\nAnd then we define the grid and the operators for the phase space:\n\n\nN_x = 101 # Number of grid points in position space\nN_px = 101 # Number of grid points in momentum space\nx_bound = 2 # Position space boundary\npx_bound = 2 # Momentum space boundary\n\n# Identity matrices for the different dimensions\nIx = sparse.eye(N_x)\nIpx = sparse.eye(N_px)\n\nx_list = np.linspace(-x_bound, x_bound, N_x)\npx_list = np.linspace(-px_bound, px_bound, N_px)\n\ndx = x_list[1] - x_list[0]\ndpx = px_list[1] - px_list[0]\n\n# Define the operators\nx_op = sparse.diags(x_list)\npx_op = sparse.diags(px_list)\n\n# Use central differences for derivatives\nd_x_op = sparse.diags([np.ones(N_x-1)/(2*dx), \n                -np.ones(N_x-1)/(2*dx)], offsets=[1, -1])\nd_px_op = sparse.diags([np.ones(N_px-1)/(2*dpx),\n                -np.ones(N_px-1)/(2*dpx)], offsets=[1, -1])\n\n# Create the full operator for the 4D phase space\nx = ft.reduce(sparse.kron, [x_op, Ipx]).todia()\npx = ft.reduce(sparse.kron, [Ix, px_op]).todia()\n\nd_x = ft.reduce(sparse.kron, [d_x_op, Ipx]).todia()\nd_px = ft.reduce(sparse.kron, [Ix, d_px_op]).todia()\n\n\nWe can now compute the time evolution defined by Equation 6.5 by using the Euler method described in Section 5.3:\n\n\nm = 0.5 # Mass of the particle\nk = 2.0 # Spring constant\nG = 0.3 # Nonlinear constant\n\ndV_dx = k * x + 4 * G * x @ x @ x\n\n# Liouville operator\nL = dV_dx @ d_px - (px / m) @ d_x\n\nt_list = np.linspace(0, 2, 50000)\n\n# Initial state: we will use a Gaussian wave packet to avoid singularities\nx_0 = np.sqrt(gaussian(x_list, 1.0, 0.2)) * np.sqrt(dx)\np_0 = np.sqrt(gaussian(px_list, 0.1, 0.2)) * np.sqrt(dpx)\nrho_0 = np.kron(x_0, p_0)\n\nx_t = np.zeros(t_list.shape[0])\nv_t = np.zeros(t_list.shape[0])\nE_t = np.zeros(t_list.shape[0])\n\nx_t[0] = rho_0.dot(x.dot(rho_0))\nv_t[0] = rho_0.dot(px.dot(rho_0)) / m\nE_t[0] = (0.5 * m * v_t[0]**2 + 0.5 * k * x_t[0]**2 + G * x_t[0]**4)\n\nrho_t = [rho_0.copy()]\nfor i, t in enumerate(t_list[1:], 1):\n    drho_dt = L @ rho_t[-1]\n\n    # Simple Euler integration\n    rho_t.append(rho_t[-1] + drho_dt * (t_list[1] - t_list[0]))\n\n    x_t[i] = rho_t[-1].dot(x.dot(rho_t[-1]))\n    v_t[i] = rho_t[-1].dot(px.dot(rho_t[-1])) / m\n    E_t[i] = (0.5 * m * v_t[i]**2 + 0.5 * k * x_t[i]**2 + G * x_t[i]**4)\n\n\nAnd we can finally visualize the evolution of the phase space density \\(\\rho(x, p, t)\\) as a 2D plot animation:\n\n\nfrom IPython.display import HTML\nfrom matplotlib.animation import FuncAnimation\n\nplt.rcParams.update({'font.size': 8})\nfig, ax = plt.subplots(figsize=(4.6, 2.8))\n\nfig.suptitle(r'Phase space density $\\rho(x, p, t)$ evolution')\n\nimg = ax.pcolormesh(x_list, px_list, rho_t[0].reshape(N_x, N_px)**2,\n                    shading='gouraud', rasterized=True)\nax.set_xlabel(\"Position $x$\")\nax.set_ylabel(\"Momentum $p_x$\")\n\ndef animate(i):\n    img.set_array(rho_t[i]**2)\n    return img,\n\nani = FuncAnimation(fig, animate, frames=range(0, len(t_list), 500), interval=50, blit=True)\n\nplt.close(fig)\nHTML(ani.to_jshtml())\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>From Hamilton's equations to the Liouville equation in phase space</span>"
    ]
  },
  {
    "objectID": "lecture1/quantum_objects_numpy.html",
    "href": "lecture1/quantum_objects_numpy.html",
    "title": "7  Representing Quantum States and Operators with NumPy",
    "section": "",
    "text": "7.1 Pauli Operators\nIn quantum mechanics, states and observables are represented using the algebra of Hilbert spaces. However, their infinite dimensions is incompatible with numerical simulations, that always requires finite elements. Hence, we truncate Hilbert spaces to a finite size, allowing us to run the quantum calculation on a computer. We can thus say that the whole problem of numerical quantum mechanics is then reduced to a problem of linear algebra. However, the intricated tensor structures of many-body Hilbert spaces requires also a powerful organization of the code and an easy way to access relevant information.\nIn the following we consider a system with a Hilbert space of dimension \\(d\\). The set of basis states \\(\\{|k\\rangle: k=1, \\ldots, d\\}\\) form an orthonormal basis, i.e., \\(\\left\\langle k \\mid k^{\\prime}\\right\\rangle=\\delta_{k, k^{\\prime}}\\). In general there are systems with an infinite dimensional Hilbert space, or systems, where the dimension is too large to be tractable on a computer. In this case \\(d\\) denotes the number of truncated basis states, which is used in the numerical simulation. For a given choice of basis states we can express any state vector and any operator as\n\\[\n|\\psi\\rangle = \\sum_{k=1}^{d} c_{k} |k\\rangle, \\quad \\hat{A}=\\sum_{k, l} A_{k l}|k\\rangle\\langle l|,\n\\]\nwhere \\(c_{k} = \\langle k \\mid \\psi\\rangle\\) and \\(A_{k l}=\\langle k| \\hat{A}|l\\rangle\\). Therefore, in numerical simulations we represent states by vectors and operators by matrices according to the mapping\n\\[\n|\\psi\\rangle \\mapsto \\vec{\\psi}=\\left(\\begin{array}{c}\nc_{1} \\\\\nc_{2} \\\\\n\\vdots \\\\\nc_{d}\n\\end{array}\\right), \\quad \\hat{A} \\mapsto A=\\left(\\begin{array}{cccc}\nA_{11} & A_{12} & \\ldots & A_{1 d} \\\\\nA_{21} & A_{22} & \\ldots & A_{2 d} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nA_{d 1} & A_{d 2} & \\ldots & A_{d d}\n\\end{array}\\right) .\n\\]\nThe left and right operations of an operator on a vector then simply translate into matrix vector multiplications,\n\\[\n\\hat{A}|\\psi\\rangle \\mapsto \\texttt{np.dot(A, psi)}, \\quad \\langle \\psi| \\hat{A} \\mapsto \\texttt{np.dot(np.conj(psi.T), A)},\n\\]\nwhere in Numpy np.conj(psi.T) is the hermitian transpose of a matrix or vector.\nThe Pauli operators are fundamental in quantum mechanics, especially in the context of qubits. They are represented as matrices in a two-dimensional Hilbert space, which is the simplest non-trivial quantum system.\n\\[\n\\hat{\\sigma}_{x} \\mapsto\\left(\\begin{array}{cc}\n0 & 1  \\\\\n1 & 0\n\\end{array}\\right), \\quad \\hat{\\sigma}_{y} \\mapsto\\left(\\begin{array}{cc}\n0 & i \\\\\n-i & 0\n\\end{array}\\right), \\quad \\hat{\\sigma}_{z} \\mapsto\\left(\\begin{array}{cc}\n1 & 0 \\\\\n0 & -1\n\\end{array}\\right) .\n\\]\nIn Numpy we simply define the corresponding matrices",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Representing Quantum States and Operators with NumPy</span>"
    ]
  },
  {
    "objectID": "lecture1/quantum_objects_numpy.html#pauli-operators",
    "href": "lecture1/quantum_objects_numpy.html#pauli-operators",
    "title": "7  Representing Quantum States and Operators with NumPy",
    "section": "",
    "text": "import numpy as np\n\nsx = np.array([[0, 1], [1, 0]])\nsy = np.array([[0, 1j], [-1j, 0]])\nsz = np.array([[1, 0], [0, -1]])",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Representing Quantum States and Operators with NumPy</span>"
    ]
  },
  {
    "objectID": "lecture1/quantum_objects_numpy.html#harmonic-oscillator",
    "href": "lecture1/quantum_objects_numpy.html#harmonic-oscillator",
    "title": "7  Representing Quantum States and Operators with NumPy",
    "section": "7.2 Harmonic Oscillator",
    "text": "7.2 Harmonic Oscillator\nIn a Hilbert space of dimension \\(N\\), quantum states can be represented as vectors, and operators as matrices. Here we demonstrate the destroy operator, \\(a\\), which lowers the state by one quantum number. For a detailed discussion on the quantum harmonic oscillator and the bosonic annihilation operator, refer to Appendix A.\nFor a harmonic oscillator with number states \\(|n\\rangle\\) the only nonzero matrix elements of the annihilation operator \\(\\hat{a}\\) are given by \\(\\langle n-1| \\hat{a}|n\\rangle=\\sqrt{n}\\)\n\\[\n\\hat{a} \\mapsto A=\\left(\\begin{array}{cccccc}\n0 & 1 & \\ldots & \\cdots & \\ldots & 0  \\\\\n0 & 0 & \\sqrt{2} & \\ldots & \\cdots & 0 \\\\\n0 & 0 & 0 & \\sqrt{3} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\ddots & \\vdots \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & 0 & \\sqrt{d-1} \\\\\n0 & 0 & 0 & \\cdots & \\cdots & 0\n\\end{array}\\right)\n\\]\nThis operator acts on Fock states to lower their quantum number by one, with a factor of \\(\\sqrt{n}\\), where \\(n\\) is the quantum number of the initial state. In other words, \\(\\hat{a}|n\\rangle = \\sqrt{n}|n-1\\rangle\\). In the following code, we define the destroy operator by using NumPy, and we also define some Fock states for demonstration.\nIn Numpy we use the command np.diag(v, k=r), which creates a diagonal matrix with the elements of the vector v placed in the \\(r\\)-th diagonal \\((r=0, \\pm 1, \\pm 2, \\ldots)\\).\n\n\ndef destroy(d):\n    # creates a vector of the d-1 off-diagonal elements\n    v=np.sqrt( np.arange(d-1) )\n    # matrix with the elements of vec placed in the upper diagonal\n    a=np.diag(v,k=1)\n    return a\n\n# Define the fock states\ndef fock(d, i):\n    res = np.zeros(d)\n    res[i] = 1\n    return res\n\nd = 7\nzero_state = fock(d, 0)\none_state = fock(d, 1)\ntwo_state = fock(d, 2)\nthree_state = fock(d, 3)\n\ndestroy_operator = destroy(d)\ndestroy_operator\n\narray([[0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        ],\n       [0.        , 0.        , 1.        , 0.        , 0.        ,\n        0.        , 0.        ],\n       [0.        , 0.        , 0.        , 1.41421356, 0.        ,\n        0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 1.73205081,\n        0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        ,\n        2.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 2.23606798],\n       [0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.        ]])\n\n\n\nOther operators (e.g., \\(\\hat{a}^{\\dagger}, \\hat{a}^{\\dagger} \\hat{a}\\)) can be obtained by a hermitian transpose\n\\[\n\\hat{a}^{\\dagger} \\mapsto\\texttt{np.conj(a.T)}\n\\]\nand matrix multiplications\n\\[\n\\hat{a}^{\\dagger} \\hat{a} \\mapsto \\texttt{np.matmul(np.conj(a.T) , a)} \\, .\n\\]\nNote that in some cases this introduces truncation artifacts. For example, the matrix for the operator \\(M=\\texttt{np.matmul(a, np.conj(a.T))}\\) has a zero diagonal element \\(M[d, d]=0\\) inherited from the matrix \\(\\texttt{np.conj(a.T))}\\), while the same operator constructed in a different way, \\(M_2=\\texttt{np.conj(a.T) * a + np.eye(d)}\\), does not. This can be avoided by constructing this operator explicitly. Note that this type of truncation artifacts are related to the fact that in a infinite Hilbert space \\(\\mathrm{Tr}([a, a^{\\dagger}])\\neq 0\\) (actually, striclty speaking, \\(=\\infty\\)) as a consequence of the canonical commutation relation. On the contrary, in a finite Hilbert space for any two operators \\(O_1, O_2\\), \\(\\mathrm{Tr}([O_1, O_2])= 0\\). Taking a dimension \\(d\\) large enough allows to make these artifacts a negligible error in the whole computation.\n\n7.2.1 Action of the Destroy Operator on a Fock State\nThe action of the destroy operator \\(a\\) on a Fock state \\(|n\\rangle\\) lowers the state by one quantum number, multiplied by a factor \\(\\sqrt{n}\\). For example, applying \\(a\\) to the state \\(|3\\rangle\\) yields:\n\\[\n\\hat{a}|3\\rangle = \\sqrt{3}|2\\rangle\n\\]\nThis demonstrates the lowering action of the destroy operator with a specific factor, dependent on the quantum number of the state being acted upon.\n\n\n# Apply the destroy operator on the one state\nresult_state = np.dot(destroy_operator, three_state)\n\nprint(\"Resulting State:\")\nresult_state\n\nResulting State:\n\n\narray([0.        , 0.        , 1.41421356, 0.        , 0.        ,\n       0.        , 0.        ])",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Representing Quantum States and Operators with NumPy</span>"
    ]
  },
  {
    "objectID": "lecture1/quantum_objects_numpy.html#partial-trace",
    "href": "lecture1/quantum_objects_numpy.html#partial-trace",
    "title": "7  Representing Quantum States and Operators with NumPy",
    "section": "7.3 Partial Trace",
    "text": "7.3 Partial Trace\nIn Section 3.1.4, we have already discussed the concept of tensor products. Here we will introduce the partial trace, a crucial operation in quantum mechanics that allows us to focus on a subsystem of a larger composite system.\nThe partial trace over a subsystem, say \\(B\\), of a composite system \\(AB\\), mathematically expresses as “tracing out” \\(B\\), leaving the reduced state of \\(A\\). For a bipartite state \\(\\rho_{AB}\\), the partial trace over \\(B\\) is:\n\\[\n\\text{Tr}_B(\\hat{\\rho}_{AB}) = \\sum_{i \\in \\mathcal{H}_B} \\langle i| \\hat{\\rho}_{AB} |i\\rangle\n\\]\nwhere \\(\\{|i\\rangle\\}\\) forms a complete basis for subsystem \\(B\\).\nLet’s try it with an entangled Bell’s state between two qubits:\n\\[\n\\vert \\phi^+ \\rangle = \\frac{1}{\\sqrt{2}} \\left( \\vert 0, 0 \\rangle + \\vert 1, 1 \\rangle \\right)\n\\]\n\n\ndef ptrace(psi, subspace_to_keep, dim_subspace):\n    dim1, dim2 = dim_subspace\n\n    rho = np.outer(psi, psi.conj())\n\n    # Reshape rho to separate the subsystems' degrees of freedom\n    rho_reshaped = rho.reshape(dim1, dim2, dim1, dim2)\n\n    if subspace_to_keep == 1:\n        # Perform the trace over the second subsystem\n        traced_out = np.trace(rho_reshaped, axis1=1, axis2=3)\n    elif subspace_to_keep == 2:\n        # Perform the trace over the first subsystem\n        traced_out = np.trace(rho_reshaped, axis1=0, axis2=2)\n    else:\n        raise ValueError(\"subspace_to_keep must be either 1 or 2.\")\n\n    return traced_out\n\n# Bell state between two qubits\nphi_plus = ( np.kron(fock(2, 1), fock(2, 1)) + np.kron(fock(2, 0), fock(2, 0)) ) / np.sqrt(2)\n\n# Reduced density matrix of the first qubit\nrho_1 = ptrace(phi_plus, 1, (2, 2))\nrho_1\n\narray([[0.5, 0. ],\n       [0. , 0.5]])",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Representing Quantum States and Operators with NumPy</span>"
    ]
  },
  {
    "objectID": "lecture1/quantum_objects_numpy.html#why-qutip",
    "href": "lecture1/quantum_objects_numpy.html#why-qutip",
    "title": "7  Representing Quantum States and Operators with NumPy",
    "section": "7.4 Why QuTiP?",
    "text": "7.4 Why QuTiP?\nWhile NumPy and SciPy are powerful tools for numerical computations, they lack specific functionalities for efficiently handling complex quantum systems. QuTiP is designed to fill this gap, offering features such as:\n\nEasy manipulation and visualization of quantum objects.\nSupport for operations on states and operators in different Hilbert spaces.\nTools for dealing with composite systems, partial traces, and superoperators. It is like to have the book “Quantum noise” (by Gardiner and Zoller) already implemented in your laptop!\n\nIn the next chapters, we’ll explore how QuTiP simplifies these tasks, making it an invaluable tool for quantum optics simulations.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Representing Quantum States and Operators with NumPy</span>"
    ]
  },
  {
    "objectID": "lecture1/qutip_introduction.html",
    "href": "lecture1/qutip_introduction.html",
    "title": "8  Introduction to QuTiP",
    "section": "",
    "text": "8.1 Quantum Operators\nThe QuTiP package can be imported with\nIt can also be imported with the command from qutip import *, that automatically imports all the QuTiP functions. However, here we use the first method, in order to explicitly see the QuTiP functions.\nQuantum operators play a crucial role in the formulation of quantum mechanics, representing physical observables and operations on quantum states. In QuTiP, operators are represented as Qobj instances, just like quantum states. This section introduces the creation and manipulation of quantum operators.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to QuTiP</span>"
    ]
  },
  {
    "objectID": "lecture1/qutip_introduction.html#quantum-operators",
    "href": "lecture1/qutip_introduction.html#quantum-operators",
    "title": "8  Introduction to QuTiP",
    "section": "",
    "text": "8.1.1 Creating Operators\nOperators in quantum mechanics can represent measurements, such as position or momentum, and transformations, such as rotation. Let’s see how we can define some common operators in QuTiP.\n\nThe Annihilation Operator of the Quantum Harmonic oscillator\nThe harmonic oscillator is a fundamental model in quantum mechanics for understanding various physical systems. Its quantization leads to the concept of creation and annihilation operators, which respectively increase and decrease the energy of the system by one quantum of energy.\nThe annihilation operator, often denoted by \\(\\hat{a}\\), acts on a quantum state to reduce its quantum number. The action of \\(\\hat{a}\\) on a state \\(|n\\rangle\\) is defined as:\n\\[\n\\hat{a} |n\\rangle = \\sqrt{n} |n-1\\rangle\n\\]\nHere, \\(|n\\rangle\\) represents a quantum state with \\(n\\) quanta of energy (also known as a Fock state), and \\(\\sqrt{n}\\) is the normalization factor. The matrix representation of the annihilation operator in an \\(d\\)-dimensional Hilbert space is given by an upper triangular matrix with the square roots of natural numbers as its off-diagonal elements.\n\n\n# Define the annihilation operator for d-dimensional Hilbert space\nd = 10\n\na = qutip.destroy(d)\n\nprint(\"Annihilation operator (a) for d=7:\")\na\n\nAnnihilation operator (a) for d=7:\n\n\nQuantum object: dims=[[10], [10]], shape=(10, 10), type='oper', dtype=Dia, isherm=False\nQobj data =\n[[0.         1.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         1.41421356 0.         0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         1.73205081 0.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         2.         0.\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         2.23606798\n  0.         0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  2.44948974 0.         0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         2.64575131 0.         0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         2.82842712 0.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         3.        ]\n [0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.        ]]\n\n\n\n\n\nPauli Matrices\nThe Pauli matrices are fundamental in the study of quantum mechanics, representing the spin operators for a spin-1/2 particle and quantum two-level systems.\n\\[\n\\sigma_x = \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}, \\ \\sigma_y = \\begin{pmatrix} 0 & -i \\\\ i & 0 \\end{pmatrix}, \\ \\sigma_z = \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix}\n\\]\nWe can define these matrices in QuTiP as follows:\n\n\nsigma_x = qutip.sigmax()\nsigma_y = qutip.sigmay()\nsigma_z = qutip.sigmaz()\n\nprint(\"Sigma X:\")\ndisplay(sigma_x)\nprint(\"\\n\")\nprint(\"Sigma Y:\")\ndisplay(sigma_y)\nprint(\"\\n\")\nprint(\"Sigma Z:\")\nsigma_z\n\nSigma X:\n\n\nQuantum object: dims=[[2], [2]], shape=(2, 2), type='oper', dtype=CSR, isherm=True\nQobj data =\n[[0. 1.]\n [1. 0.]]\n\n\n\n\nSigma Y:\n\n\nQuantum object: dims=[[2], [2]], shape=(2, 2), type='oper', dtype=CSR, isherm=True\nQobj data =\n[[0.+0.j 0.-1.j]\n [0.+1.j 0.+0.j]]\n\n\n\n\nSigma Z:\n\n\nQuantum object: dims=[[2], [2]], shape=(2, 2), type='oper', dtype=CSR, isherm=True\nQobj data =\n[[ 1.  0.]\n [ 0. -1.]]\n\n\n\n\n\n\n8.1.2 Operator Functions and Operations\nQuTiP supports various operations on operators, including addition, multiplication (both scalar and matrix), and the commutator. These operations are essential for constructing Hamiltonians, calculating observables, and more.\n\nExample: Commutator of Pauli Matrices\nThe commutator of two operators \\(A\\) and \\(B\\) is defined as \\([A, B] = AB - BA\\). Let’s calculate the commutator of \\(\\sigma_x\\) and \\(\\sigma_y\\).\n\n\ncommutator_xy = qutip.commutator(sigma_x, sigma_y)\nprint(\"Commutator of Sigma X and Sigma Y:\")\ndisplay(commutator_xy)\ncommutator_xy == 2j * sigma_z\n\nCommutator of Sigma X and Sigma Y:\n\n\nQuantum object: dims=[[2], [2]], shape=(2, 2), type='oper', dtype=CSR, isherm=False\nQobj data =\n[[0.+2.j 0.+0.j]\n [0.+0.j 0.-2.j]]\n\n\nTrue",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to QuTiP</span>"
    ]
  },
  {
    "objectID": "lecture1/qutip_introduction.html#quantum-states",
    "href": "lecture1/qutip_introduction.html#quantum-states",
    "title": "8  Introduction to QuTiP",
    "section": "8.2 Quantum States",
    "text": "8.2 Quantum States\nQuantum states describe the state of a quantum system. In QuTiP, states are represented again as Qobj instances. This section focuses on the representation and manipulation of quantum states.\n\n8.2.1 Fock States\nThe most basic quantum states are the fock states, often denoted as \\(|n\\rangle\\) (with \\(n \\in \\mathbb{N}\\)). Let’s see how we can create these in QuTiP.\n\n\n8.2.2 Superposition States\nQuantum mechanics allows particles to be in a superposition of states. Let’s create a superposition state.\n\\[\n\\vert \\psi \\rangle = \\frac{1}{\\sqrt{2}} \\left( \\vert 0 \\rangle + \\vert 1 \\rangle \\right)\n\\]\n\n\nfock_0 = qutip.fock(d, 0)  # Fock state |0&gt;\nfock_1 = qutip.fock(d, 1)  # Fock state |1&gt;\n\n# Creating a superposition state\nsuperposition_state = (fock_0 + fock_1).unit()  # Normalize the state\n\nprint(\"Superposition state:\")\nsuperposition_state\n\nSuperposition state:\n\n\nQuantum object: dims=[[10], [1]], shape=(10, 1), type='ket', dtype=Dense\nQobj data =\n[[0.70710678]\n [0.70710678]\n [0.        ]\n [0.        ]\n [0.        ]\n [0.        ]\n [0.        ]\n [0.        ]\n [0.        ]\n [0.        ]]\n\n\n\n\n\n8.2.3 Coherent States\nCoherent states in QuTiP represent quantum states closest to classical waves, defined as \\[\n|\\alpha\\rangle = e^{-|\\alpha|^2/2} \\sum_{n=0}^{\\infty} \\frac{\\alpha^n}{\\sqrt{n!}} |n\\rangle \\, ,\n\\]\nwith minimal uncertainty.\nThe coherent state is an eigenstate of the annihilation operator \\[\n\\hat{a} \\vert \\alpha \\rangle = \\alpha \\vert \\alpha \\rangle\n\\]\n\n\n\n\n\n\nWarning!\n\n\n\nRemember that every Qobj lives in a truncated Hilbert space. If the \\(\\alpha\\) value is too large, the state will become a non-physical state because it will touch the high energy levels of the truncated Hilbert space.\n\n\n\n\nalpha = 0.8\ncoherent_state = qutip.coherent(d, alpha)\n\ncoherent_state\n\nQuantum object: dims=[[10], [1]], shape=(10, 1), type='ket', dtype=Dense\nQobj data =\n[[7.26149037e-01]\n [5.80919230e-01]\n [3.28617541e-01]\n [1.51781941e-01]\n [6.07127755e-02]\n [2.17212702e-02]\n [7.09408207e-03]\n [2.14540751e-03]\n [6.04780881e-04]\n [1.71316475e-04]]\n\n\n\nLet’s compute the fidelity between \\(\\vert \\alpha \\rangle\\) and \\(\\hat{a} \\vert \\alpha \\rangle / \\alpha\\).\n\n\nqutip.fidelity(a * coherent_state / alpha, coherent_state)\n\nnp.float64(0.9999999837403134)\n\n\n\n\n\n8.2.4 Spin States\n\n\nqutip.spin_state(0.5, -1)\n\nQuantum object: dims=[[2], [1]], shape=(2, 1), type='ket', dtype=Dense\nQobj data =\n[[0.]\n [1.]]\n\n\n\n\n\n8.2.5 Density Matrices\nQuantum states can also be represented using density matrices, which are useful for describing mixed states.\n\nCreating a Density Matrix\nLet’s convert our superposition state into a density matrix.\n\n\n# Creating a density matrix from a state\ndensity_matrix = superposition_state * superposition_state.dag()  # Outer product\n\nprint(\"Density matrix of the superposition state:\")\ndensity_matrix\n\nDensity matrix of the superposition state:\n\n\nQuantum object: dims=[[10], [10]], shape=(10, 10), type='oper', dtype=Dense, isherm=True\nQobj data =\n[[0.5 0.5 0.  0.  0.  0.  0.  0.  0.  0. ]\n [0.5 0.5 0.  0.  0.  0.  0.  0.  0.  0. ]\n [0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n [0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n [0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n [0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n [0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n [0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n [0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n [0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]]\n\n\n\n\n\n\n8.2.6 Partial Trace\nThe partial trace over a subsystem, say \\(B\\), of a composite system \\(AB\\), mathematically expresses as “tracing out” \\(B\\), leaving the reduced state of \\(A\\). For a bipartite state \\(\\rho_{AB}\\), the partial trace over \\(B\\) is:\n\\[\n\\text{Tr}_B(\\hat{\\rho}_{AB}) = \\sum_{i \\in \\mathcal{H}_B} \\langle i| \\hat{\\rho}_{AB} |i\\rangle\n\\]\nwhere \\(\\{|i\\rangle\\}\\) forms a complete basis for subsystem \\(B\\).\nLet’s try it with an entangled Bell’s state between two qubits:\n\\[\n\\vert \\phi^+ \\rangle = \\frac{1}{\\sqrt{2}} \\left( \\vert 0, 0 \\rangle + \\vert 1, 1 \\rangle \\right)\n\\]\n\n\n# Bell state between two qubits\nphi_plus = ( qutip.tensor(qutip.spin_state(1/2, -1), qutip.spin_state(1/2, -1)) + qutip.tensor(qutip.spin_state(1/2, 1), qutip.spin_state(1/2, 1)) ).unit()\n\n# Reduced density matrix of the first qubit\nrho_1 = qutip.ptrace(phi_plus, 1)\nrho_1\n\nQuantum object: dims=[[2], [2]], shape=(2, 2), type='oper', dtype=Dense, isherm=True\nQobj data =\n[[0.5 0. ]\n [0.  0.5]]\n\n\n\nWe now apply the partial trace to a more complicated state, that is composed by two bosonic modes and two spins \\(\\vert j_1, m_1 \\rangle\\) and \\(\\vert j_2, m_2 \\rangle\\), with \\(j_1 = 1\\) and \\(j_2 = \\frac{1}{2}\\), \\(m_1 = 0\\), and \\(m_2 = 1\\).\n\n\nd = 10\nj1 = 1\nj2 = 1/2\nm1 = 0\nm2 = 1\n\n\npsi = qutip.tensor(qutip.fock(d, 3), qutip.fock(d, 1), qutip.spin_state(j1, 0), qutip.spin_state(j2, 1))\n\n# Trace only the second spin state\nrho_0 = qutip.ptrace(psi, [0, 1, 2])\ndisplay(rho_0)\n\n# Trace only the first bosonic mode and the second spin state\nrho_1 = qutip.ptrace(psi, [1, 2])\ndisplay(rho_1)\n\n# Trace all except the second bosonic mode\nrho_2 = qutip.ptrace(psi, [1])\nrho_2\n\nQuantum object: dims=[[10, 10, 3], [10, 10, 3]], shape=(300, 300), type='oper', dtype=Dense, isherm=True\nQobj data =\n[[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]]\n\n\nQuantum object: dims=[[10, 3], [10, 3]], shape=(30, 30), type='oper', dtype=Dense, isherm=True\nQobj data =\n[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n  0. 0. 0. 0. 0. 0.]]\n\n\nQuantum object: dims=[[10], [10]], shape=(10, 10), type='oper', dtype=Dense, isherm=True\nQobj data =\n[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to QuTiP</span>"
    ]
  },
  {
    "objectID": "lecture1/qutip_introduction.html#eigenstates-and-eigenvalues",
    "href": "lecture1/qutip_introduction.html#eigenstates-and-eigenvalues",
    "title": "8  Introduction to QuTiP",
    "section": "8.3 Eigenstates and Eigenvalues",
    "text": "8.3 Eigenstates and Eigenvalues\nThe eigenstates and eigenvalues of a system or an operator provide crucial insights into its properties. Let’s explore how to calculate these in QuTiP.\n\n\n# Example: Eigenstates and eigenvalues of Pauli Z\neigenvalues, eigenstates = sigma_z.eigenstates()\n\nprint(\"Eigenvalues of Sigma Z:\")\ndisplay(eigenvalues)\nprint(\"\\n\")\nprint(\"Eigenstates of Sigma Z:\")\ndisplay(eigenstates)\n\nEigenvalues of Sigma Z:\n\n\narray([-1.,  1.])\n\n\n\n\nEigenstates of Sigma Z:\n\n\narray([Quantum object: dims=[[2], [1]], shape=(2, 1), type='ket', dtype=Dense\n       Qobj data =\n       [[ 0.]\n        [-1.]]                                                               ,\n       Quantum object: dims=[[2], [1]], shape=(2, 1), type='ket', dtype=Dense\n       Qobj data =\n       [[-1.]\n        [-0.]]                                                               ],\n      dtype=object)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to QuTiP</span>"
    ]
  },
  {
    "objectID": "lecture1/qutip_introduction.html#computing-expectation-values",
    "href": "lecture1/qutip_introduction.html#computing-expectation-values",
    "title": "8  Introduction to QuTiP",
    "section": "8.4 Computing Expectation Values",
    "text": "8.4 Computing Expectation Values\nThe expectation value of an operator provides insight into the average outcome of a quantum measurement. For a quantum state \\(|\\psi\\rangle\\) and an operator \\(\\hat{O}\\), the expectation value is given by:\n\\[\n\\langle \\hat{O} \\rangle = \\langle\\psi| \\hat{O} |\\psi\\rangle\n\\]\nExpectation values are crucial for predicting measurable quantities in quantum mechanics. Let’s compute the expectation value of the number operator \\(\\hat{n} = \\hat{a}^\\dagger \\hat{a}\\) for a coherent state, which represents a quantum state closest to a classical harmonic oscillator.\n\n\n# Define the coherent state |psi&gt; with alpha=2\nalpha = 0.8\npsi = qutip.coherent(d, alpha)\n\n# Define the number operator n = a.dag() * a\nn = a.dag() * a\n\n# Compute the expectation value of n for the state |psi&gt;\nexpectation_value_n = qutip.expect(n, psi)\n\nprint(\"Expectation value of the number operator for |psi&gt;:\")\ndisplay(expectation_value_n)\nprint(\"\\n\")\nprint(\"The squared modulus of alpha is:\")\ndisplay(abs(alpha) ** 2)\n\nExpectation value of the number operator for |psi&gt;:\n\n\n0.6399999989126254\n\n\n\n\nThe squared modulus of alpha is:\n\n\n0.6400000000000001",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introduction to QuTiP</span>"
    ]
  },
  {
    "objectID": "bibliography.html",
    "href": "bibliography.html",
    "title": "Bibliography",
    "section": "",
    "text": "Campaioli, Francesco, Jared H. Cole, and Harini Hapuarachchi. 2024.\n“Quantum Master Equations: Tips and Tricks for Quantum Optics,\nQuantum Computing, and Beyond.” PRX Quantum 5 (June):\n020202. https://doi.org/10.1103/PRXQuantum.5.020202.\n\n\nD’Alessandro, Domenico. 2021. Introduction to\nQuantum Control and Dynamics. Chapman; Hall/CRC. https://doi.org/10.1201/9781003051268.\n\n\nJohansson, J. R., P. D. Nation, and Franco Nori. 2012. “QuTiP: An open-source Python framework for the dynamics\nof open quantum systems.” Computer Physics\nCommunications 183 (8): 1760–72. https://doi.org/10.1016/j.cpc.2012.02.021.\n\n\nKhaneja, Navin, Timo Reiss, Cindie Kehlet, Thomas Schulte-Herbrüggen,\nand Steffen J. Glaser. 2005. “Optimal Control of Coupled Spin\nDynamics: Design of NMR Pulse Sequences by Gradient Ascent\nAlgorithms.” Journal of Magnetic Resonance 172 (2):\n296–305. https://doi.org/10.1016/j.jmr.2004.11.004.\n\n\nLambert, Neill, Eric Giguère, Paul Menczel, Boxi Li, Patrick Hopf,\nGerardo Suárez, Marc Gali, et al. 2024. “QuTiP 5: The Quantum Toolbox in Python.”\narXiv:2412.04705. https://arxiv.org/abs/2412.04705.\n\n\nLvovsky, A. I., and M. G. Raymer. 2009. “Continuous-Variable\nOptical Quantum-State Tomography.” Rev. Mod. Phys. 81\n(March): 299–332. https://doi.org/10.1103/RevModPhys.81.299.\n\n\nMercurio, Alberto, Yi-Te Huang, Li-Xun Cai, Yueh-Nan Chen, Vincenzo\nSavona, and Franco Nori. 2025. “QuantumToolbox.jl: An Efficient Julia\nFramework for Simulating Open Quantum Systems.” arXiv\nPreprint arXiv:2504.21440. https://doi.org/10.48550/arXiv.2504.21440.\n\n\nPeruzzo, Alberto, Jarrod McClean, Peter Shadbolt, Man-Hong Yung, Xiao-Qi\nZhou, Peter J. Love, Alán Aspuru-Guzik, and Jeremy L. O’Brien. 2014.\n“A Variational Eigenvalue Solver on a Photonic Quantum\nProcessor.” Nature Communications 5 (1). https://doi.org/10.1038/ncomms5213.",
    "crumbs": [
      "Bibliography"
    ]
  },
  {
    "objectID": "appendices/quantum-harmonic-oscillator.html",
    "href": "appendices/quantum-harmonic-oscillator.html",
    "title": "Appendix A — The quantum harmonic oscillator",
    "section": "",
    "text": "As an example, here we consider the quantum harmonic oscillator. Classically, the harmonic oscillator is defined as a system subject to the force \\(\\mathbf{F} = - k \\mathbf{r}\\), where \\(k\\) is the elastic constant. In other words, the force is proportional to the displacement from a stable point (in this case the origin).\nFollowing the relation \\(\\mathbf{F} = - \\nabla V(\\mathbf{r})\\), we can say that the corresponding potential is \\(V(\\mathbf{r}) = k/2 \\ \\mathbf{r}^2\\). The solution of the Schrodinger equation\n\\[\n    i \\hbar \\frac{\\partial}{\\partial t} \\Psi(\\mathbf{r}, t) = -\\frac{\\hbar^2}{2 m} \\nabla^2 \\Psi(\\mathbf{r}, t) + V(\\mathbf{r}) \\Psi(\\mathbf{r}, t) \\, ,\n\\]\nwhere \\(\\hbar\\) is the reduced Planck constant, \\(m\\) is the mass of the particle, and \\(\\nabla^2\\) is the Laplacian operator, gives us the eigenstates of the system. Considering only the one-dimensional case, we obtain the following eigenstates for the quantum harmonic oscillator:\n\\[\n\\psi_n(x) = \\frac{1}{\\sqrt{2^n n!}} \\left(\\frac{m\\omega}{\\pi \\hbar}\\right)^{1/4} e^{-\\frac{m\\omega x^2}{2\\hbar}} H_n\\left(\\sqrt{\\frac{m\\omega}{\\hbar}} x\\right) \\, ,\n\\tag{A.1}\\]\nwhere \\(\\omega = \\sqrt{k / m}\\) is the resonance frequency of the oscillator and \\(H_n\\) is the \\(n\\)-th Hermite polynomial.\nA useful way to describe the quantum harmonic oscillator is by using the ladder operators\n\\[\\begin{eqnarray}\n    \\hat{a} &=& \\sqrt{\\frac{m \\omega}{2 \\hbar}} \\left( \\hat{x} + i \\frac{1}{m \\omega} \\hat{p} \\right) \\\\\n    \\hat{a}^\\dagger &=& \\sqrt{\\frac{m \\omega}{2 \\hbar}} \\left( \\hat{x} - i \\frac{1}{m \\omega} \\hat{p} \\right) \\, ,\n\\end{eqnarray}\\]\nnote that the position \\(\\hat{x}\\) and conjugate momentum \\(\\hat{p}\\) are operators too. If we now write the eigenstates in Equation 5.1 in the bra-ket notation (\\(\\psi_n \\to \\ket{n}\\)), the ladder operators allow us to move from one eigenstate to the next or previous one:\n\\[\\begin{eqnarray}\n    \\label{eq1: destroy operator action}\n    \\hat{a} \\ket{n} &=& \\sqrt{n} \\ket{n-1} \\\\\n    \\label{eq1: creation operator action}\n    \\hat{a}^\\dagger \\ket{n} &=& \\sqrt{n+1} \\ket{n+1} \\, ,\n\\end{eqnarray}\\]\nand it is straightforward to recognize the creation (\\(\\hat{a}^\\dagger\\)) and annihilation (\\(\\hat{a}\\)) operators. In this framework the system Hamiltonian of the quantum harmonic oscillator becomes \\[\n\\hat{H} = \\hbar \\omega \\left( \\hat{a}^\\dagger \\hat{a} + \\frac{1}{2} \\right) \\, .\n\\]\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import eval_hermite, factorial\n\n# Physical parameters\nm = 1.0\nk = 1.0\nw = np.sqrt(k/m)\nalpha = -np.sqrt(2)   # coherent‐state parameter\n\n# Grid\nbounds = 6.0\nx = np.linspace(-bounds, bounds, 1000)\n\n# n-th eigenfunction of the HO (hbar=1)\ndef psi(n, x):\n    Hn = eval_hermite(n, np.sqrt(m*w) * x)\n    norm = (m*w/np.pi)**0.25 / np.sqrt(2**n * factorial(n))\n    return norm * Hn * np.exp(-m*w*x**2/2)\n\n# Build the first six eigenstates and energies\npsi_n = [psi(n, x) for n in range(6)]\nE_n   = [(n + 0.5) * w for n in range(6)]\n\n# Coherent-state wavefunction (real alpha ⇒ no overall phase)\npsi_coh = (m*w/np.pi)**0.25 * np.exp(- (x - np.sqrt(2)*alpha)**2 / 2)\n\n# Plotting\nfig, ax = plt.subplots()\n\n# 1) potential\nax.plot(x, 0.5*k*x**2, 'k--', lw=2, label=r'$V(x)=\\tfrac12 k x^2$')\n\n# 2) coherent state\nax.fill_between(x, psi_coh, color='gray', alpha=0.5)\nax.plot(x, psi_coh, color='gray', lw=2, label='Coherent state')\n\n# 3) eigenstates offset by E_n\nlines = []\nfor n in range(6):\n    y = psi_n[n] + E_n[n]\n    line, = ax.plot(x, y, lw=2, label=fr'$|{n}\\rangle$')\n    lines.append(line)\n\n# Cosmetics\nax.set_ylim(0, 7)\nax.set_xlabel(r'$x$')\n\n# State labels on the right\nfor n, line in enumerate(lines):\n    ax.text(4.5, E_n[n] + 0.2, rf'$|{n}\\rangle$', color=line.get_color())\n\nax.text(-3.5, 6.5, r\"$V(x)$\")\nax.text(-3, 0.9, r\"$\\ket{\\alpha}$\", color=\"grey\")\nax.annotate(\"\", xy=(-5.5,3.5), xytext=(-5.5,2.5), arrowprops=dict(arrowstyle=\"&lt;-&gt;\"))\nax.text(-5.4, 3, r\"$\\hbar \\omega$\", ha=\"left\", va=\"center\")\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure A.1: First eigenstates of one-dimensional the quantum harmonic oscillator, each of them vertically shifted by the corresponding eigenvalue. The grey-filled curve corresponds to a coherent state with \\(\\alpha=-\\sqrt{2}\\). The used parameter are \\(m=1\\), \\(\\omega=1\\), and \\(\\hbar=1\\).\n\n\n\n\n\nIt is worth introducing the coherent state \\(\\ket{\\alpha}\\) of the harmonic oscillator, defined as the eigenstate of the destroy operator, with eigenvalue \\(\\alpha\\), in other words, \\(\\hat{a} \\ket{\\alpha} = \\alpha \\ket{\\alpha}\\). It can be expressed analytically in terms of the eigenstates of the quantum harmonic oscillator \\[\n\\begin{aligned}\n    \\ket{\\alpha} = e^{-\\frac{1}{2} \\abs{\\alpha}^2} \\sum_{n=0}^{\\infty} \\frac{\\alpha^n}{\\sqrt{n!}} \\ket{n} \\, ,\n\\end{aligned}\n\\]\nand it can be seen as the most classic-like state since it has the minimum uncertainty \\(\\Delta x \\Delta p = \\hbar / 2\\).\nFigure A.1 shows the first eigenstates of the quantum harmonic oscillator, each of them vertically shifted by the respective energy, while the grey-filled curve is a coherent state with \\(\\alpha = - \\sqrt{2}\\). The black dashed curve is the potential, choosing \\(k = 1\\), \\(m = 1\\), and \\(\\hbar = 1\\). It is worth noting that also the groundstate \\(\\ket{0}\\) has a nonzero energy (\\(E_0 = \\hbar \\omega / 2\\)).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The quantum harmonic oscillator</span>"
    ]
  }
]